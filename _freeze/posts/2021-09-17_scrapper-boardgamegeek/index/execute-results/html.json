{
  "hash": "0b7161fd20b524abb3cd127c7e084433",
  "result": {
    "markdown": "---\ntitle: \"Raspando a página do ranking do BoardGameGeek\"\nsubtitle: |\n  Neste post eu faço a raspagem da tabela do ranking dos jogos de tabuleiro do BoardGameGeek. Essa tarefa foi necessária para que eu conseguisse interagir da melhor forma possível com a API XML que o site oferece.\ndate: 09-17-2021\ndate-modified: 01-17-2024\nbibliography: bibliografia.bib\ncategories:\n  - web scraping\n  - r\n  - boardgames\nexecute:\n  warning: false\n  fig-align: center\n---\n\n\n\n\n# Motivação\n\nSempre joguei os jogos de tabuleiro mais tradicionais, como Banco Imobiliário, Scotland Yard e War. Esses são jogos muito populares, apesar de cada partida ser muito repetitiva e eles demandarem uma quantidade razoável de jogadores para que tenham graça - e, no meio de uma pandemia, se já acabava sendo chato jogar um deles, a coisa passou a ser impossível. Mas será que não existem alternativas (mais divertidas, inclusive) para continuar com a distração num momento tão difícil como esse? Como eu bem descobri, a resposta estava nos próprios jogos de tabuleiro - mais precisamente, na reinvenção que eles sofreram nas últimas décadas.\n\nExistem inúmeros jogos de tabuleiro disponíveis atualmente e um número crescente de pessoas que os curtem. Dada esta diversidade de novos títulos, inúmeros portais têm focado em criar e manter essa cultura, trazendo reportagens, fóruns, _marketplaces_, _reviews_, rankings e fichas técnicas de cada um deles. Dois exemplos destes sites são o [BoardGameGeek](https://boardgamegeek.com/) e a [Ludopedia](https://www.ludopedia.com.br/): ambos possuem praticamente o mesmo conteúdo, mas o primeiro é um portal americano e o segundo é brasileiro. Outro ponto interessante é que o consumo de informações desses portais não precisa ocorrer pelo _browser_, uma vez que ambos fornecem uma API. A Ludopedia oferece uma API REST bastante intuitiva^[Essa API ainda está em desenvolvimento, e devo escrever sobre o consumo de informações através dela em outro post], enquanto o BoardGameGeek usa uma API XML que eu acabei achando meio complicada de usar. Mas o que isto tudo tem haver com dados?\n\nBom, logo que descobri esse _hobby_, acabei ficando muito perdido sobre quais são os títulos mais legais para se jogar. São tantas as possibilidades e informações disponíveis sobre cada jogo, que eu me peguei navegando entre inúmeras páginas naqueles portais para tentar encontrar aquilo que eu estava buscando. Assim, acabei tendo a ideia de compilar essas informações e colocar tudo dentro de uma linguagem de programação, a fim de deixar a análise de dados me ajudar a encontrar os jogos que mais combinavam com aquilo que eu estava buscando. Para isso, tive a ideia de pegar as informações dos jogos do BoardGameGeek (BGG daqui em diante) através de sua API, tabular tudo o que estava buscando e partir para o abraço. Mas nada é tão simples quanto parece.\n\nA parede que encontrei é bem chatinha: o _request_ da API XML do BoardGameGeek funciona muito melhor quando usamos o código numérico de identificação do jogo. Quando passamos o nome do jogo para o _request_, ele precisa estar grafado igual à como está na base do BGG, caso contrário ele pode falhar em trazer o que você está buscando ou trazer todos os títulos que tenham um _match_ parcial com aquele que você buscou (daí para a frente é só caos). Outra ressalva aqui é que essa API não oferece nenhum tipo de método através do qual podemos pegar um de-para dos IDs numéricos para os nomes dos jogos, e o código numérico deles também não é sequencial. Logo, não dá para fazer uma busca gulosa e _loopar_ os IDs de 1 até _n_. A solução mais simples para o problema é montar a nossa própria base de-para, catando o nome dos títulos e o seu ID numérico de algum lugar do site do BGG - e esse lugar é a página que contém o ranking dos jogos de tabuleiro no site.\n\nNeste _post_ eu vou mostrar como raspar a página do ranking do BGG, usando como base o fluxo do Web Scrapping que a galera da Curso-R criou (@Lente2018), e muito bem ilustrada na figura abaixo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::include_graphics(path = 'images/web_scrapping_cycle_curso_r.png')\n```\n\n::: {.cell-output-display}\n![Fluxo do Web Scrapping de acordo com o Lente (2018). Figura copiada de https://blog.curso-r.com/posts/2018-02-18-fluxo-scraping/.](images/web_scrapping_cycle_curso_r.png){width=874}\n:::\n:::\n\n\n# Identificar\n\nA primeira coisa a se fazer é entender como funciona a página que queremos raspar e o seu fluxo de paginação - isto é, como fazer para navegar de uma página para a outra. No nosso caso, navegamos até a página inicial do ranking do BGG através do link `https://boardgamegeek.com/browse/boardgame`; isso deve nos levar à uma página similar à da figura abaixo.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\nknitr::include_graphics(path = 'images/identificar_1.jpg')\n```\n\n::: {.cell-output-display}\n![](images/identificar_1.jpg)\n:::\n:::\n\n\nPodemos ver que a página que contém o Top 100 dos jogos de tabuleiro apresenta as informações do ranking dentro de uma tabela: cada jogo no ranking ocupa uma linha da tabela, e cada coluna abriga uma informação distinta sobre o título que ocupa àquela posição (_i.e._, ranking, título, uma pequena descrição, quantidade de votos, notas,...). Além disso, podemos ver que o ranking é composto por dezenas de páginas, e podemos navegar através delas pela paginação acima ou abaixo da tabela. \n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\nknitr::include_graphics(path = 'images/identificar_2.jpg')\n```\n\n::: {.cell-output-display}\n![](images/identificar_2.jpg)\n:::\n:::\n\n\nAo passarmos para a segunda página, podemos ver que agora temos acesso às informações do Top 101 ao 200 dos jogos de tabuleiro. Assim, dá para entender que cada página deve conter uma tabelinha com 100 linhas, uma para cada título ocupando cada uma das posições. Outro ponto importante é que quando mudamos para a segunda página do ranking, houve a adição do sufixo `page/2` à _url_. Se brincarmos um pouquinho com essa _url_ podemos ver que é possível navegar entre as páginas simplesmente mudando o número ao final da url: `https://boardgamegeek.com/browse/boardgame/page/1`, `https://boardgamegeek.com/browse/boardgame/page/2`, `https://boardgamegeek.com/browse/boardgame/page/3` e assim por diante. Ou seja, para raspar as páginas do ranking basta usarmos a _url_ base (`https://boardgamegeek.com/browse/boardgame/page/`) e variar apenas a numeração ali no fim.\n\n# Navegar\n\nUma vez que entendemos de que forma funcionam as páginas que queremos raspar, precisamos agora é entender de onde vem o dado que queremos extrair através do código da página. De forma bem geral, podemos usar as ferramentas do desenvolvedor do nosso navegar e ir até a aba de _Network_ para ver se a página está fazendo alguma chamada à uma API para carregar o seu conteúdo - se esse for o caso, podemos aprender a usar a API e usar ela para obter os dados que buscamos. Caso não haja uma API por trás da informação que estamos buscando, podemos ir direto na aba _Elements_ e olhar o código HTML para entender como o que estamos buscando está estruturado.\n\nNo nosso caso, não consegui achar uma API alimentando os dados da tabela que queremos raspar - aparentemente, todo o dado é carregando junto do HTML da página. Assim, olhando o código HTML da página, dá para ver que a tabela que buscamos está dentro de uma tag _table_. Portanto, se conseguirmos pegar o HTML da página, teremos acesso aos dados que estamos buscando.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\nknitr::include_graphics(path = 'images/navegar_1.jpg')\n```\n\n::: {.cell-output-display}\n![](images/navegar_1.jpg)\n:::\n:::\n\n\n# Replicar\n\nComo sabemos a _url_ base e como navegar entre as páginas, a ideia será tentar pegar uma das páginas e ver se temos sucesso na tarefa. Vamos então para o R. Primeiro vou carregar alguns pacotes para nos ajudar a fazer o web scrapping, manipular os dados e algumas coisas mais. Vou então criar um objeto que vai receber a _url_ base do ranking do BGG e mais um objeto com a numeração da página do ranking que vamos tentar pegar na primeira tentativa. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # core\nlibrary(httr) # web scrapping\nlibrary(xml2) # parsear\nlibrary(fs) # mexer com paths\nlibrary(ggforce) # extendendo o ggplot2\n\n## para raspar o site\nbase_url <- 'https://boardgamegeek.com/browse/boardgame/page/'\n\n# definindo qual pagina vamos raspar\npagina_alvo <- 1\n```\n:::\n\n\nCom estes pacotes e objetos definindos, vou utilizar então a função `GET` do pacote `httr` para fazer o _request_ da página. Vou passar o endereço da página para a função e, também, escrever o resultado do _request_ para o disco passando uma outra função (`write_disk`) para o `GET`. O benefício disso são pelo menos dois: (1) podemos manter uma memória exata do que foi raspado naquele dia e (2) usar o dado raspado _offline_, sem a necessidade de ter que ficar bombardeando o site o tempo todo de _resquests_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## criando pasta temporaria caso ela nao exista\ntemp_folder <- 'temp'\nif (!dir_exists(path = temp_folder)) {\n  fs::dir_create(path = temp_folder)\n}\n\n## passando o get e salvando o arquivo html\nresultado <- GET(url = str_glue(base_url, pagina_alvo), \n                 write_disk(path = str_glue('{temp_folder}/page_{pagina_alvo}.html'),\n                            overwrite = TRUE)\n)\n```\n:::\n\n\nUma vez que o _request_ foi feito, podemos ver o objeto resultante. É possível ver que tivemos sucesso em nossa requisição através do status 200, e também podemos ver que o conteúdo obtido é um documento HTML (conforme esperado). Beleza, temos o conteúdo; agora precisamos extrair os dados a partir dele.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsprintf('Status code: %.0d', resultado$status_code)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Status code: 200\"\n```\n:::\n:::\n\n\n# Parsear\n\nO que estamos buscando está dentro de uma tabela no código HTML, conforme havíamos visto anteriormente. Podemos tomar vantagem disto e buscar a _tag_ _table_ dentro do código HTML através do respectivo XPath utilizando a função `xml_find_all`. Com este resultado, podemos então utilizar a função `html_table` do pacote `rvest`, que é muito útil em casos como esse: ela é capaz de converter a tabela HTML diretamente para uma `tibble`. Como o resultado desse processo é uma `tibble` como o primeiro elemento de uma lista, vou usar a função `pluck` para extrair este objeto.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code}\n## parseando o html para um tibble\nranking_pagina_1 <- resultado %>% \n  # pegando o conteudo do GET\n  content() %>% \n  # pegando o xpath que contém a tabela\n  xml_find_all(xpath = '//table') %>% \n  # parseando o codigo html para o rvest\n  rvest::html_table() %>% \n  # extraindo o primeiro elemento da lista\n  pluck(1) %>% \n  # dropando qualquer valor do ranking the não seja um número\n  filter(str_detect(string = `Board Game Rank`, pattern = '[A-Za-z]', negate = TRUE))\nranking_pagina_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 7\n   `Board Game Rank` `Thumbnail image` Title          `Geek Rating` `Avg Rating`\n   <chr>             <chr>             <chr>          <chr>         <chr>       \n 1 1                 \"\"                \"Brass: Birmi… 8.416         8.60        \n 2 2                 \"\"                \"Pandemic Leg… 8.385         8.53        \n 3 3                 \"\"                \"Gloomhaven\\n… 8.366         8.60        \n 4 4                 \"\"                \"Ark Nova\\n\\t… 8.330         8.54        \n 5 5                 \"\"                \"Twilight Imp… 8.242         8.61        \n 6 6                 \"\"                \"Terraforming… 8.219         8.36        \n 7 7                 \"\"                \"Dune: Imperi… 8.219         8.43        \n 8 8                 \"\"                \"War of the R… 8.181         8.54        \n 9 9                 \"\"                \"Gloomhaven: … 8.178         8.46        \n10 10                \"\"                \"Star Wars: R… 8.172         8.42        \n# ℹ 90 more rows\n# ℹ 2 more variables: `Num Voters` <chr>, Shop <chr>\n```\n:::\n:::\n\n\nBeleza! Já temos quase tudo o que queríamos! Só faltou uma coisinha: onde estão os links com o _id_ numérico único de cada título? Se voltarmos ao código HTML, essa informação parece não estar presente na tabela...mas calma aí, vamos dar um passo para trás e tentar entender onde estaria essa informação. Se abrirmos a página de alguns jogos e olharmos as _urls_ podemos ver que existe um padrãozinho: a _url_ de cada título usa um sufixo na forma `boardgame/<sequência numérica>/<nome do jogo>`. \n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\nknitr::include_graphics(path = 'images/parsear_1.jpg')\n```\n\n::: {.cell-output-display}\n![](images/parsear_1.jpg)\n:::\n:::\n\n\nEssa sequência numérica ali na _url_ para o título na página nada mais é do que o _id_ que estamos buscando. Se procurarmos este padrão dentro do HTML da página do ranking do BGG, podemos ver que ele está dentro do XPath da tabela que já havíamos rapasdo, mas dentro de uma outra _tag_. Dado que ainda temos o resultado do nosso _request_, podemos então utilizar o XPath para pegar todos os _links_ na tabela HTML que possuem a classe _primary_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresultado %>% \n  # pegando o conteudo do GET\n  content() %>% \n  # pegando todos as tags de link na classe primary\n  xml_find_all(xpath = '//table//a[@class=\"primary\"]') %>% \n  # pegando so os primeiros exemplos\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (6)}\n[1] <a href=\"/boardgame/224517/brass-birmingham\" class=\"primary\">Brass: Birmi ...\n[2] <a href=\"/boardgame/161936/pandemic-legacy-season-1\" class=\"primary\">Pand ...\n[3] <a href=\"/boardgame/174430/gloomhaven\" class=\"primary\">Gloomhaven</a>\n[4] <a href=\"/boardgame/342942/ark-nova\" class=\"primary\">Ark Nova</a>\n[5] <a href=\"/boardgame/233078/twilight-imperium-fourth-edition\" class=\"prima ...\n[6] <a href=\"/boardgame/167791/terraforming-mars\" class=\"primary\">Terraformin ...\n```\n:::\n:::\n\n\nE aí estão os links! Agora é só pegar eles, colocar como uma coluna dentro da tabela que já havíamos parseado e pronto! Os elementos básicos do que buscávamos estão em seus devidos lugares. Agora é tentar repetir e escalar.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code}\n# colocando os links em uma lista\nlinks <- resultado %>% \n  # pegando o conteudo do GET\n  content() %>% \n  # pegando todos as tags de link na classe primary\n  xml_find_all(xpath = '//table//a[@class=\"primary\"]') %>% \n  # pegando o atributo href\n  xml_attr(attr = 'href')\n\n# colocando os links na tabela\nranking_pagina_1 <- ranking_pagina_1 %>% \n  mutate(\n    # colocando os links em uma coluna\n    link = links\n  )\nranking_pagina_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 8\n   `Board Game Rank` `Thumbnail image` Title          `Geek Rating` `Avg Rating`\n   <chr>             <chr>             <chr>          <chr>         <chr>       \n 1 1                 \"\"                \"Brass: Birmi… 8.416         8.60        \n 2 2                 \"\"                \"Pandemic Leg… 8.385         8.53        \n 3 3                 \"\"                \"Gloomhaven\\n… 8.366         8.60        \n 4 4                 \"\"                \"Ark Nova\\n\\t… 8.330         8.54        \n 5 5                 \"\"                \"Twilight Imp… 8.242         8.61        \n 6 6                 \"\"                \"Terraforming… 8.219         8.36        \n 7 7                 \"\"                \"Dune: Imperi… 8.219         8.43        \n 8 8                 \"\"                \"War of the R… 8.181         8.54        \n 9 9                 \"\"                \"Gloomhaven: … 8.178         8.46        \n10 10                \"\"                \"Star Wars: R… 8.172         8.42        \n# ℹ 90 more rows\n# ℹ 3 more variables: `Num Voters` <chr>, Shop <chr>, link <chr>\n```\n:::\n:::\n\n\n# Validar\n\nComo já sabemos pegar e parsear uma página do ranking do BGG, a ideia aqui será repetir o processo anterior para uma segunda ou uma enésima página. Para isso, vou consolidar o que fizemos anteriormente em duas funções: uma para fazer o _request_ da página do ranking do BGG (`pega_pagina`) e uma outra função para parsear o conteúdo de um _request_ para uma `tibble` (`parser_pagina`).  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# função para fazer o GET\npega_pagina <- function(url_base, pagina, save_dir) {\n  ## junta a base url com o numero da pagina e salva no diretorio alvo\n  resultado_do_get <- GET(url = str_glue(url_base, pagina), \n                          write_disk(path = str_glue('{save_dir}/page_{pagina}.html'), \n                                     overwrite = TRUE)\n  )\n  # retorna o resultado do GET\n  resultado_do_get\n}\n\n# função para parsear o resultado\nparser_pagina <- function(pagina_raspada){\n  # pegando todos os links que estão dentro da tabela \n  links_da_pagina <- pagina_raspada %>% \n    xml_find_all(xpath = '//table//a[@class=\"primary\"]') %>% \n    xml_attr(attr = 'href')\n  \n  ## parseando o codigo HTML da tabela para um tibble\n  tabela_da_pagina <- pagina_raspada %>% \n    xml_find_all(xpath = '//table') %>% \n    rvest::html_table() %>% \n    pluck(1) %>% \n    filter(str_detect(string = `Board Game Rank`, pattern = '[A-Za-z]', negate = TRUE)) %>% \n    mutate(link = links_da_pagina)\n  \n  ## retornando a tabela\n  tabela_da_pagina\n}\n```\n:::\n\n\nAplicando estas funções à segunda página do ranking do BGG temos então o Top 101 à 200 na tabela.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\n## pegando o conteudo da segunda pagina do ranking\nranking_pagina_2 <- pega_pagina(url_base = base_url, \n                                pagina = 2,\n                                save_dir = temp_folder\n)\n\n## parseando o resultado do GET\nranking_pagina_2 <- parser_pagina(pagina_raspada = content(ranking_pagina_2))\nranking_pagina_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 8\n   `Board Game Rank` `Thumbnail image` Title          `Geek Rating` `Avg Rating`\n   <chr>             <chr>             <chr>          <chr>         <chr>       \n 1 101               \"\"                \"Raiders of t… 7.528         7.74        \n 2 102               \"\"                \"Caylus\\n\\t\\t… 7.524         7.72        \n 3 103               \"\"                \"Dominion: In… 7.522         7.69        \n 4 104               \"\"                \"Tigris & Eup… 7.521         7.70        \n 5 105               \"\"                \"Concordia Ve… 7.519         8.29        \n 6 106               \"\"                \"Lorenzo il M… 7.517         7.85        \n 7 107               \"\"                \"Eldritch Hor… 7.516         7.75        \n 8 108               \"\"                \"Troyes\\n\\t\\t… 7.516         7.72        \n 9 109               \"\"                \"Mombasa\\n\\t\\… 7.513         7.85        \n10 110               \"\"                \"Hegemony: Le… 7.513         8.53        \n# ℹ 90 more rows\n# ℹ 3 more variables: `Num Voters` <chr>, Shop <chr>, link <chr>\n```\n:::\n:::\n\n\n# Iterar\n\nO objetivo aqui seria passar uma sequência de páginas de 1 até `n` e usar as funções `pega_pagina` e `parser_pagina` para escalar o processo. Como a idéia é apenas demonstrar, vou raspar apenas as 5 primeiras páginas do ranking, fazendo uma pequena modificação na função que criei para não bombardear o BGG de _requests_ num curto período de tempo. Para isso, vou adicionar um `Sys.sleep` depois do `GET`, para que ele espere alguns segundos entre requisições, definindo o tempo de espera entre requisições através de uma distribuição uniforme.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code}\n# adicionando um sys.sleep na função\npega_pagina_com_espera <- function(url_base, pagina, save_dir) {\n  ## junta a base url com o numero da pagina e salva no diretorio alvo\n  resultado_do_get <- GET(url = str_glue(url_base, pagina), \n                          write_disk(path = str_glue('{save_dir}/page_{pagina}.html'), \n                                     overwrite = TRUE)\n  )\n  Sys.sleep(time = runif(n = 1, min = 1, max = 3))\n  # retorna o resultado do GET\n  resultado_do_get\n}\n\n## raspando todas as paginas do ranking\nwalk(.x = 1:5, .f = pega_pagina_com_espera, url_base = base_url, \n     save_dir = temp_folder)\n\n## listando os arquivos html na pasta temporaria\npaginas_salvas <- dir_ls(path = temp_folder)\n\n## parseando as paginas na pasta data\npaginas_raspadas <- map(.x = paginas_salvas, .f = read_html) %>% \n  map(.f = parser_pagina) %>% \n  bind_rows\npaginas_raspadas\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 8\n   `Board Game Rank` `Thumbnail image` Title          `Geek Rating` `Avg Rating`\n   <chr>             <chr>             <chr>          <chr>         <chr>       \n 1 1                 \"\"                \"Brass: Birmi… 8.416         8.60        \n 2 2                 \"\"                \"Pandemic Leg… 8.385         8.53        \n 3 3                 \"\"                \"Gloomhaven\\n… 8.366         8.60        \n 4 4                 \"\"                \"Ark Nova\\n\\t… 8.330         8.54        \n 5 5                 \"\"                \"Twilight Imp… 8.242         8.61        \n 6 6                 \"\"                \"Terraforming… 8.219         8.36        \n 7 7                 \"\"                \"Dune: Imperi… 8.219         8.43        \n 8 8                 \"\"                \"War of the R… 8.181         8.54        \n 9 9                 \"\"                \"Gloomhaven: … 8.178         8.46        \n10 10                \"\"                \"Star Wars: R… 8.172         8.42        \n# ℹ 490 more rows\n# ℹ 3 more variables: `Num Voters` <chr>, Shop <chr>, link <chr>\n```\n:::\n:::\n\n\n# Faxinar (não incluído, mas importante)\n\nApesar desta etapa não estar apresentada de forma explícita na figura da Curso-R, acredito que seja importante e legal a gente dar um jeitinho nos dados antes de terminar. O código abaixo dá conta disso, fazendo algumas coisas: (a) ajeitando o nome das colunas, (b) removendo colunas que não vou precisar, (c) extraindo e organizando o nome do jogo, o ano e sua descrição curta a partir da coluna `Title`, (d) extraindo o _id_ numérico único de cada jogo e (e) renomeando algumas colunas para algo um pouco mais intuitivo. Com isso, a tabela fica bem arrumada e mais apresentável.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\npaginas_faxinadas <- paginas_raspadas %>% \n  # usando o janitor para ajustar o nome das colunas\n  janitor::clean_names() %>% \n  # pegando somente algumas das colunas\n  select(-thumbnail_image, -shop) %>% \n  # ajustando a string do titulo\n  mutate(\n    # removendo o excesso de espaços da string do title\n    title = str_squish(string = title),\n    # pegando a apenas o titulo do jogo\n    titulo    = str_extract(string = title, \n                            pattern = '(.*)(?=\\\\s\\\\(\\\\-?[0-9]{1,4}\\\\))'),\n    # ano de lançamento\n    ano       = str_extract(string = title, \n                            pattern = '(?<=\\\\()(\\\\-?[0-9]{1,4})(?=\\\\))'),\n    # descrição\n    descricao = str_extract(string = title, \n                            pattern = '(?<=\\\\s\\\\(\\\\-?[0-9]{1,4}\\\\)\\\\s)(.*)'),\n    # extraindo o id do jogo\n    id        = str_extract(string = link, \n                            pattern = '(?<=boardgame\\\\/)([0-9]+)(?=\\\\/)'),\n    # parseando variaveis que sao numeros para tal\n    ano         = parse_number(ano),\n    geek_rating = parse_number(geek_rating),\n    avg_rating  = parse_number(avg_rating),\n    num_voters  = parse_number(num_voters)\n  ) %>% \n  # organizando a tabela\n  relocate(\n    id, titulo, ano, .after = board_game_rank\n  ) %>% \n  relocate(\n    title, .after = num_voters\n  ) %>% \n  # renomeando as colunas\n  rename(\n    rank = board_game_rank, nota_bgg = geek_rating, \n    nota_usuarios = avg_rating, votos = num_voters\n  )\npaginas_faxinadas\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 10\n   rank  id     titulo    ano nota_bgg nota_usuarios votos title link  descricao\n   <chr> <chr>  <chr>   <dbl>    <dbl>         <dbl> <dbl> <chr> <chr> <chr>    \n 1 1     224517 Brass:…  2018     8.42          8.6  43453 Bras… /boa… Build ne…\n 2 2     161936 Pandem…  2015     8.39          8.53 52595 Pand… /boa… Mutating…\n 3 3     174430 Gloomh…  2017     8.37          8.6  61085 Gloo… /boa… Vanquish…\n 4 4     342942 Ark No…  2021     8.33          8.54 39762 Ark … /boa… Plan and…\n 5 5     233078 Twilig…  2017     8.24          8.61 22823 Twil… /boa… Build an…\n 6 6     167791 Terraf…  2016     8.22          8.36 96079 Terr… /boa… Compete …\n 7 7     316554 Dune: …  2020     8.22          8.43 41886 Dune… /boa… Influenc…\n 8 8     115746 War of…  2011     8.18          8.54 20598 War … /boa… The Fell…\n 9 9     291457 Gloomh…  2020     8.18          8.46 32691 Gloo… /boa… Vanquish…\n10 10    187645 Star W…  2016     8.17          8.42 31630 Star… /boa… Strike f…\n# ℹ 490 more rows\n```\n:::\n:::\n\n\nE só para fechar: uma figura sobre os dados que raspamos né! Vamos tentar visualizar qual a relação entre a nota do BGG, a nota média dada pelos usuários e quantidade de votos recebidos por cada jogo. A figura abaixo mostra que as notas dadas pelos usuários parecem ser maiores do que as notas do ranking final do BGG - fato que ocorre pois o BGG usa uma média bayesiana para montar o seu ranking. Além disso, podemos ver que parece existir uma tendência aos jogos que recebem mais votos também terem maiores notas no ranking do BGG. Por outro lado, parece que os jogos mais votados são aqueles com menores notas dadas pelos usuários. Curioso, não?\n\n\n::: {.cell .preview-image .column-body-outset fig.dpi='300'}\n\n```{.r .cell-code}\npaginas_faxinadas %>% \n  ggplot() +\n  geom_autopoint(alpha = 0.7, shape = 21, fill = 'deepskyblue3') +\n  geom_autodensity(mapping = aes(x = .panel_x, y = .panel_y), \n                   fill = 'deepskyblue3', color = 'black', alpha = 0.7) +\n  facet_matrix(rows = vars(nota_bgg:votos), layer.diag = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/figura_exploratoria-1.png){width=672}\n:::\n:::\n\n\n# Conclusões\n\nEsse foi o passo-a-passo que eu usei para pegar os jogos que estão na página do ranking do BoardGameGeek (BGG) e o _id_ numérico deles. Precisei fazer essa raspagem pois a API XML do BGG funciona melhor através da utilização deste _id_, e não encontrei outra forma de pegar essa informação em nenhum outro lugar no site ou na API. Existem melhorias que poderiam ser feitas na função e no _pipeline_ de raspagem aqui, tais como adicionar uns `safely` ou `insistently` no `GET` em caso de falhas ou coisas do gênero. Outra melhoria seria também automatizar a identificação da última página que precisa ser raspada. Para este último, deixo até a dica aqui de como pegar o valor: ele está numa _tag_ _div_ no início e no fim do HTML, de forma que basta pegarmos a sua primeira ocorrência para saber o valor máximo da quantidade de páginas; então, é só colocá-lo em um objeto e chamar ele na hora de usar o `walk` ali em cima.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultima_pagina <- resultado %>% \n  # pegando o conteudo do GET\n  content() %>% \n  # pegando a tag que contem o limite de paginas\n  xml_find_first(xpath = '//div//*[@title=\"last page\"]') %>% \n  # limpando a tag\n  xml_text() %>% \n  # passando ela para um numero\n  parse_number()\nultima_pagina\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1520\n```\n:::\n:::\n\n\nDúvidas ou sugestões? É só me procurar que a gente conversa!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22621)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] ggforce_0.4.1   fs_1.6.3        xml2_1.3.5      httr_1.4.7     \n [5] lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.2    \n [9] purrr_1.0.2     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n[13] tidyverse_2.0.0 extrafont_0.19  ggplot2_3.4.3  \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.3        generics_0.1.3    renv_1.0.3        stringi_1.7.12   \n [5] extrafontdb_1.0   hms_1.1.3         digest_0.6.33     magrittr_2.0.3   \n [9] evaluate_0.21     grid_4.3.1        timechange_0.2.0  fastmap_1.1.1    \n[13] jsonlite_1.8.7    rvest_1.0.3       fansi_1.0.4       scales_1.2.1     \n[17] tweenr_2.0.2      cli_3.6.1         rlang_1.1.1       polyclip_1.10-6  \n[21] munsell_0.5.0     withr_2.5.0       yaml_2.3.7        tools_4.3.1      \n[25] tzdb_0.4.0        colorspace_2.1-0  curl_5.2.0        vctrs_0.6.3      \n[29] R6_2.5.1          png_0.1-8         lifecycle_1.0.3   snakecase_0.11.1 \n[33] htmlwidgets_1.6.2 MASS_7.3-60       janitor_2.2.0     pkgconfig_2.0.3  \n[37] pillar_1.9.0      gtable_0.3.3      Rcpp_1.0.11       glue_1.6.2       \n[41] xfun_0.40         tidyselect_1.2.0  rstudioapi_0.15.0 knitr_1.43       \n[45] farver_2.1.1      htmltools_0.5.6   labeling_0.4.2    rmarkdown_2.24   \n[49] Rttf2pt1_1.3.12   compiler_4.3.1   \n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}