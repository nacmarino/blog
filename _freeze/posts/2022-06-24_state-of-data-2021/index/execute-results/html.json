{
  "hash": "d0b95e8ee2cc3abf2fa9b089f40f9d81",
  "result": {
    "markdown": "---\ntitle: \"Qual a diferença entre júnior, pleno e sênior?\"\nsubtitle: |\n  Essa pergunta vive circulando nas rodas de conversa das pessoas dentro da área de dados e, apesar de existir muito material sobre o assunto, ainda não conseguimos olhar para nenhum tipo de dado para responder à essa pergunta. Neste post vou usar os resultados da pesquisa State of Data Brasil 2021 para tentar preencher esse gap, aproveitando para trazer alguns insights sobre o que o mercado de trabalho brasileiro está praticando.\ndate: 06-24-2022\ndate-modified: 02-13-2024\ncategories:\n  - analise\n  - machine learning\n  - r\n  - dataviz\n  - competicao\nexecute:\n  warning: false\n  fig-align: center\n---\n\n\n<style>\nbody {\ntext-align: justify}\n</style>\n\n\n\n\n\n::: {.callout-note}\nEste post foi a minha submissão para o Challenge do [State of Data 2021](https://www.kaggle.com/code/nacmarino/qual-a-diferen-a-entre-j-nior-pleno-e-s-nior), que me rendeu o prêmio de [3º lugar](https://www.datahackers.news/p/esses-pacotes-python-esto-roubando-sua-senha).\n:::\n\n# Por que essa pergunta é tão frequente?\n\nÉ bem comum para quem está na área de dados participar de conversas sobre a diferença entre profissionais de nível Júnior, Pleno e Sênior. Existem diversos posts de [blog](https://medium.com/data-hackers/qual-a-diferenca-de-atribucoes-de-um-cientista-de-dados-junior-pleno-e-senior-1e753534ba6f#:~:text=Quanto%20ao%20s%C3%AAnior%2C%20al%C3%A9m%20das,sugerir%20fontes%20de%20conhecimento%2C%20etc.), canais do [YouTube](https://www.youtube.com/watch?v=GdfJWFru5rs) e [podcasts](https://medium.com/data-hackers/o-senior-de-hoje-%C3%A9-o-staff-de-amanh%C3%A3-staff-podcast-02-db4c02839284) que falam sobre isso de forma bem detalhada e extensiva, e apontam para um consenso: a principal diferença entre aqueles profissionais está na forma de atuar, de realizar entregas, seu nível de conhecimento e/ou o impacto sobre o negócio.\n\nParece que este consenso é tão claro que existem até [check-lists de atuação](littleblah.com/post/2019-09-01-senior-engineer-checklist/) e [matrizes de competência](https://sijinjoseph.com/programmer-competency-matrix/) que tangibilizam o que é esperado das pessoas entre aqueles níveis. Entretanto, alguns dos itens existentes nestas ferramentas podem não generalizar tão bem quanto se espera quando consideramos uma área tão heterogênea como a de dados (_e.g._, diferenças no tipo de indústria, maturidade da empresa, os diferentes papéis existentes dentro de uma função e etc). Uma consequência disso é que apesar de termos uma boa noção do que faz a senioridade de uma pessoa na área de dados, é difícil saber quão aderente o mercado está às nossas expectativas.\n\nEntender o quão bem alinhado está o que a comunidade espera de um profissional em determinado nível de carreira com o que de fato acontece no mercado é bastante importante. Um dos principais motivos para isso é que em um cenário no qual o mercado está tão aquecido, o nível de senioridade de um cargo passou a ser uma moeda de troca para reter ou atrair as pessoas profissionais de dados. Isto, por sua vez, pode gerar um desequilíbrio de expectativas, discursos e práticas que podem acabar prejudicando o desenvolvimento das próprias pessoas. Assim, parece ser um momento importante para buscar definir o perfil médio do profissional de dados por nível de senioridade no mercado brasileiro. \n\n# _State of Data Brasil 2021_: um retrato do mercado de trabalho de dados no Brasil\n\nO _Data Hackers_ e a _Bain & Company_ promoveram uma pesquisa de outubro à dezembro de 2021 que teve como intuito mapear o mercado de trabalho de dados no Brasil. Cerca de 2.645 pessoas que estão se preparando ou que já atuam na área de dados como analistas, cientistas ou engenheiros de dados, bem como gestores, responderam um questionário online aberto com diversas perguntas relacionadas à demografia, qualificação, remuneração, senioridade, atuação e outros aspectos do trabalho na área. A tabela abaixo traz a lista de quase todas as perguntas feitas aos respondentes, bem como algumas das possíveis respostas à cada uma delas - ilustrando o detalhamento da pesquisa, além da diversidade de tópicos abordados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# carregando os pacotes necessários para a exploração dos dados\nlibrary(tidyverse) # core\nlibrary(reactable) # tabelas interativas\nlibrary(ggridges) # ridge plots\nlibrary(scales) # escalas dos gráficos\nlibrary(vegan) # análise multivariada\nlibrary(patchwork) # compor figuras\nlibrary(gghighlight) # para o highlight\nlibrary(glue) # concatenar strings\nlibrary(tidymodels) # machine learning\nlibrary(tidytext) # arrumar os textos\nlibrary(ggalluvial) # alluvial plot\nlibrary(ggrepel) # repelir texto dos plots\n\n# carregando os dados da pesquisa State of Data\ndf <- read_csv(file = 'data/raw/State of Data 2021 - Dataset - Pgina1.csv')\n\n# criando um dicionário para mapear o nome das colunas no estado atual para um nome\n# mais legível, bem como para conseguirmos mapear o texto dela às figuras depois\ndicionario <- tibble(\n  ## pegando o nome das colunas do dataframe\n  coluna = names(df) \n) %>%\n  ## limpando o string com o nome das colunas - o padrão geral é \"(Pergunta, texto)\",\n  ## onde a Pergunta é codificada com base em três informações: Parte, Letra da Pergunta,\n  ## Letra da Opção escolhida. Isto é representado através do código: \n  ## 'P<numero_parte>_<letra_da_pergunta>_<letra_opcao>'. A ideia aqui será quebrar cada\n  ## nome de coluna em parte, letra da pergunta, letra da opção e texto da pergunta, bem \n  ## como mapear se aquela é uma pergunta principal (e.g., 'P<numero>' ou 'P<numero>_<letra>').\n  ## Para tal, vamos começar tratando o texto dos nomes das colunas que capturamos aqui e,\n  ## na sequência, vamos separarar a tupla com base num padrão de regex\n  mutate(\n    ### removendo as aspas simples no nome das colunas: e.g., ('P0', 'id') -> (P0, id)\n    informacao = str_replace_all(string = coluna, pattern = \"'\", replacement = ''),\n    ### removendo os parenteses do nome das colunas: e.g., (P0, id) -> P0, id\n    informacao = str_replace_all(string = informacao, pattern = '\\\\(|\\\\)', replacement = ''),\n    ### ajustando a primeira coluna do dataframe, (P0, id) pois é a única delas que foge do\n    ### padrão \"(Parte , texto)\", onde Parte e texto estão separados por espaço-vírgula-espaço\n    informacao = str_replace_all(string = informacao, pattern = 'P0, id', replacement = 'P0 , id')\n  ) %>% \n  # colocando o identificador da pergunta daquele do texto de descrição da pergunta em colunas\n  # diferentes com base no padrão de regex 'espaço-vírgula-espaço' que os separa\n  separate(col = 'informacao', into = c('pergunta_id', 'texto'), sep = ' , ') %>% \n  # separando o código identificador da pergunta em parte, letra da pergunta e letra da opcao\n  separate(col = 'pergunta_id', into = c('parte', 'pergunta', 'opcao'), sep = '_', remove = FALSE) %>% \n  # corrigindo typos e coisas similares no dicionario com o nome das colunas vindo dos próprios\n  # dados ou da manipulação\n  mutate(\n    ### adicionando uma coluna booleana indicando se cada uma das perguntas é uma pergunta\n    ### principal ou uma resposta à uma pergunta principal - a última é definida pelo padrão\n    ### de regex abaixo como estamos negando o teste, o TRUE marca as perguntas principais\n    pergunta_principal = str_detect(string = coluna, pattern = 'P[2-9]_[a-z]_', negate = TRUE),\n    ### as opções da pergunta P3_d acabaram ficando bugadas na tabela original, de forma que\n    ### as opções vieram dentro do texto de descrição da pergunta. Assim, precisamos resgatar\n    ### a letra das opções de dentro do texto, e colocar ela de volta no identificador dessa\n    ### pergunta quando for o caso\n    opcao = case_when(pergunta_id == 'P3_d_' ~ str_extract(string = texto, pattern = '^[a-k](?=\\\\s)'),\n                      TRUE ~ opcao),\n    pergunta_id = case_when(pergunta_id == 'P3_d_' ~ paste0(pergunta_id, opcao),\n                            TRUE ~ pergunta_id),\n    ### limpando o texto de descrição da pergunta para remover o typo do leakage da opção e \n    ### whitespace que possa haver no texto\n    texto = case_when(pergunta_id == 'P3_d_' ~ str_remove(string = texto, pattern = '^[a-k]\\\\s'),\n                      TRUE ~ texto),\n    texto = str_squish(string = texto)\n  )\n\n## colocando o dicionário de identificador das perguntas e opções em uma tabela para referência rápida \ndicionario %>% \n  # adicionando o identificador único da parte-pergunta como uma coluna no dataframe, de forma a utilizarmos \n  # essa informação mais à frente para mapear que parte-pergunta é múltipla escolha e a remapear o título\n  # original da questão às opções da múltipla escolha. Todas as perguntas de múltipla escolha que vamos\n  # considerar estão da Parte 2 em dia, e são marcadas pelo sufixo '_' após a letra da pergunta \n  mutate(\n    pergunta_parte_id = case_when(\n      str_detect(string = pergunta_id, pattern = 'P[2-9]_[a-z]_') ~ str_extract(string = pergunta_id, pattern = 'P[2-9]_[a-z]'),\n      TRUE ~ pergunta_id\n    )\n  ) %>% \n  # agrupando o dataframe pelo identificador da parte-pergunta\n  group_by(pergunta_parte_id) %>% \n  # identificando as parte-perguntas que são múltipla escolhas através da quantidade de vezes que este \n  # identificador aparece - quando existem diversas opções associadas à uma parte-pergunta, ela deve\n  # aparecer mais de uma vez; assim, se mapearmos as linhas associadas à partes-pergunta que aparecem\n  # mais de uma vez, teremos acesso ao indicador que estamos buscando\n  mutate(\n    contem_opcao = n() > 1\n  ) %>% \n  # retendo todas as observações de  partes-pergunta que não são de múltipla escolha (i.e., '!contem_opcao')\n  # ou todas as observações de partes-pergunta que são de múltipla escolha, desde que não seja a primeira \n  # linha da parte-pergunta (i.e., 'contem_opcao & row_number() > 1') - com este último passo estamos \n  # removendo efetivamente a linha que contém o título da pergunta que dá acesso as opções, o que fará\n  # com que a tabela a seguir não traga como opção o título da pergunta, somente as opções mesmo\n  filter(!contem_opcao | contem_opcao & row_number() > 1) %>% \n  # desagrupando o dataframe\n  ungroup %>% \n  # selecionando apenas as colunas que contém as informações que precisaremos para criar a tabela\n  select(pergunta_parte_id, opcao, texto_pergunta_opcao = texto) %>%\n  # juntando o dicionário para mapear a pergunta_parte_id ao seu texto - isso servirá para pegarmos\n  # o titulo de cada pergunta apenas\n  left_join(y = select(dicionario, pergunta_id, texto), by = c('pergunta_parte_id' = 'pergunta_id')) %>% \n  # colando o identificador da pergunta_parte com o texto dela - i.e., o texto da pergunta em si\n  mutate(texto = paste0('<b>', pergunta_parte_id, '</b><br>', texto)) %>% \n  # dropando a coluna que contém o identificador pergunta_parte_id, pois já temos essa informação\n  # mapeada na coluna com o texto da pergunta\n  select(-pergunta_parte_id) %>% \n  # criando a tabela de referencia com o reactable\n  reactable(\n    groupBy = 'texto', \n    columns = list(\n      texto                = colDef(name = 'Pergunta', html = TRUE, width = 300, maxWidth = 300),\n      opcao                = colDef(name = 'Opções', aggregate = 'unique', width = 100, maxWidth = 100),\n      texto_pergunta_opcao = colDef(name = 'Respostas', width = 200, maxWidth = 200)\n    ), \n    showPageSizeOptions = TRUE, defaultPageSize = 10, borderless = TRUE, striped = TRUE, \n    highlight = TRUE, compact = TRUE, style = list(fontSize = '14px')\n  )\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"reactable html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-a23f10429546bf17cf22\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-a23f10429546bf17cf22\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"opcao\":[null,null,\"a\",null,null,\"a\",\"b\",\"b\",\"c\",null,null,null,null,null,null,null,null,null,null,null,null,null,\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",null,null,\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",null,null,null,null,\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",null,\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"x\",\"y\",\"z\",\"z1\",\"z2\",\"z3\",\"z4\",\"z5\",\"z6\",\"z7\",\"z8\",\"z9\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"x\",null,null,null,\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",null,null,null,null,\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"j\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"a\",\"b\",\"d\",\"e\",\"f\",\"g\"],\"texto_pergunta_opcao\":[\"id\",\"Idade\",\"Faixa idade\",\"Genero\",\"Estado onde mora\",\"uf onde mora\",\"Regiao onde mora\",\"Regiao de origem\",\"Mudou de Estado?\",\"Nivel de Ensino\",\"Área de Formação\",\"Qual sua situação atual de trabalho?\",\"Setor\",\"Numero de Funcionarios\",\"Gestor?\",\"Cargo como Gestor\",\"Cargo Atual\",\"Nivel\",\"Faixa salarial\",\"Quanto tempo de experiência na área de dados você tem?\",\"Quanto tempo de experiência na área de TI/Engenharia de Software você teve antes de começar a trabalhar na área de dados?\",\"Você está satisfeito na sua empresa atual?\",\"Falta de oportunidade de crescimento no emprego atual\",\"Salário atual não corresponde ao mercado\",\"Não tenho uma boa relação com meu líder/gestor\",\"Gostaria de trabalhar em em outra área de atuação\",\"Gostaria de receber mais benefícios\",\"O clima de trabalho/ambiente não é bom\",\"Falta de maturidade analítica na empresa\",\"Você participou de entrevistas de emprego nos últimos 6 meses?\",\"Você pretende mudar de emprego nos próximos 6 meses?\",\"Remuneração/Salário\",\"Benefícios\",\"Propósito do trabalho e da empresa\",\"Flexibilidade de trabalho remoto\",\"Ambiente e clima de trabalho\",\"Oportunidade de aprendizado e trabalhar com referências na área\",\"Plano de carreira e oportunidades de crescimento profissional\",\"Maturidade da empresa em termos de tecnologia e dados\",\"Qualidade dos gestores e líderes\",\"Reputação que a empresa tem no mercado\",\"Atualmente qual a sua forma de trabalho?\",\"Qual a forma de trabalho ideal para você?\",\"Caso sua empresa decida pelo modelo 100% presencial qual será sua atitude?\",\"Qual o número aproximado de pessoas que atuam com dados na sua empresa hoje?\",\"Analytics Engineer\",\"Engenharia de Dados/Data Engineer\",\"Analista de Dados/Data Analyst\",\"Cientista de Dados/Data Scientist\",\"Database Administrator/DBA\",\"Analista de Business Intelligence/BI\",\"Arquiteto de Dados/Data Architect\",\"Data Product Manager/DPM\",\"Business Analyst\",\"Pensar na visão de longo prazo de dados da empresa e fortalecimento da cultura analítica da companhia.\",\"Organização de treinamentos e iniciativas com o objetivo de aumentar a maturidade analítica das áreas de negócios.\",\"Atração, seleção e contratação de talentos para o time de dados.\",\"Decisão sobre contratação de ferramentas e tecnologias relacionadas a dados.\",\"Sou gestor da equipe responsável pela engenharia de dados e por manter o Data Lake da empresa como fonte única dos dados, garantindo a qualidade e confiabilidade da informação.\",\"Sou gestor da equipe responsável pela entrega de dados, estudos, relatórios e dashboards para as áreas de negócio da empresa.\",\"Sou gestor da equipe responsável por iniciativas e projetos envolvendo Inteligência Artificial e Machine Learning.\",\"Apesar de ser gestor ainda atuo na parte técnica, construindo soluções/análises/modelos etc.\",\"Gestão de projetos de dados, cuidando das etapas, equipes envolvidas, atingimento dos objetivos etc.\",\"Gestão de produtos de dados, cuidando da visão dos produtos, backlog, feedback de usuários etc.\",\"Gestão de pessoas, apoio no desenvolvimento das pessoas, evolução de carreira\",\"a Contratar novos talentos.\",\"b Reter talentos.\",\"c Convencer a empresa a aumentar os investimentos na área de dados.\",\"d Gestão de equipes no ambiente remoto.\",\"e Gestão de projetos envolvendo áreas multidisciplinares da empresa.\",\"f Organizar as informações e garantir a qualidade e confiabilidade.\",\"g Conseguir processar e armazenar um alto volume de dados.\",\"h Conseguir gerar valor para as áreas de negócios através de estudos e experimentos.\",\"i Desenvolver e manter modelos Machine Learning em produção.\",\"j Gerenciar a expectativa das áreas de negócio em relação as entregas das equipes de dados.\",\"k Garantir a manutenção dos projetos e modelos em produção, em meio ao crescimento da empresa.\",\"Conseguir levar inovação para a empresa através dos dados.\",\"Garantir retorno do investimento ROI em projetos de dados.\",\"Dividir o tempo entre entregas técnicas e gestão.\",\"Atuacao\",\"Dados relacionais estruturados em bancos SQL\",\"Dados armazenados em bancos NoSQL\",\"Imagens\",\"Textos/Documentos\",\"Vídeos\",\"Áudios\",\"Planilhas\",\"Dados georeferenciados\",\"Dados relacionais estruturados em bancos SQL\",\"Dados armazenados em bancos NoSQL\",\"Imagens\",\"Textos/Documentos\",\"Vídeos\",\"Áudios\",\"Planilhas\",\"Dados georeferenciados\",\"SQL\",\"R\",\"Python\",\"C/C++/C#\",\".NET\",\"Java\",\"Julia\",\"SAS/Stata\",\"Visual Basic/VBA\",\"Scala\",\"Matlab\",\"PHP\",\"Javascript\",\"Não utilizo nenhuma linguagem\",\"SQL\",\"R\",\"Python\",\"C/C++/C#\",\".NET\",\"Java\",\"Julia\",\"SAS/Stata\",\"Visual Basic/VBA\",\"Scala\",\"Matlab\",\"PHP\",\"Javascript\",\"Não utilizo nenhuma linguagem\",\"MySQL\",\"Oracle\",\"SQL SERVER\",\"SAP\",\"Amazon Aurora ou RDS\",\"Amazon DynamoDB\",\"CoachDB\",\"Cassandra\",\"MongoDB\",\"MariaDB\",\"Datomic\",\"S3\",\"PostgreSQL\",\"ElasticSearch\",\"DB2\",\"Microsoft Access\",\"SQLite\",\"Sybase\",\"Firebase\",\"Vertica\",\"Redis\",\"Neo4J\",\"Google BigQuery\",\"Google Firestore\",\"Amazon Redshift\",\"Amazon Athena\",\"Snowflake\",\"Databricks\",\"HBase\",\"Presto\",\"Splunk\",\"SAP HANA\",\"Hive\",\"Firebird\",\"Amazon Web Services AWS\",\"Google Cloud GCP\",\"Azure Microsoft\",\"Oracle Cloud\",\"IBM\",\"Servidores On Premise/Não utilizamos Cloud\",\"Cloud Própria\",\"Microsoft PowerBI\",\"Qlik View/Qlik Sense\",\"Tableau\",\"Metabase\",\"Superset\",\"Redash\",\"MicroStrategy\",\"IBM Analytics/Cognos\",\"SAP Business Objects\",\"Oracle Business Intelligence\",\"Amazon QuickSight\",\"Salesforce/Einstein Analytics\",\"Mode\",\"Alteryx\",\"Birst\",\"Looker\",\"Google Data Studio\",\"SAS Visual Analytics\",\"Grafana\",\"TIBCO Spotfire\",\"Pentaho\",\"Fazemos todas as análises utilizando apenas Excel ou planilhas do google\",\"Não utilizo nenhuma ferramenta de BI no trabalho\",\"Qual oportunidade você está buscando?\",\"Há quanto tempo você busca uma oportunidade na área de dados?\",\"Como tem sido a busca por um emprego na área de dados?\",\"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\",\"Realizo construções de ETLs em ferramentas como Pentaho, Talend, Dataflow etc.\",\"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\",\"Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\",\"Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\",\"Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\",\"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts etc.\",\"Cuido da qualidade dos dados, metadados e dicionário de dados.\",\"Nenhuma das opções listadas refletem meu dia a dia.\",\"Scripts Python\",\"SQL & Stored Procedures\",\"Apache Airflow\",\"Luigi\",\"AWS Glue\",\"Talend\",\"Stitch\",\"Fivetran\",\"Google Dataflow\",\"Oracle Data Integrator\",\"IBM DataStage\",\"SAP BW ETL\",\"SQL Server Integration Services SSIS\",\"SAS Data Integration\",\"Qlik Sense\",\"Knime\",\"Não utilizo ferramentas de ETL\",\"Sua organização possui um Data Lake?\",\"Qual tecnologia utilizada como plataforma do Data Lake?\",\"Sua organização possui um Data Warehouse?\",\"Qual tecnologia utilizada como plataforma do Data Warehouse?\",\"great_expectations\",\"dbt\",\"AWS Deequ\",\"Apache Griffin\",\"Datafold\",\"Amundsen\",\"Monte Carlo\",\"SODA\",\"Big Eye\",\"Data Band\",\"Anomalo\",\"Metaplane\",\"Acceldata\",\"Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\",\"Realizando construções de ETLs em ferramentas como Pentaho, Talend, Dataflow etc.\",\"Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\",\"Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\",\"Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\",\"Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\",\"Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts etc.\",\"Cuidando da qualidade dos dados, metadados e dicionário de dados.\",\"Processo e analiso dados utilizando linguagens de programação como Python, R etc.\",\"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\",\"Utilizo APIs para extrair dados e complementar minhas análises.\",\"Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\",\"Desenvolvo/cuido da manutenção de ETLs utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\",\"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados, Data Warehouses, Data Marts etc.\",\"Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\",\"Utilizo ferramentas avançadas de estatística como SAS\",\"Scripts Python\",\"SQL & Stored Procedures\",\"Apache Airflow\",\"Luigi\",\"AWS Glue\",\"Talend\",\"Stitch\",\"Fivetran\",\"Google Dataflow\",\"Oracle Data Integrator\",\"IBM DataStage\",\"SAP BW ETL\",\"SQL Server Integration Services SSIS\",\"SAS Data Integration\",\"Qlik Sense\",\"Knime\",\"Não utilizo ferramentas de ETL\",\"Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.\",\"\\\"\\\"Point and Click\\\"\\\" Analytics como Alteryx, Knime, Rapidminer etc.\",\"Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.\",\"Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.\",\"Minha empresa não utiliza essas ferramentas.\",\"Não sei informar.\",\"Processando e analisando dados utilizando linguagens de programação como Python, R etc.\",\"Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\",\"Utilizando APIs para extrair dados e complementar minhas análises.\",\"Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\",\"Desenvolvendo/cuidando da manutenção de ETLs utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\",\"Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados, Data Warehouses, Data Marts etc.\",\"Desenvolvendo/cuidando da manutenção de planilhas do Excel ou Google Sheets para atender as áreas de negócio.\",\"Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\",\"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\",\"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\",\"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas produtos de dados.\",\"Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\",\"Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\",\"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc\",\"Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises estatísticas e ajustar modelos.Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\",\"Sou responsável por criar e manter a infra que meus modelos e soluções rodam clusters, servidores, API, containers, etc.\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação\",\"Desenvolvo sistemas de recomendação RecSys\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo técnicas de NLP Natural Language Processing para análisar dados não-estruturados\",\"Utilizo métodos estatísticos clássicos Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatistica para analisar dados\",\"Utilizo cadeias de Markov ou HMMs para realizar análises de dados\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, DBScan etc\",\"Realizo previsões através de modelos de Séries Temporais Time Series\",\"Utilizo modelos de Reinforcement Learning aprendizado por reforço\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo métodos de Visão Computacional\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, Anaconda\",\"Ambientes de desenvolvimento na nuvem Google Colab, AWS Sagemaker, Kaggle Notebooks etc\",\"Ferramentas de AutoML Datarobot, H2O, Auto-Keras etc\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc\",\"Plataformas de Machine Learning TensorFlow, Azure Machine Learning, Kubeflow etc\",\"Feature Store Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitlab etc\",\"Plataformas de Data Apps Streamlit, Shiny, Plotly Dash etc\",\"Ferramentas de estatística avançada como SPSS, SAS, etc.\",\"Não utilizo nenhuma dessas ferramentas no meu dia a dia.\",\"Coletando e limpando os dados que uso para análise e modelagem.\",\"Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\",\"Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas produtos de dados.\",\"Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.\",\"Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\",\"Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\",\"Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.\",\"Criando e gerenciando soluções de Feature Store e cultura de MLOps.\",\"Criando e mantendo a infra que meus modelos e soluções rodam clusters, servidores, API, containers, etc.\",\"Blog/Medium do Data Hackers\",\"Podcast do Data Hackers\",\"Newsletter Semanal\",\"Canal do Slack\",\"Canal do Youtube do Data Hackers\",\"Ainda não conhecia o Data Hackers\"],\"texto\":[\"<b>P0<\\/b><br>id\",\"<b>P1_a<\\/b><br>Idade\",\"<b>P1_a_a<\\/b><br>Faixa idade\",\"<b>P1_b<\\/b><br>Genero\",\"<b>P1_e<\\/b><br>Estado onde mora\",\"<b>P1_e_a<\\/b><br>uf onde mora\",\"<b>P1_e_b<\\/b><br>Regiao onde mora\",\"<b>P1_g_b<\\/b><br>Regiao de origem\",\"<b>P1_g_c<\\/b><br>Mudou de Estado?\",\"<b>P1_h<\\/b><br>Nivel de Ensino\",\"<b>P1_i<\\/b><br>Área de Formação\",\"<b>P2_a<\\/b><br>Qual sua situação atual de trabalho?\",\"<b>P2_b<\\/b><br>Setor\",\"<b>P2_c<\\/b><br>Numero de Funcionarios\",\"<b>P2_d<\\/b><br>Gestor?\",\"<b>P2_e<\\/b><br>Cargo como Gestor\",\"<b>P2_f<\\/b><br>Cargo Atual\",\"<b>P2_g<\\/b><br>Nivel\",\"<b>P2_h<\\/b><br>Faixa salarial\",\"<b>P2_i<\\/b><br>Quanto tempo de experiência na área de dados você tem?\",\"<b>P2_j<\\/b><br>Quanto tempo de experiência na área de TI/Engenharia de Software você teve antes de começar a trabalhar na área de dados?\",\"<b>P2_k<\\/b><br>Você está satisfeito na sua empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_l<\\/b><br>Qual o principal motivo da sua insatisfação com a empresa atual?\",\"<b>P2_m<\\/b><br>Você participou de entrevistas de emprego nos últimos 6 meses?\",\"<b>P2_n<\\/b><br>Você pretende mudar de emprego nos próximos 6 meses?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_o<\\/b><br>Quais os principais critérios que você leva em consideração no momento de decidir onde trabalhar?\",\"<b>P2_q<\\/b><br>Atualmente qual a sua forma de trabalho?\",\"<b>P2_r<\\/b><br>Qual a forma de trabalho ideal para você?\",\"<b>P2_s<\\/b><br>Caso sua empresa decida pelo modelo 100% presencial qual será sua atitude?\",\"<b>P3_a<\\/b><br>Qual o número aproximado de pessoas que atuam com dados na sua empresa hoje?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_b<\\/b><br>Quais desses papéis/cargos fazem parte do time ou chapter de dados da sua empresa?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_c<\\/b><br>Quais dessas responsabilidades fazem parte da sua rotina atual de trabalho como gestor?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P3_d<\\/b><br>Quais são os 3 maiores desafios que você tem como gestor no atual momento?\",\"<b>P4_a<\\/b><br>Atuacao\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_b<\\/b><br>Quais das fontes de dados listadas você já analisou ou processou no trabalho?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_c<\\/b><br>Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_d<\\/b><br>Quais das linguagens listadas abaixo você utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_e<\\/b><br>Entre as linguagens listadas abaixo, qual é a que você mais utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_f<\\/b><br>Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_g<\\/b><br>Quais das opções de Cloud listadas abaixo você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P4_h<\\/b><br>Quais as Ferramentas de Business Intelligence você utiliza no trabalho?\",\"<b>P5_b<\\/b><br>Qual oportunidade você está buscando?\",\"<b>P5_c<\\/b><br>Há quanto tempo você busca uma oportunidade na área de dados?\",\"<b>P5_d<\\/b><br>Como tem sido a busca por um emprego na área de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual como engenheiro de dados?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Engineer?\",\"<b>P6_c<\\/b><br>Sua organização possui um Data Lake?\",\"<b>P6_d<\\/b><br>Qual tecnologia utilizada como plataforma do Data Lake?\",\"<b>P6_e<\\/b><br>Sua organização possui um Data Warehouse?\",\"<b>P6_f<\\/b><br>Qual tecnologia utilizada como plataforma do Data Warehouse?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_g<\\/b><br>Quais as ferramentas de gestão de Qualidade de dados, Metadados e catálogo de dados você utiliza no trabalho?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P6_h<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com análise de dados?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_b<\\/b><br>Quais as ferramentas/tecnologias de ETL que você utiliza no trabalho como Data Analyst?\",\"<b>P7_c<\\/b><br>Sua empresa utiliza alguma das ferramentas listadas para dar mais autonomia em análise de dados para as áreas de negócio?\",\"<b>P7_c<\\/b><br>Sua empresa utiliza alguma das ferramentas listadas para dar mais autonomia em análise de dados para as áreas de negócio?\",\"<b>P7_c<\\/b><br>Sua empresa utiliza alguma das ferramentas listadas para dar mais autonomia em análise de dados para as áreas de negócio?\",\"<b>P7_c<\\/b><br>Sua empresa utiliza alguma das ferramentas listadas para dar mais autonomia em análise de dados para as áreas de negócio?\",\"<b>P7_c<\\/b><br>Sua empresa utiliza alguma das ferramentas listadas para dar mais autonomia em análise de dados para as áreas de negócio?\",\"<b>P7_c<\\/b><br>Sua empresa utiliza alguma das ferramentas listadas para dar mais autonomia em análise de dados para as áreas de negócio?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P7_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo de trabalho?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_a<\\/b><br>Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_b<\\/b><br>Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_c<\\/b><br>Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P8_d<\\/b><br>Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?\",\"<b>P9_a<\\/b><br>Quais das iniciativas do Data Hackers que você já acessou/acompanhou?\",\"<b>P9_a<\\/b><br>Quais das iniciativas do Data Hackers que você já acessou/acompanhou?\",\"<b>P9_a<\\/b><br>Quais das iniciativas do Data Hackers que você já acessou/acompanhou?\",\"<b>P9_a<\\/b><br>Quais das iniciativas do Data Hackers que você já acessou/acompanhou?\",\"<b>P9_a<\\/b><br>Quais das iniciativas do Data Hackers que você já acessou/acompanhou?\",\"<b>P9_a<\\/b><br>Quais das iniciativas do Data Hackers que você já acessou/acompanhou?\"]},\"columns\":[{\"id\":\"opcao\",\"name\":\"Opções\",\"type\":\"character\",\"aggregate\":\"unique\",\"maxWidth\":100,\"width\":100},{\"id\":\"texto_pergunta_opcao\",\"name\":\"Respostas\",\"type\":\"character\",\"maxWidth\":200,\"width\":200},{\"id\":\"texto\",\"name\":\"Pergunta\",\"type\":\"character\",\"html\":true,\"maxWidth\":300,\"width\":300}],\"groupBy\":[\"texto\"],\"defaultPageSize\":10,\"showPageSizeOptions\":true,\"highlight\":true,\"borderless\":true,\"striped\":true,\"compact\":true,\"style\":{\"fontSize\":\"14px\"},\"dataKey\":\"7c1e7a84ded14fc1cfb1947d07de733f\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nEsta pesquisa traz um retrato muito contemporâneo do mercado de trabalho do Brasil e abre as portas para usarmos os dados em favor da própria comunidade. Nesse sentido, há cerca de dois anos, um [_post_](https://medium.com/data-hackers/qual-a-diferenca-de-atribucoes-de-um-cientista-de-dados-junior-pleno-e-senior-1e753534ba6f#:~:text=Quanto%20ao%20s%C3%AAnior%2C%20al%C3%A9m%20das,sugerir%20fontes%20de%20conhecimento%2C%20etc.) no Medium do Data Hackers compilou as respostas de três pessoas para a pergunta '_Qual a diferença entre um Cientista de Dados júnior, pleno e sênior?_'. Todas as respostas passaram pelos mesmos temas, e reforçam que a senioridade está associada não só a atuação das pessoas em si, como também ao impacto que elas têm no negócio em que atuam. Contudo, os resultados da pesquisa _State of Data Brasil 2021_ nos permite revisitar essa mesma pergunta mas buscando a resposta sobre o que é praticado no mercado.\n\nMeu intuito aqui será tentar definir o perfil médio da pessoa Cientista de Dados por nível de senioridade no mercado de trabalho brasileiro. Para isso, vou começar buscando entender um pouquinho melhor os dados que temos à mão e quão heterogênea pode ser a amostra de pessoas cientistas de dados que responderam à pesquisa. Na sequência, vou utilizar um modelo preditivo para identificar os fatores que mais contribuem para diferenciar os profissionais por nível de senioridade.     nas informações sobre as atuações gerais e específicas que foram mapeadas, tais como os conhecimentos, tecnologias utilizadas e os tipos de problemas de dados enfrentados, bem como alguma coisa relacionada ao contexto de trabalho no qual as pessoas estão inseridas. Finalmente, farei uma análise do erro desse modelo para entender o que mais contribui para os desvios de nível de senioridade previstos por ele (_e.g._ qual o perfil do profissional Júnior que atua como Sênior ou vice-versa?). Vamos começar!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# renomeando as colunas e implementando pequenos tratamentos aos dados\ndf <- df %>% \n  # substituindo a tupla mais complexa que está atualmente no nome das colunas pelo código\n  # identificador de cada uma das perguntas a partir do dicionário de dados que criamos - \n  # i.e., \"(Pergunta, texto)\" -> Pergunta\n  set_names(nm = pull(dicionario, pergunta_id)) %>% \n  # corrigindo erros gerais na base de dados\n  mutate(\n    # corrigindo grafia de Arquiteto de dados, que aparece das duas formas na base de dados\n    P2_f = ifelse(test = P2_f == 'Arquiteto de dados', yes = 'Arquiteto de Dados', no = P2_f),\n    # removendo o ponto final da coluna P4_a\n    P4_a = str_remove(string = P4_a, pattern = '\\\\.')\n  ) %>% \n  # removendo registros duplicados - existem 4 pessoas cujas respostas aparecem duas vezes\n  # na base de dados - 1 Cientista de dados, 1 Dev, 1 Engenheiro de ML e 1 Tech Lead\n  distinct(P0, .keep_all = TRUE)\n```\n:::\n\n\n<div class=\"warning\" style='margin-left:2em; margin-right:2em; padding:0.1em; background-color:#F3F3B7; color:#000000; font-size:80%'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>Importante</b></p>\n<p style='margin-left:1em; margin-right:1em;'>\nExistem pequenos tratamentos que precisamos fazer aos dados da pesquisa disponibilizados no Kaggle para corrigir algumas possíveis inconsistências que podem impactar a análise dos dados. Além disso, é importante estar atento à alguns detalhes importantes sobre a forma que as informações foram fornecidas:<br>\n1 - De acordo com as informações na coluna `P0`, existem quatro pessoas respondentes que parecem ter tido o seu registro duplicado na base de dados (<i>e.g.</i>, `7lkav4o0fukny1644lc7lkav4o0c5wsb`). Como as respostas para todas as perguntas são similares entre os registros para estes casos, resolvi manter uma única cópia das instâncias associadas à estes IDs;<br>\n2 - Existem alguns <i>typos</i> nos campos preenchidos para algumas respostas. Um exemplo disto é o cargo de `Arquiteto de Dados`, que aparece grafado de duas formas distintas;<br>\n3 - As perguntas com múltiplas opções já possuem suas respostas no formato de <i>one-hot-encoding</i>. No entanto, encontrei um ou outro caso no qual o procedimento parece não ter sido funcionado 100% bem (<i>e.g.</i>, para a pessoa cujo `P0` é `9oxi24d4ok01hkdst59oxi24d4xriqtr`). Como foram poucas as observações nas quais isto ocorreu, não achei necessário um esforço maior para recriar os <i>encodings</i> a partir das colunas originais;<br>\n4 - É possível que algumas respostas contenham `NA` como resultado do processo de anonimização de dados. Neste contexto, optei por não aplicar nenhum tratamento aos dados para substituir tais valores e, quanto necessário, resolvi por simplesmente remover àquela instância com valores faltantes da análise.<br>\n</p></span>\n</div>\n\n# O perfil dos Cientistas de Dados júnior, pleno e sênior é muito heterogêneo no mercado brasileiro\n\nProfissionais com diferentes papéis e funções na área de dados responderam à pesquisa _State of Data Brasil 2021_: estatísticos, cientistas de dados, engenheiros de dados, professores, desenvolvedores de software, analistas de negócios e etc (conforme mapeado pelo pergunta `P2_f`). Contudo, esses papéis podem não dizer muito sobre o que a pessoa realmente faz no dia a dia e, assim, a pesquisa também mapeou o tipo de atuação que melhor retrata o cotidiano de cada pessoa. Um exemplo de atuação seria a de `Ciência de Dados`, onde a pessoa teria um dia a dia onde ela '_desenha e executa experimentos com o objetivo de responder perguntas do negócio, e desenvolve modelos preditivos e algoritmos de Machine Learning com o objetivo de otimizar e automatizar a tomada de decisão_'. É em cima desta amostra de respondentes que vamos conduzir nossas análises que, por sinal, foi a terceira atuação mais frequente na pesquisa conforme podemos observar na figura abaixo - cerca de 410 respondentes, dos 2.641 existentes após a remoção de IDs duplicados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# criando figura para descrever a quantidade de respondentes por tipo de atuação\ncount(df, P4_a) %>% \n  # reordenando as atuações para que a figura fique com a barra em ordem decrescente\n  mutate(P4_a = str_wrap(string = P4_a, width = 10),\n         P4_a = fct_reorder(.f = P4_a, .x = n, .desc = TRUE)) %>% \n  # criando a figura\n  ggplot(mapping = aes(x = P4_a, y = n, fill = P4_a)) +\n  geom_col(color = 'black', show.legend = FALSE) +\n  geom_text(mapping = aes(label = paste0(round(x = (n / sum(n))  * 100, digits = 2), '%')), \n            stat = 'identity', vjust = -1) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 950), \n                     breaks = seq(from = 0, to = 1000, by = 100)) +\n  scale_fill_manual(values = c('grey15', 'grey30', 'grey45', 'grey65', 'grey80', 'grey95')) +\n  labs(\n    title    = 'Que tipo de atuação melhor caracteriza o seu dia a dia?',\n    subtitle = 'A Ciência de Dados é a terceira atuação mais frequente entre as pessoas respondentes da pesquisa',\n    x        = 'Atuação no dia a dia',\n    y        = 'Quantidade de respondentes'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_observacoes_por_area-1.png){width=672}\n:::\n:::\n\n\nAssociada à atuação de cada pessoa temos também disponível através da pergunta `P2_g` o seu nível de senioridade. Com base nessa informação, podemos observar que cerca de 40.73% das pessoas cientistas de dados que responderam à pesquisa têm o nível de Pleno. O restante das pessoas respondentes parecem representar igualmente os níveis Júnior e Sênior, indicando que temos algum desbalanceamento na representatividade dos níveis de senioridade na amostra analisada. Isto é algo importante de ser considerado nas análises que faremos a seguir.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  # pegando apenas as observações das pessoas que responderam ter uma atuação que reflete \n  # a de uma pessoa cientista de dados\n  filter(P4_a == 'Ciência de Dados') %>% \n  # plotando a quantidade de respondentes por nivel de senioridade\n  ggplot(mapping = aes(x = P2_g, fill = P2_g)) +\n  geom_bar(color = 'black', show.legend = FALSE) +\n  geom_text(mapping = aes(label = paste0(round(x = (..count.. / sum(..count..))  * 100, digits = 2), '%')), \n            stat = 'count', vjust = -1) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 180), breaks = seq(from = 0, to = 180, by = 30)) +\n  scale_fill_manual(values = cores_data_hackers) +\n  labs(\n    title    = 'Qual o nível de senioridade das pessoas Cientista de Dados?',\n    subtitle = '40% dos respondentes têm o nível de senioridade Pleno, enquanto os outros 60% são divididos\\npraticamente de forma igual entre os níveis de Júnior e Sênior',\n    x        = 'Nível de senioridade',\n    y        = 'Número de respondentes'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_observacoes_por_senioridade-1.png){width=672}\n:::\n:::\n\n\nComo dito anteriormente, é consenso que diferenças na forma de atuar, no nível de conhecimento e no impacto sobre o negócio devem estar relacionadas ao nível de senioridade da pessoa cientista de dados. Todavia, é difícil mapear estas coisas em uma pesquisa tão grande e complexa quanto a _State of Data_: afinal, como poderíamos comparar de forma robusta, por exemplo, o quanto cada profissional está trazendo impacto para o negócio? Apesar de não conseguirmos ir tão a fundo nesse nível de detalhe, temos à nossa disposição uma série de indicadores que devem estar pelo menos indiretamente relacionados aqueles itens, mapeados na sequência de perguntas sobre atuações gerais da pessoa profissional de dados (_i.e._, perguntas de prefixo `P4`) e específicas das pessoas cientistas de dados (_i.e._, perguntas de prefixo `P8`). \n\nEstas perguntas tentam explorar de forma bem detalhada cada uma das linguagens, fontes de dados, formas de atuação, tarefas e etc que cada uma das pessoas têm, mas é preciso considerar dois aspectos importantes quando formos utilizá-las. O primeiro é que estas perguntas falam muito mais sobre **o que** a pessoa faz ou usa, enquanto muito da discussão sobre o tema fala do **como** ela faz ou usa. Assim, é importante não perder de vista esta limitação das informações disponíveis. O outro aspecto importante é que algumas perguntas podem ser bastante redundantes, e não faria sentido considerar todas elas simultaneamente. Um exemplo disso está nas perguntas `P4_b` e `P4_c`: a primeira mapeia os tipos de fontes de dados que a pessoa _já usou_ no trabalho, a segunda mapeia àquelas fontes que a pessoa _utiliza a maior parte do tempo_. Como o foco de nossas análises deve estar nos indicadores que falam sobre o momento atual de cada pessoa e como isso se relaciona ao seu nível de senioridade, decidi por utilizar apenas um subconjunto das atuações gerais e específicas disponíveis. Mais especificamente, foquei nas respostas para cada uma das perguntas abaixo^[Na análise exploratória que fiz inicialmente com os dados da pesquisa cheguei a usar todas as perguntas prefixadas com `P4` e `P8`, individualmente e tudo junto, mas os resultados foram os mesmos do que aqueles apresentados aqui. Então resolvi ir um pouco mais simples e definir como critério de inclusão da pergunta o caso dela estar falando ou não do momento atual da pessoa.]:  \n\n| Pergunta |                                         Descrição                                         |\n|:--------:|:-----------------------------------------------------------------------------------------:|\n|   P4_c   |       Entre as fontes de dados listadas, quais você utiliza na maior parte do tempo?      |\n|   P4_d   |               Quais das linguagens listadas abaixo você utiliza no trabalho?              |\n|   P4_f   |    Quais dos bancos de dados/fontes de dados listados abaixo você utiliza no trabalho?    |\n|   P4_g   |            Quais das opções de Cloud listadas abaixo você utiliza no trabalho?            |\n|   P4_h   |          Quais as ferramentas de Business Intelligence você utiliza no trabalho?          |\n|   P8_a   | Quais das opções abaixo fazem parte da sua rotina no trabalho atual com ciência de dados? |\n|   P8_b   |       Quais as técnicas e métodos listados abaixo você costuma utilizar no trabalho?      |\n|   P8_c   |       Quais dessas tecnologias fazem parte do seu dia a dia como cientista de dados?      |\n|   P8_d   |        Em qual das opções abaixo você gasta a maior parte do seu tempo no trabalho?       |\n\nCom 9 perguntas forcem informações de de cerca de 129 atuações distintas, que vamos utilizar para caracterizar a atuação das pessoas cientistas de dados que responderam a pesquisa. Com isso, podemos seguir com a análise exploratória de dados, começando pela divisão dos dados em uma base de treino e outra de teste, trabalhando sobre a primeira de forma que nenhum insight que possamos tirar desta análise inicial possa contaminar o desenvolvimento do modelo preditivo que vamos ajustar depois. Uma vez que essa separação esteja feita, vamos olhar a base de treino para explorar os tipos de atuação mais frequentes entre as pessoas. Alguns padrões interessantes podem ser observados na figura abaixo:  \n\n* Sem nenhuma surpresa, quase todas estas pessoas utilizam Python e/ou SQL em seu trabalho, enquanto um pouco mais de 30% delas disseram usar R;  \n* Outro ponto interessante é que a utilização de técnicas de modelagem mais tradicionais (_i.e._, regressões) foram mais frequentes do que outros tipos de abordagem (_e.g._, redes neurais e modelos baseados em árvore), enquanto ambas foram muito mais frequentes do que a utilização de ferramentas de AutoML;  \n* Na mesma linha do item anterior, parece que a maior parte das pessoas atuam muito mais com o ajuste de modelos aos dados do que na coleta e limpeza dos mesmos;  \n* A tarefa de desenvolvimento dos modelos preditivos parece ser muito mais frequente do que a de colocá-los de fato em produção;  \n* Algumas atuações simplesmente não se aplicam ou são exceções para a amostra de cientistas de dados analisadas aqui. Um exemplo disso é a utilização de ferramentas de BI como `Mode` e a utilização de Áudios como fonte de dados; e,\n* Poucas skills são muito frequentes, e parecem compor um _core_ básico do que é necessário saber e/ou fazer para atuar como cientista de dados. Por outro lado, muitas skills parecem preencher nichos de atuação específicos, sendo compartilhadas por poucas pessoas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# criando um dataframe que contem a matriz de atuação de cada pessoa respondente que diz \n# atuar como Cientista de Dados e o nivel de senioridade associada à cada uma delas\ndf_atuacao <- df %>%\n  # pegando apenas as observações das pessoas que responderam ter uma atuação que reflete \n  # a de uma pessoa cientista de dados\n  filter(P4_a == 'Ciência de Dados') %>% \n  # pegando a coluna com o identificador do respondente e o seu nível de senioridade, bem\n  # como todas as colunas que contém as respostas sobre as atuações gerais e específicas \n  # dos respondentes cuja atuação no dia a dia foi o de Ciência de Dados\n  select(P0, P2_g, matches('P4_[cdfgh]_'), matches('P8_[abcd]_')) %>% \n  # dropando observações que possuem valores faltantes - isso ocorre devido ao mascaramento\n  # de dados\n  drop_na()\n\n# fazendo um shuffle na base antes de separar os dados de treino e de teste, para tentar quebrar \n# qualquer tipo de estrutura que possa haver na base de dados\nset.seed(42)\ndf_atuacao <- slice_sample(.data = df_atuacao, prop = 1, replace = FALSE)\n\n# fazendo o split da base analitica\nset.seed(33)\ndf_atuacao <- initial_split(data = df_atuacao, prop = 0.8, strata = P2_g) %>% \n  training()\n\n# criando a figura para mostrar a distribuição da frequência com a qual as atuações apareceram\n# entre os respondentes\nselect(df_atuacao, -c(P0, P2_g)) %>% \n  # como os respondentes estão na linhas e as perguntas nas colunas, o somatório das colunas \n  # nos trará a  quantidade de respondentes que disseram ter aquele tipo de atuação\n  colSums %>%\n  # colocando o vetor resultante em ordem decrescente, de forma a conseguimos rankeá-lo mais\n  # racilmente depois\n  sort(decreasing = TRUE) %>% \n  # passando o vetor para um tibble, onde teremos uma coluna com o identificador da pergunta \n  # e a outra coluna com a quantidade de pessoas respondentes que disseram ter aquele tipo de\n  # atuação\n  enframe(name = 'pergunta_id', value = 'n_respondentes') %>% \n  # removendo todas as atuações que não tiveram nenhum respondente\n  filter(n_respondentes > 0) %>% \n  # mapeando o identificador único de cada pergunta ao seu respectivo texto, de forma a usar \n  # essa  informação mais tarde na figura\n  left_join(y = dicionario, by = 'pergunta_id') %>%\n  # enriquecendo a base de com informações para plotarmos\n  mutate(\n    # calculando a proporção total dos respondentes que disseram ter cada uma das atuações\n    proporcao    = n_respondentes / nrow(df_atuacao),\n    sequencia    = 1:n(),\n    to_highlight = sequencia %in% c(1, 3, 5, 6, 8, 16, 27, 59, 66, 122, 123),\n    texto = str_trunc(string = texto, width = 54),\n    texto = str_wrap(texto, width = 30)\n  ) %>% \n  ggplot(mapping = aes(x = sequencia, y = proporcao)) +\n  geom_point(size = 3, shape = 21, color = 'black', fill = 'grey50') +\n  gghighlight(\n    to_highlight, \n    label_key = texto, \n    label_params = list(force_pull = -0.005, label.size = NA, fill = NA, seed = 666, vjust = 0.6,\n                        min.segment.length = 0.3),\n    unhighlighted_params = list(shape = 16, size = 1, color = 'black')\n  ) +\n  scale_x_continuous(breaks = c(1, seq(from = 20, to = 150, by = 20))) +\n  scale_y_continuous(labels = label_percent(),\n                     breaks = seq(from = 0, to = 1, by = 0.1), limits = c(0, 1)) +\n  labs(\n    title    = 'Quais as atuações mais frequentes entre os respondentes?',\n    subtitle = 'A maior parte das atuações é pouco frequente entre os respondentes da pesquisa',\n    x        = 'Rank de frequência',\n    y        = 'Porcentagem de pessoas respondentes'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_frequencia_atuacoes-1.png){width=672}\n:::\n:::\n\n\nA visão acima nos conta sobre a identidade das atuações mais frequentes, mas não fala nada sobre a forma como elas estão relacionadas ao nível de senioridade das pessoas. Assim, um primeiro entendimento que podemos buscar é se existe e como se dá a variação na quantidade total de atuações (_i.e._, atuações gerais + específicas) que cada pessoa respondente disse ter de acordo com o seu nível de senioridade. O resultado dessa análise mostra que existe uma grande variabilidade dentro de cada nível de senioridade na quantidade de atuações, e parece que as pessoas de nível Júnior tendem a ter menos atuações do que àquelas em nível de Pleno ou Sênior. Este padrão não é de todo inesperado, mas é curioso ver que as diferenças entre os níveis de senioridade não foram assim tão claras (_e.g._, note que existem pessoas de nível Sênior que têm pouquíssimas atuações).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# criando a figura para mostrar a distribuição da quantidade de atuações distintas dos \n# respondentes por nivel de senioridade\nselect(df_atuacao, -c(P0, P2_g)) %>% \n  # como os respondentes estão na linhas e as perguntas nas colunas, o somatório das \n  # linhas nos trará a  quantidade total de atuações distintas que cada um dos \n  # respondentes assinalou\n  rowSums %>% \n  # colocando o resultado da operação em um tibble, para facilitar a tarefa de plotagem\n  enframe(value = 'n_atuacoes') %>% \n  # adicionando o indicador de senioridade - como os resultados estao na mesma ordem dos\n  # respondentes,  basta copiar a informação do dataframe original para cá\n  mutate(P2_g = df_atuacao$P2_g) %>% \n  # criando a figura per se\n  ggplot(mapping = aes(x = n_atuacoes, y = P2_g, fill = P2_g)) +\n  geom_density_ridges2(quantile_lines = TRUE, quantiles = 2, vline_color = 'grey90', \n                       scale = 0.9, show.legend = FALSE) +\n  scale_fill_manual(values = cores_data_hackers) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +\n  labs(\n    title    = 'Quantas atuações os respondentes têm por nível de senioridade?',\n    subtitle = 'Parece existir uma diferença na quantidade de atuações de acordo com a senioridade da pessoa,\\nmuito embora exista uma grande variabilidade dentro de um mesmo nível de senioridade',\n    x        = 'Quantidade de atuações do respondente',\n    y        = 'Senioridade do respondente'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_respondentes_atuacoes-1.png){width=672}\n:::\n:::\n\n\nAs informações apresentadas nas duas figuras acima podem ser combinadas em uma matriz de calor para buscarmos entender como cada uma das atuações estão associadas às pessoas de acordo com seu nível de senioridade e dependendo de quantas atuações elas disseram ter. Neste caso, as colunas desta matriz representam cada uma das 129 atuações que estamos analisando, ordenadas da esquerda para a direta da atuação mais comum para a mais incomum - _i.e._, Python está mais à esquerda, áudios mais à direta. Já as linhas desta matriz representam cada uma das pessoas respondentes, ordenadas de cima para baixo daquelas que têm a maior quantidade de atuações para àquelas com a menor quantidade. Finalmente, o preenchimento das linhas indica que àquela combinação de pessoa-atuação existe, e a cor do preenchimento está mapeada ao nível de senioridade da pessoa (utilizando o mesmo esquema de cores que temos usado até aqui). Podemos tirar três insights importantes dessa matriz:\n\n1. O primeiro insight está relacionado ao grau de aninhamento das atuações entre as pessoas - isto é, o quanto as atuações das pessoas que disseram ter a menor quantidade delas é um subconjunto daquelas que têm a maior quantidade. Se este aninhamento existisse, poderíamos esperar que o preenchimento da matriz se concentrasse totalmente na diagonal superior, o que claramente não é o caso. Logo, quais atuações as pessoas têm não parece estar relacionado à quantas atuações elas têm; \n2. Outro ponto importante é que se aquele aninhamento fosse dependente do nível de senioridade das pessoas, esperaríamos ver a parte superior desta diagonal preenchida com a cor mais escura (_i.e._, representando as pessoas de nível Sênior) e a parte inferior preenchida com a mais clara (_i.e._, representando as pessoas de nível Júnior). Isto sugeriria que a atuação das pessoas Júnior são um subconjunto daquele das pessoas Sênior o que, mais uma vez, parece não ser o caso; e,\n3. Esta matriz revela que existe uma grande substituição (_i.e._, _turnover_) das atuações entre as pessoas respondentes. Isto sugere que deve existir uma grande heterogeneidade na atuação entre as pessoas cientistas de dados que responderam a pesquisa _State of Data Brasil 2021_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# criando uma matriz para visualizar a variação na marcação\ndf_atuacao %>% \n  # passando as informações sobre a atuação do formato largo para o formato longo - i.e., cada\n  # uma das atuações para cada pessoa respondente passa a estar nas linhas ao invés de nas colunas\n  pivot_longer(cols = -c(P0, P2_g), names_to = 'atuacao', values_to = 'flag') %>% \n  # agrupando o dataframe pelo nivel de atuacao da pessoa respondente\n  group_by(atuacao) %>% \n  # somando a quantidade total de pessoas respondentes que disseram ter cada uma das atuações\n  # isso acabará replicando esse total para cada uma das atuações, mas o intuito é esse mesmo\n  # pois usaremos essa informação mais abaixo para ordenar as atuações daquela com maior número\n  # de respondentes para a com o menor número\n  mutate(\n    total_atuacao = sum(flag)\n  ) %>% \n  # regrupando o dataframe pelo identificador de cada pessoa respondente\n  group_by(P0) %>% \n  # somando a quantidade total de atuação que cada pessoa respondente disser ter. Isso também\n  # acabará replicando esse total entre todas as linhas de um dado respondente mas, novamente, \n  #  usaremos essa informação mais abaixo para ordenar as pessoas respondentes daquelas com o\n  # maior número de atuações para a com o menor número\n  mutate(\n    total_respondente = sum(flag)\n  ) %>% \n  # quebrando a estrutura de grupos do dataframe\n  ungroup %>% \n  # preparando os dados para criar a matriz de respondentes por atuação\n  mutate(\n    # definindo a ordem dos respondentes de acordo com a quantidade total de atuações que cada\n    # uma dessas pessoas marcou\n    P0       = fct_reorder(.f = P0, .x = total_respondente),\n    # definindo a ordem das atuações de acordo com a quantidade total de pessoas respondentes\n    # que disseram ter aquele tipo de atuação\n    atuacao  = fct_reorder(.f = atuacao, .x = total_atuacao, .desc = TRUE),\n    # codificando uma coluna para carregar o mapa de cores do preenchimento da matriz de acordo\n    # com o nível de senioridade da pessoa respondente\n    fill_col = ifelse(test = flag == 1, yes = P2_g, no = 'Vazio')\n  ) %>% \n  # criando a figura per se\n  ggplot(mapping = aes(x = atuacao, y = P0, fill = fill_col)) +\n  geom_tile(show.legend = FALSE) +\n  scale_fill_manual(values = c(cores_data_hackers, 'white')) +\n  labs(\n    title    = 'De que forma a atuação variou entre os respondentes?',\n    subtitle = 'Existe uma grande heterogeneidade nas atuações que cada respondente têm, que não parece estar\\nassociada ao seu nível de senioridade ou à quantidade de atuações que as pessoas têm',\n    caption  = 'As linhas da matriz estão ordenadas de cima para baixo, do respondente com maior número de atuações para aquele\\nde menor. De forma similar, a ordenação da matriz da esquerda para a direita representa àquelas atuações que foram\\nselecionadas por quase todos os respondentes para àquelas que foram pouquíssimo selecionadas. As cores representam\\nos respondentes nos três níveis de atuação, seguindo a mesma paleta utilizada anteriormente.',\n    x        = 'Atuação',\n    y        = 'Pessoa respondente'\n  ) +\n  theme(\n    axis.text = element_blank(),\n    axis.line = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_matriz_atuacao-1.png){width=672}\n:::\n:::\n\n\nUma forma mais direta de testar o quão heterogênea é a atuação dos cientistas de dados dentro e entre os níveis de senioridade é através de uma Análise de Dispersão Multivariada de Grupos (_i.e._, PERMDISP: Permutational Multivariate Analysis of Group Dispersions). Esta análise é o análogo multivariado do teste de Levene para homogeneidade de variâncias, e testa a hipótese nula de que a distância entre o centróide de cada grupo e as instâncias associadas à ele é homogênea entre os grupos. Posto de outra forma, e simplificando, esta análise determina se a diversidade de atuações dentro de cada nível de senioridade é similar entre os níveis e, caso negativo, em qual deles as atuações são mais diferentes entre as pessoas. Para essa finalidade, utilizei a distância de Jaccard para quantificar a dissimilaridade de atuações entre as pessoas, uma vez que todas estão atuações são indicadores binários (_i.e._, presença ou ausência) e àquela métrica de distância normalmente tem um bom ajuste a este tipo de dado^[Cheguei a utilizar outras métricas de distância, mas os resultados foram praticamente os mesmos e, então, resolvi ficar com a opção mais simples mesmo.]. O output da análise pode ser visualizado através do biplot de uma ACoP (_i.e._, Análise de Coordenadas Principais) onde podemos projetar cada uma das instâncias, os centróides de cada grupo (_i.e._, nível de senioridade) e o polígono convexo que contém todas as instâncias de cada um deles: se existir uma diferença entre os níveis de senioridade, então o polígono convexo de pelo menos um deles deve ter um tamanho diferente dos demais (_i.e._, tamanho do polígono = heterogeneidade do grupo). \n\nOs resultados dessa análise sugerem que as pessoas de nível Júnior que responderam à pesquisa têm uma diversidade de atuações maior do que àquelas de nível Pleno e Sênior (_i.e._, baseado no p-valor da análise), mas sem diferença entre estes dois últimos^[Esse resultado pode ser encontrado dentro do objeto `permutacao`, e suprimi ele no texto somente para fins de claridade.]. Contudo, se explorarmos o tamanho do efeito (_i.e._, a distância média para o centróide de cada grupo)^[Esse resultado pode ser encontrado dentro do objeto `analise_dispersao`, e suprimi ele no texto somente para fins de claridade.] e o tamanho dos polígonos convexos de cada grupo, podemos ver que esta diferença não é tão significativa assim. Existe outro insight importante que podemos tirar através da análise do biplot em si: se existirem diferenças nas atuações entre os níveis de senioridade, então os centróides de cada grupo devem estar distantes uns dos outros. Nesse sentido, podemos observar que as atuações das pessoas de nível Júnior  parecem diferir daquelas das pessoas de nível Pleno e Sênior mas, novamente, não foi clara a diferença entre estes dois últimos. \n\n\n::: {.cell fig.dpi='140'}\n\n```{.r .cell-code}\n# calculando a dissimilaridade de atuação entre todos os respondentes da pesquisa, utilizando\n# a dissimilaridade de Jaccard\nmatriz_distancia <- vegdist(x = select(df_atuacao, -c(P0, P2_g)), method = 'jaccard')\n\n# implementando uma análise de dispersão do modo de atuação de acordo com a senioridade da\n# pessoa respondente\nset.seed(33)\nanalise_dispersao <- betadisper(d = matriz_distancia, group = df_atuacao$P2_g, type = 'centroid')\n\n# pegando o resultados do teste de permutação\nset.seed(42)\npermutacao <- permutest(x = analise_dispersao, pairwise = TRUE)\nndf <- permutacao$tab$Df[1]\nddf <- permutacao$tab$Df[2]\nFval <- round(x = permutacao$tab$F[1], digits = 2)\npval <- round(x = permutacao$tab$`Pr(>F)`[1], digits = 4)\n\n# criando uma figura para visualizar a dispersão da forma de atuação dos respondentes de\n# acordo com o seu grau de senioridade\n## extraindo os escores da posição dos respondentes na ordenação da PCoA\nescores_respondentes <- scores(x = analise_dispersao, display = 'sites') %>% \n  # parseando a matriz de escores para um dataframe, uma vez que a classe de objeto \n  # resultante não interage bem com o tidyverse\n  data.frame %>% \n  # colocando a senioridade do respondente como uma coluna no dataframe - usaremos essa\n  # informação para mapear as cores dos pontos à senioridade do respondente; além disso,\n  # como o input e o output da função estão alinhados, basta pegar a coluna de senioridade\n  # do input e copiar ela para dentro deste output\n  mutate(P2_g = df_atuacao$P2_g)\n\n# levantando os dados necessários para desenhar o convex hull ao redor de cada nivel de\n# senioridade no gráfico de dispersão\npoligonos_senioridade <- escores_respondentes %>% \n  # agrupando o dataframe pelo nivel de senioridade, de forma a obtermos o convex hull para\n  # cada nivel de senioridade\n  group_by(P2_g) %>% \n  # pegando as instância que podem ser usadas para desenhar o convex hull do nível de senioridade\n  slice(chull(PCoA1, PCoA2))\n\n# extraindo as coordenadas da posição dos centroides relacionados à cada um dos níveis\n# de senioridade\nposicao_centroides <- scores(x = analise_dispersao, display = 'centroids') %>% \n  # parseando a matriz de escores para um dataframe, uma vez que a classe de objeto \n  # resultante não interage bem com o tidyverse\n  data.frame %>% \n  # adicionando o string com a senioridade ao dataframe - essa informação está como\n  # rowname do dataframe\n  rownames_to_column(var = 'P2_g')\n\n# criando a figura per se\nggplot(data = escores_respondentes, mapping = aes(x = PCoA1, y = PCoA2, shape = P2_g, fill = P2_g)) +\n  geom_hline(yintercept = 0, color = 'grey80') +\n  geom_vline(xintercept = 0, color = 'grey80') +\n  geom_polygon(data = poligonos_senioridade, alpha = 0.05, color = NA) +\n  geom_point(size = 2, alpha = 0.3, color = 'white') +\n  # adicionando o centroide da distribuição da dispersão dos respondentes na figura, que\n  # ficará marcando com uma estrela\n  geom_point(\n    data = posicao_centroides,\n    mapping = aes(x = PCoA1, y = PCoA2, fill = P2_g, shape = P2_g), size = 4, color = 'black'\n  ) +\n  # adicionando o texto associando o centróide de cada grupo de respondentes à sua senioridade\n  geom_text(\n    data = posicao_centroides,\n    mapping = aes(x = PCoA1, y = PCoA2, label = P2_g, color = P2_g), size = 3, hjust = 1.4, fontface = 'bold'\n  ) +\n  scale_shape_manual(values = c(21, 22, 24)) +\n  scale_color_manual(values = cores_data_hackers) +\n  scale_fill_manual(values = cores_data_hackers) +\n  labs(\n    title    = 'Quão diferente é a atuação dos Cientistas de Dados?',\n    subtitle = 'A atuação das pessoas é heterogênea dentro e entre os níveis de senioridade',\n    caption  = glue('Análise de Dispersão Multivariada de Grupos: ndf = {ndf}; ddf = {ddf}; Pseudo-F = {Fval}; p = {pval}.'),\n    x        = 'PCoA #1',\n    y        = 'PCoA #2'\n  ) +\n  theme(\n    legend.position = 'none',\n    axis.line = element_blank(),\n    plot.title    = element_text(size = 10),\n    plot.subtitle = element_text(size = 8),\n    plot.caption  = element_text(size = 6),\n    axis.title.x  = element_text(size = 8),\n    axis.title.y  = element_text(size = 8),\n    axis.text     = element_text(size = 8)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_betadisper-1.png){width=480}\n:::\n:::\n\n\nEsta breve análise exploratória nos traz duas mensagens principais. A primeira delas é que parece ser mais fácil diferenciar um profissional Júnior de um Sênior do que um Sênior de um Pleno. A outra mensagem é que a heterogeneidade nas atuações dentro dos níveis de senioridade é tão grande que parece ser comum que uma pessoa de nível Júnior em um contexto pudesse ser a Sênior em outro, e vice-versa. Acredito que nenhuma dessas mensagens seja inédita ou surpreendente para quem está atuando na área, mas é interessante ver elas representadas através dos dados. De toda forma, isto reforça a necessidade de tentarmos definir de que forma o mercado se orienta para definir o perfil de atuação das pessoas cientistas de dados entre os níveis de senioridade. \n\n# O que mais diferencia os profissionais entre os níveis de senioridade no mercado de trabalho brasileiro?\n\nVou tentar responder à essa pergunta fazendo o feijão com arroz que nós, cientistas de dados, estamos acostumados: usando um algoritmo de aprendizado de máquina para tentar prever o nível de senioridade das pessoas que responderam a pesquisa _State of Data 2021_. Mais especificamente, vamos analisar o peso que o modelo dará à cada uma das atuações gerais e específicas, bem como ao contexto de trabalho no qual as pessoas estão inseridas, para predizer o senível de senioridade. No entanto, antes de seguir, acredito que seja importante ter duas considerações em mente. Uma delas é que construiremos esse modelo utilizando apenas uma amostra dos cientistas de dados que atuam no mercado brasileiro - _i.e._, aqueles que responderam a pesquisa - e, portanto, não temos como saber o quanto ele pode generalizar de fato para o mercado como um todo. A segunda é que não devemos tomar uma relação entre _e.g._ uma atuação e um nível de senioridade como boa ou ruim...na realidade, essas relações só nos dirão que dentro da amostra de pessoas que responderam a pesquisa é mais frequente vermos aquele nível de senioridade associado (ou não) àquela atuação, e é isso.\n\nCom isso em mente, vamos organizar a base que utilizaremos para o ajuste do modelo preditivo, além de derivar algumas informações à partir de outras já existentes. Note que não devemos incluir nenhuma informação que seja consequência do nível de senioridade (_e.g._, salário e tempo de atuação), além de eu ter optado por não considerar informações demográficas sobre a pessoa (_e.g._, idade, gênero, UF onde mora) pois eu realmente acredito que estas não devem e não podem ser alavancas para definir a senioridade de uma pessoa. Em suma, vamos incluir 6 informações adicionais além daquelas 129 atuações que já estamos analisando, de acordo com as seguintes hipóteses associadas:\n\n+ Pode ser que as empresas tenham diferentes graus de maturidade para entender o que é um cientista de dados e/ou alinhar o cargo à sua atuação _per se_ (_e.g._, a pessoa atua com desenvolvimento de _front-end_, mas tem o cargo de cientista de dados). Assim, substituí a variável que informa o cargo da pessoa (_i.e._, pergunta `P2_f`) por uma indicador se ela tem ou não o cargo de `Cientista de Dados`. Com isso, poderemos avaliar de forma bem simples se é mais provável que pessoas que tenham esse cargo estejam mais frequentemente associadas a um nível de senioridade específico;    \n+ É possível que a definição do nível de senioridade varie entre os tipos de indústria, de forma que o que é Sênior em uma indústria pode ser o Júnior em outra. Assim, resolvi criar uma variável com base na pergunta `P2_b`, codificando o tipo de indústria no qual a pessoa atua: `indeterminado` se a resposta à pergunta é `NA` ou foi 'Outro', setor de `indústria` se a sua resposta foi uma entre agronegócios, construção, energia, indústria, setor alimentício ou setor automotivo ou, caso nenhuma das anteriores, setor de `serviços` (_e.g._, consultoria, entretenimento, bancos,...);  \n+ Já escutei algumas discussões de que o Sênior em uma _start-up_ pode ser bem diferente daquele de uma _big tech_. Para representar essa ideia, optei por recodificar a informação da pergunta `P2_c` que fala sobre a quantidade de funcionários que a empresa da pessoa têm usando os [critérios do IBGE](https://conube.com.br/blog/como-definir-o-porte-da-empresa/#:~:text=IBGE%20%E2%80%93%20Por%20n%C3%BAmero%20de%20colaboradores&text=Micro%3A%20com%20at%C3%A9%2019%20empregados,Grande%3A%20mais%20de%20500%20empregados): `microempresa`, `pequeno porte`, `médio porte` e `grande porte` (além de `desconhecido` quando esta informação estava faltando). Existem critérios distintos para o setor da indústria e o de comércio e serviços, mas como a grande maioria das pessoas que responderam à pesquisa atua no último, resolvi utilizá-lo como base;  \n+ Está bem claro à essa altura que você não precisa ter um diploma acadêmico ou um doutorado para atuar na área de ciência de dados. No entanto, também é consenso de que uma qualificação de nível superior pode ajudar bastante não só na empregabilidade como também na forma como você acaba pensando e abordando os problemas de dados. Desta forma, pode ser o caso que pessoas com um grau de qualificação maior acabem atuando de forma diferenciada e isso, por sua vez, facilite a sua escalada de carreira (_e.g._, fazendo entregas acima da média)^[Sei que esse tema pode ser controverso e, assim como na academia, nem toda pessoa com um diploma de baixo do braço tem o mesmo grau de preparação e prontidão que um par seu...todavia, acho importante considerar este aspecto de alguma forma nas análises pois cada vez mais vemos pessoas saindo da academia e indo para o mercado de trabalho.]. Tendo isso em vista, aproveitei quase que totalmente as respostas à pergunta `P1_h`, só recodificando os casos em que assumi que poderia dizer que a pessoa não tem um diploma (_i.e._, estudantes de graduação, quem não tem uma graduação formal e quem optou por não responder); e,\n+ Vimos que parece existir uma variação natural na quantidade de atuações que as pessoas disseram ter, e que isso pode estar relacionado ao seu nível de senioridade. Apesar dessa informação não ser uma alavanca muito acionável, acredito que seja importante considerar uma variável que quantifique o total de atuações gerais e outra o total de atuações específicas de cada pessoa. Eu espero que elas acabem sendo colineares mas, através delas, podemos avaliar se _e.g._ o importante para o profissional Júnior é ter muito das atuações gerais e, ao progredir para os próximos níveis, ter cada vez mais atuações específicas. Dada essa possibilidade (e curiosidade), achei melhor manter essas variáveis na análise.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# criando a base analítica a partir da base de dados original da pesquisa, seguindo \n# a lógica de feature engineering explicada no texto\ndf_base_analitica <- df %>% \n  # pegando apenas as observações das pessoas que responderam ter uma atuação que \n  # reflete a de uma pessoa cientista de dados\n  filter(P4_a == 'Ciência de Dados') %>% \n  # codificando features a partir dos dados disponíveis na base de dados original\n  mutate(\n    # adicionando um indicador para mapear se a pessoa respondente possui o título \n    # de cientista de dados no cargo ou não\n    has_role = case_when(\n      str_detect(string = P2_f, pattern = 'Cientista de Dados') ~ 1L,\n      TRUE ~ 0L\n    ),\n    # agrupando o setor de atuação dos respondentes em torno de três categorias \n    # mais simples - Indústria, Comércio e Serviços (i.e., só Serviços) e Indeterminada\n    tipo_industria = case_when(\n      str_detect(string = P2_b, pattern = 'Agronegócios|Construção|Energia|Indústria|Alimentício|Automotivo') ~ 'Indústria',\n      is.na(P2_b) | P2_b == 'Outro' ~ 'Indeterminado',\n      TRUE ~ 'Serviços'\n    ),\n    # mapeando o tamanho da empresa da pessoa respondente às categorias definidas pelo\n    # IBGE de acordo com o número de colaboradores - simplificando e utilizando a categoria\n    # 'Comércio e Serviços', dado que a maior parte dos respondentes de Ciência de Dados\n    # está nessa área\n    tamanho_empresa = case_when(\n      is.na(P2_c) ~ 'Desconhecido',\n      P2_c %in% c('de 1 a 5', 'de 6 a 10') ~ 'Microempresa',\n      P2_c %in% c('de 11 a 50') ~ 'Pequeno porte',\n      P2_c %in% c('de 51 a 100') ~ 'Médio porte',\n      TRUE ~ 'Grande porte'\n    ),\n    # juntando tudo o que é grau de formação onde a pessoa ainda não tem um diploma de \n    # ensino superior (ou preferiu) não informar em uma categoria só\n    instrucao = case_when(\n      P1_h %in% c('Estudante de Graduação', 'Não tenho graduação formal', 'Prefiro não informar') ~ 'Sem diploma',\n      TRUE ~ P1_h\n    )\n  ) %>% \n  # pegando apenas as colunas que têm as informações que utilizaremos para a modelagem\n  select(P0, P2_g, has_role, tamanho_empresa, instrucao, tipo_industria, \n         matches('P4_[cdfgh]_'), matches('P8_[abcd]_')) %>% \n  # passando todas as colunas que estão como double para integer\n  mutate(across(where(is.double), as.integer)) %>% \n  # dropando qualquer linha com NA\n  drop_na() %>% \n  # agrupando o tibble linha a linha\n  rowwise() %>%\n  # calculando a quantidade de respostas gerais e especificas de cada pessoa respondente\n  mutate(\n    respostas_geral = sum(c_across(contains('P4'))),\n    respostas_especificas = sum(c_across(contains('P8')))\n  ) %>%\n  # quebrando o agrupamento das linhas\n  ungroup\n```\n:::\n\n\nO próximo passo será estabelecer o fluxo de ajuste do modelo aos dados. Primeiro, vou usar as mesmas instâncias da análise exploratória de dados para estabelecer a base de treino (326 pessoas respondentes), e deixarei o restante delas na base de teste (83 pessoas; precisei remover 1 delas da base de dados pois ela tinha todas as informações sobre a atuação como `NA`). Também vou preparar um esquema de validação cruzada com 5 divisões, de forma a avaliarmos a performance média do modelo entre elas durante o seu treinamento. Na sequência, vou preparar um pipeline de pré-processamento dos dados onde vamos: (a) fazer o _one-hot-encoding_ de todas as variáveis categóricas que criamos, (b) adicionar interações entre o tamanho da empresa e o tipo de indústria^[A interação entre o tamanho da empresa e o tipo de indústria testará a possibilidade de que o nível de senioridade da pessoa difere entre os tipos de indústria para um mesmo tamanho de empresa.], e de cada uma destas com o fato da pessoa possuir ou não o cargo de cientista de dados^[A interação entre o tamanho da empresa e a existência de um cargo de cientista de dados testará a possibidade de que a importância do cargo não é a mesma entre empresas _e.g._ de pequeno e grande porte. De forma similar, a interação entre o tipo de indústria e posse do cargo de cientista de dados testará a possibilidade de que a importância do cargo para a definição do nível de senioridade da pessoa varia de um tipo de indústria para a outra.], (c) removeremos então qualquer variável preditora que tenha pelo menos 97% de seus valores iguais (_i.e._, quase tudo 0 ou 1 - efetivamente, estamos removendo àquelas atuações que menos de 12 pessoas disseram ter) e, finalmente, (d) vamos padronizar as duas variáveis quantitativas que incluímos no modelo (_i.e._, quantidade total de atuações gerais e de atuações específicas). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fazendo o split da base analitica usando a mesma divisão da base em treino e teste do que aquela\n# utilizada na análise exploratória de dados - para isso, usaremos o make_splits, que levará como\n# argumentos os índices das instâncias que estavam na base de treino utilizada na EDA para criar \n# o split de dados baseados nas mesmas instâncias, garantindo que não há leakage da EDA para a \n# modelagem\nsplit_dos_dados <- make_splits(\n  x = list(\n    analysis = which(df_base_analitica$P0 %in% df_atuacao$P0), \n    assessment = which(!df_base_analitica$P0 %in% df_atuacao$P0)\n  ),\n  data = df_base_analitica\n)\n\n# criando folds para validacao cruzada\nset.seed(42)\nskfolds <- vfold_cv(data = training(split_dos_dados), v = 5, strata = P2_g)\n\n# criando a receita de preprocessamento dos dados\n## criando uma receita através da qual vamos modelar a senioridade da pessoa respondente com base em\n## todas as informações disponíveis na base analítica - usando o split de treino para criar a receita\n## de pre-processamento (e que só será treinada de fato quando rodarmos cada um dos modelos)\npre_processamento <- recipe(P2_g ~ ., data = training(split_dos_dados)) %>% \n  # parseando a coluna P0 como o identificador único de cada pessoa respondente\n  update_role(P0, new_role = 'id') %>% \n  # fazendo o one hot encoding das variáveis categóricas com mais de um nível \n  step_dummy(tamanho_empresa, instrucao, tipo_industria, one_hot = TRUE) %>% \n  # adicionando uma interação entre o tamanho da empresa e o tipo de industria, de forma a testar a \n  # hipotese que o nivel de senioridade exigido difere de acordo com o tipo de ramo no qual a empresa\n  # da pessoa respondente atua\n  step_interact(terms = ~ starts_with('tamanho_empresa'):starts_with('tipo_industria')) %>%\n  step_interact(terms = ~ starts_with('tamanho_empresa'):has_role) %>% \n  step_interact(terms = ~ starts_with('tipo_industria'):has_role) %>% \n  # removendo todas as variáveis preditoras que têm a variância muito baixa - i.e., são todas variáveis\n  # categóricas com um viés grande demais para uma das respostas (e.g. ou quase tudo 1 ou quase tudo 0)\n  step_nzv(all_predictors(), freq_cut = 97/3) %>% \n  # normalizando todas as variáveis preditoras que, embora sejam categóricas, faz com que possamos \n  # comparar o impacto de cada uma delas considerando a frequência com a qual ocorrem\n  step_normalize(respostas_geral, respostas_especificas)\n```\n:::\n\n\nUma vez que tenhamos feito todo o trabalho de preparação dos dados, vamos agora à etapa de seleção do algoritmo. Optei por utilizar uma regressão multinomial com regularização L1 e L2 para resolver o problema de previsão do nível de senioridade da pessoa cientista de dados, por três principais motivos: é um algoritmo bastante simples, a regularização ajudará a lidar com a grande quantidade de variáveis preditoras e, o mais importante, temos acesso à explicabilidade de suas decisões através dos `betas` da regressão - que, nesse caso, representam o logarítimo da razão de probabilidade do impacto de cada variável preditora sobre o nível predito (_i.e._, quão mais provável é que uma pessoa que tenha àquela atuação ou característica esteja associada aquele nível de senioridade do que uma outra pessoa que não os tenha). Definido o algoritmo, vou juntá-lo ao pipeline de pré-processamento de dados para que todas àquelas etapas ocorram na sequência correta dentro de cada _fold_ da validação cruzada durante o treinamento desse algoritmo. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# criando uma instância do modelo de regressão multinomial usando a engine do glmnet e deixando os dois\n# hiperparametros principais para a otimização bayesiana - equivalente ao C e ao l1_ratio do sklearn\nregressao_multinomial <- multinom_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(engine = 'glmnet', standardize = FALSE) %>% \n  set_mode(mode = 'classification')\n\n# criando workflow para a regressão multinomial - seria a mesma coisa que criar uma instância do pipeline\n# do sklearn, empacotando o pre-processamento e o algoritmo em um objeto só\npipeline <- workflow() %>% \n  add_recipe(recipe = pre_processamento) %>% \n  add_model(spec = regressao_multinomial)\n```\n:::\n\n\nUma vez que este pipeline esteja consolidado, realizaremos uma otimização dos hiperparâmetros da regressão multinomial - os valores que definem a intensidade da regularização L2 (`penalty`) e L1 (`mixture`) - utilizando uma busca bayesiana. Utilizaremos essa buscar para tentar encontrar a combinação daqueles dois hiperparâmetros que forneça o modelo cujas probabilidades preditas estejam o mais bem calibradas com o nível de senioridade observado (_i.e._, que a probabilidade seja 1 para o nível de senioridade que a pessoa de fato tem e 0 para aqueles que não é o dela). Para isso, vou focar em otimizar a entropia cruzada do modelo (`log loss`), de forma a obter o modelo que nos forneça o menor valor para esta métrica; além disso, vou aproveitar para monitorar os valores de AUC de cada modelo ajustado durante esse processo. Uma característica interessante da busca bayesiana é que ela explora o espaço de hiperparâmetros de forma à selecionar àquelas combinações que têm maior expectativa de gerar um bom resultado, de acordo o histórico de exploração de cada iteração anterior. Com isso, aumentamos as chances de encontrar um bom modelo de forma mais rápida do que se explorássemos todo o espaço _e.g._ de forma aleatória. Finalmente, uma vez que tenhamos descoberto os valores daqueles hiperparâmetros usando a validação cruzada, vamos empregá-los para treinar o modelo mais uma vez usando todos os dados da base de treino e realizar a sua avaliação final na base de teste.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# implementando uma otimização bayesiana de hiperparâmetros para a regressão multinomial - usando o log\n# loss como métrica principal e monitorando o AUC. Este processo será paralelizado para ganharmos um \n# pouco mais de velocidade\ndoParallel::registerDoParallel()\nset.seed(42)\ngrid_search <- pipeline %>% \n  tune_bayes(\n    resamples = skfolds, iter = 50, metrics = metric_set(mn_log_loss, roc_auc), initial = 15,\n    control = control_bayes(no_improve = 10, seed = 42, verbose = TRUE, parallel_over = 'resamples')\n  )\n\n# extraindo a melhor combinação de hiperparâmetros do estimador a partir do grid search e finalizando o \n# pipeline com esta combinação \nmelhor_estimador <- pipeline %>% \n  finalize_workflow(parameters = select_best(x = grid_search, metric = 'mn_log_loss'))\n\n# treinando o melhor estimador na base de treino e escorando ele na base de teste\nmodelo_treinado <- melhor_estimador %>% \n  last_fit(split = split_dos_dados, metrics = metric_set(mn_log_loss, roc_auc))\n\n# extraindo o valor das métricas de avaliação do modelo definidas acima\nmetricas_no_teste <- modelo_treinado %>% \n  collect_metrics() %>% \n  # pegando apenas a coluna com o nome da metrica e o seu valor\n  select(.metric, .estimate) %>% \n  # colocando cada uma das metricas como uma coluna em um tibble\n  pivot_wider(names_from = .metric, values_from = .estimate)\n# printando as metricas do modelo na base de teste em uma tabela\nrmarkdown::paged_table(x = metricas_no_teste)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mn_log_loss\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"roc_auc\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.9662006\",\"2\":\"0.7337173\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nO valor de AUC do modelo que treinamos sugere que ele é capaz de fornecer uma probabilidade maior para o nível de senioridade correto da pessoa em 73.37% dos casos, o que não parece ser nada mal. Podemos ter um pouco mais de entendimento sobre as suas predições se observarmos a matriz de confusão da classificação feita pelo modelo na base de teste, conforme apresentado abaixo. Em todos os casos, o modelo foi capaz de classificar corretamente a maior parte das pessoas ao seu nível de senioridade; por outro, podemos ver também que o tipo de erro mais comum cometido pelo modelo foi associar a pessoa ao nível de senioridade imediatamente acima (_e.g._, profissional Júnior classificado como Pleno) ou abaixo (_e.g._, profissional Sênior classificado como Pleno) do dela, quase não cometendo erros mais grosseiros (_e.g._, profissional Sênior classificado como Júnior). Com isso, parece que temos um modelo que pode ser útil para traçarmos o perfil médio de uma pessoa cientista de dados que atua no mercado brasileiro (e que respondeu a pesquisa, não perca isso de vista).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extraindo a matriz de confusão do modelo treinado e plotando a figura\nmodelo_treinado %>% \n  # extraindo o pipeline com o modelo ja treinado\n  extract_workflow() %>% \n  # adicionando as previsões à base de teste\n  augment(new_data = testing(split_dos_dados)) %>% \n  mutate(P2_g = as.factor(P2_g)) %>% \n  # extraindo a matriz de confusao a partir da base de teste\n  conf_mat(truth = P2_g, estimate = .pred_class) %>% \n  # extraindo a tabela que compõem a matriz de confusão a partir do objeto resultante\n  # de forma a utilizar essa informação para personalizar a plotagem da nossa matriz de confusão\n  pluck('table') %>%\n  # parseando o tipo de dado resultante para um dataframe\n  data.frame %>% \n  # adicionando informacoes para ajudar na criação da figura\n  mutate(\n    # adicionando indicadores para determinar se a previsão estava certo, se a previsão é de\n    # que a pessoa atua em um nível abaixo do qual ela está ou se ela está atuando em um nível\n    # acima do qual ela está\n    tipo_previsao = case_when(\n      Truth != Prediction ~ 'errou',\n      TRUE ~ 'acertou'\n    ),\n    # reordenando os niveis da coluna com as class labels da previsao de forma a ter um output\n    # mais similar à matriz de confusao do sklearn\n    Prediction = fct_rev(f = Prediction)\n  ) %>% \n  # criando a figura per\n  ggplot(mapping = aes(x = Truth, y = Prediction, fill = tipo_previsao, label = Freq)) +\n  geom_tile(mapping = aes(alpha = Freq), color = 'black', show.legend = FALSE) +\n  geom_text(fontface = 'bold') +\n  scale_fill_manual(values = c('deepskyblue1', 'tomato1')) +\n  labs(\n    title    = 'Como o modelo performou na base de teste?',\n    x        = 'Senioridade observada',\n    y        = 'Senioridade predita',\n    caption  = 'A diagonal principal (em azul) apresenta as classificações corretas feitas pelo modelo na base de teste, enquanto\\nque os demais valores (em vermelho) representam os erros do modelo. A transparência das cores remete à\\nquantidade de pessoas respondentes mapeadas à cada combinação de categorias preditas e observadas.'\n  ) +\n  theme(\n    axis.line = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_matriz_confusao-1.png){width=672}\n:::\n:::\n\n\nVamos definir o perfil médio da pessoa cientista de dados por nível de senioridade no mercado de trabalho brasileiro utilizando os betas da regressão multinomial que treinamos. Para facilitar essa análise e focar naquilo que teve maior peso para o modelo, vou olhar para as 10 variáveis com maior valor absoluto dos betas para cada nível de senioridade. Esses resultados podem ser visualizados através da figura abaixo, onde podemos ver que:  \n\n+ O profissional cientista de dados de nível **Júnior** muito provavelmente ainda não tem um curso de ensino superior completo, e tem como principal atividade em seu dia a dia a resolução de problemas de dados que passam pela utilização de modelos de séries temporais (que pode ser uma coisa bem difícil para quem está começando). Além disso, são pessoas que normalmente não utilizam bancos de dados relacionais e que não têm muita oportunidade de interagir com as diferentes tecnologias disponíveis na área de dados (_e.g._, bancos de dados, linguagens de programação, Cloud,...) e, tampouco, com as atuações mais relacionadas à ciência de dados em si;  \n+ O profissional de nível **Pleno** está atuando frequentemente em empresas de grande porte, principalmente quando elas oferecem o cargo de cientista de dados. Estes profissionais já tendem a ter um diploma de nível de superior e, mais importante, algum curso de pós-graduação _lato sensu_^[Não sei se é claro para todos, mas acredito que você precisa ter um diploma de nível superior completo para fazer uma pós-graduação; assim, dá para deduzir uma coisa pela outra.]. Em seu dia a dia, é bem provável que eles utilizem algumas ferramentas específicas da área de dados (_e.g._, bancos de dados MySQL e ferramentas de ETL populares no mercado), e atuem em etapas específicas dos projetos de ciência de dados - especificamente na etapa do entendimento do problema, no desenvolvimento de modelos estatísticos e de aprendizado de máquina e na apresentação desses resultados. Por outro lado, é pouco provável que estes profissionais utilizem ferramentas de visualização de dados com frequência (_e.g._, Tableau) ou que utilizem modelos de séries temporais para fazer suas entregas;  \n+ Por fim, é muito provável que profissionais de nível **Sênior** tenham um diploma de pós-graduação _stricto sensu_ (_i.e._, mestrado e/ou doutorado)^[Também não sei se é claro para todo mundo, mas você não precisa de um diploma de pós-graduação _lato sensu_ para fazer um _stricto sensu_ ou vice-versa; mas, mais comumente, você precisa de um diploma de mestrado para fazer o doutorado - e, em todos esses casos, você precisa ter uma graduação de ensino superior completa.]. É comum o uso de ferramentas de visualização de dados (_e.g._, Tableau, Power BI,...) e modelos de regressão entre estes profissionais, e eles parecem estar focando principalmente na resolução de problemas de dados relacionados à detecção de _churn_ (cujas particularidades podem fazer com que estes sejam problemas bem complexos de se trabalhar). Outro ponto importante é que estes profissionais conhecem e/ou usam uma ampla gama de tecnologias disponíveis na área de dados (_e.g._, diferentes tipos de linguagem de programação, utilizam difentes fontes de dados,...), mas não atuam necessariamente com uma quantidade grande de temas específicos à área de ciência de dados. É pouco provável que esses profissionais utilizem o serviço de Cloud da IBM, focando mais em soluções de Cloud que associadas ao processamento distribuído como o Databricks - o que também justifica o fato de eles pouco utilizarem planilhas para fazer as suas análises.      \n\n\n::: {.cell .preview-image .column-screen}\n\n```{.r .cell-code}\n# criando uma figura para analisar a variação na contribuição de cada feature para a previsão\n# de cada nível de senioridade\ndf_coeficientes <- modelo_treinado %>% \n  # extraindo o pipeline com o modelo ja treinado\n  extract_workflow() %>% \n  # extraindo os coeficientes do modelo treinado já dentro de um tibble\n  tidy %>% \n  # dropando o intercepto e qualquer feature cuja estimativa do slope tenha sido 0\n  filter(term != '(Intercept)', estimate != 0) %>% \n  # juntando o dicionario de id das perguntas para o texto das perguntas, de forma a\n  # nos ajudar a dicionar um texto mais informativo as features com maior impacto\n  # sobre cada nível de senioridade\n  left_join(y = dicionario, by = c('term' = 'pergunta_id')) %>% \n  # agrupando o tibble pelo nivel de senioridade para fazermos a operação seguinte\n  group_by(class) %>% \n  # extraindo as 10 features com maior impacto sobre a previsão de cada nivel de senioridade,\n  # removendo features sem contribuição desta lista\n  filter(estimate != 0) %>% \n  slice_max(order_by = abs(estimate), n = 10) %>% \n  # dropando o agrupando pelo nível de senioridade\n  ungroup %>% \n  # editando os textos de cada pergunta para torná-los mais apresentável na figura, além de\n  # ajustar a ordem das features dentro de cada nível de senioridade de forma a tornar mais\n  # clara a variação na contribuição de cada feature por nível\n  mutate(\n    # padronizando os textos de descrição criando uma nova coluna - colocando o nome da feature\n    # de acordo com aquele existente no dicionário quando ele existir ou o nome do termo quando\n    # a feature não existir naquele dicionario - e.g., as variáveis com os níveis de instrução,\n    # tipo e tamanho de empresa\n    termo = ifelse(test = is.na(texto), yes = term, no = texto),\n    # removendo todas as ocorrências de pontos nas strings dos termos, que neste caso estão sendo\n    # usados para representar o espaço entre palavras\n    termo = str_replace_all(string = termo, pattern = '\\\\.', replacement = ' '),\n    # tratando qualquer string relacionada às features de tamanho da empresa e tipo de indústria,\n    # de forma que o nível da categoria esteja mais claramente relacionada à feature em si (e.g.,\n    # 'Empresa de Grande Porte', ao invés da visualização trazer apenas 'Grande Porte')\n    termo = str_replace(string = termo, pattern = 'tamanho_empresa_', replacement = 'Empresa de '),\n    termo = str_replace(string = termo, pattern = 'tipo_industria_', replacement = 'Indústria de '),\n    # substituindo todos os prefixos relacionados à feature de instrução pelo verbo 'Ter', de forma\n    # a tornar mais intuitivo o que o essa feature quer dizer\n    termo = str_replace(string = termo, pattern = 'instrucao_', replacement = 'Ter '),\n    # adicionando o separador corretor para as ocorrências de pós-graduação e entre as opções de ter\n    # bacharelado ou graduação\n    termo = str_replace(string = termo, pattern = '(?<=Graduação)\\\\s', replacement = '/'),\n    termo = str_replace(string = termo, pattern = '(?<=Pós)\\\\s', replacement = '-'),\n    # tornando mais claro o significado das features relacionadas à quantidade de atuações gerais\n    # e específicas assinaladas por cada pessoa respondente\n    termo = str_replace_all(string = termo, \n                            pattern = 'respostas_geral', replacement = 'Quantidade de atuações gerais'),\n    termo = str_replace_all(string = termo, \n                            pattern = 'respostas_especificas', replacement = 'Quantidade de atuações específicas'),\n    # tratando exceção ao nível da feature de instrução onde o caso é não ter diploma de ensino superior\n    termo = str_replace_all(string = termo, \n                            pattern = 'Ter Sem diploma', replacement = 'Não ter diploma de ensino superior'),\n    # tornando mais clara o significado do string relacionado às interações entre ter o cargo de\n    # cientista de dados e as outras features\n    termo = str_replace_all(string = termo, \n                            pattern = '(.+)_x_has_role', replacement = 'Ter o cargo de DS em \\\\1'),\n    termo = str_replace(string = termo, pattern = 'has_role', replacement = 'Ter o cargo de DS'),\n    # removendo qualquer whitespace do início e/ou do fim de cada string\n    termo = str_squish(string = termo),\n    # adicionando quebras de linhas à cada strig para tornar mais fácil a visualização dos resultados\n    texto_final = str_wrap(string = termo, width = 55),\n    # ordenando os termos dentro de cada nível de senioridade de acordo com o valor dos betas,\n    # assim poderemos ter uma visão das features com impacto mais positivo para o mais negativo\n    # dentro de cada um dos níveis de senioridade\n    texto_final = reorder_within(x = texto_final, within = class, by = estimate)\n  )\n\n# criando a figura per se\nggplot(data = df_coeficientes, mapping = aes(x = estimate, y = texto_final, fill = class)) +\n  facet_wrap(~ class, scales = 'free') +\n  geom_vline(xintercept = 0) +\n  geom_col(color = 'black', show.legend = FALSE) +\n  scale_y_reordered() +\n  scale_fill_manual(values = cores_data_hackers) +\n  labs(\n    title    = 'Que atuações e contextos têm maior impacto para a previsão do nível de senioridade?',\n    subtitle = 'Estes são os 10 fatores mais fortemente relacionados à cada nível de senioridade das pessoas cientistas de dados que responderam à pesquisa State of Data 2021',\n    x        = 'Contribuição da feature (coeficientes da regressão)'\n  ) +\n  theme(axis.title.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_coeficientes_modelo-1.png){width=2400}\n:::\n:::\n\n\nEstes perfis dos profissionais entre os níveis de senioridade conversam bastante com o senso comum dentro da área de ciência de dados. Assim, é interessante o fato de que o modelo preditivo conseguiu capturar e representar estes padrões e, mais ainda, o de podermos identificar o que exatamente é frequentemente praticado em termos de atuação para cada nível de carreira dentro do mercado de trabalho brasileiro. De toda forma, os erros de classificação cometidos pelo modelo revelam que estes perfis não são uma verdade absoluta, e que existem algumas exceções ao que podemos esperar em média. Com isso, seria importante identificar que exceções são essas e, eventualmente, de que forma as pessoas poderiam agir para corrigi-las: seja para que o profissional tenha o reconhecimento de sua atuação no nível acima ao qual está, ou para aquele profissional mais experiente passe a ter uma atuação que não se confuda com àquela do nível abaixo.\n\n# Quais as exceções às previsões do modelo?\n\nO modelo que criamos cometeu dois tipos principais de erros: prever que uma pessoa está atuando no nível acima daquele no qual ela está atualmente (_i.e._, triângulo abaixo da diagonal principal na matriz de confusão: pessoa de nível Júnior que atua como se fosse de nível Pleno ou Sênior OU pessoa de nível Pleno que atua como se fosse Sênior) ou que ela está atuando no nível abaixo (_i.e._, triângulo acima da diagonal principal na matriz de confusão: pessoa de nível Sênior que atua como se fosse de nível Pleno ou Júnior OU pessoa de nível Pleno que atua como se fosse Júnior). Vou focar nestes dois grupos erros para facilitar as análises que faremos, fazendo referência a eles deste ponto em diante como 'Júnior classificado como Sênior' ou 'Sênior classificado como Júnior', respectivamente. \n\nPodemos imaginar que aqueles dois tipos de erro ocorrem principalmente porquê a pessoa respondente teve um conjunto de atuações e contextos de trabalho que as aproximou mais do comportamento médio do outro nível de senioridade que não o seu próprio, conforme aprendido pelo modelo. Consequentemente, uma forma do modelo acertar o nível de senioridade de cada uma dessas pessoas seria se pudéssemos mudar cada uma de suas atuações e contextos de forma a aproximá-la mais de cada outra pessoa que o modelo acertou a sua classificação. Todavia, explorar todas as combinações possíveis de mudanças para mapear quais delas levariam à classificação correta pode ser bastante ineficiente.\n\nUma outra forma de chegarmos a um resultado parecido seria o de olharmos cada uma das pessoas classificadas incorretamente pelo modelo, e buscarmos o seu par mais similar que tenha sido classificado corretamente. A intuição geral é que as diferenças observadas entre estas duas pessoas devem estar relacionadas ao erro de classificação, de forma que se pudermos identificar cada uma destas diferenças, podemos então entender o porquê do modelo ter errado a classificação - e, também, quais são as exceções ao perfis aprendidos pelo modelo.\n\nPara conseguir fazer isso, vamos implementar uma análise baseada no vizinho mais próximo, onde buscaremos o par de pessoas classificadas corretamente na base de treino e incorretamente na base de teste mais similares entre si. A lógica aqui é que todas as instâncias que estão na base de treino foram vistas pelo modelo durante o seu treinamento, de forma que os limiares de decisão foram aprendidos a partir destas instâncias. Além disso, se elas foram classificadas corretamente, podemos garantir que possuem as características necessárias para a previsão daquele nível de senioridade. Por outro lado, como nosso maior interesse é entender porquê do modelo não ter generalizado tão bem, o ideal é consultar as instâncias que estão na base de teste - que ele não viu durante o seu treinamento - e que foram classificadas de forma errada.\n\nCom isso em mente, separei os dados das pessoas classificadas corretamente na base de treino e das pessoas classificadas incorretamente na base de teste. A partir daí, olhei para cada uma dessas últimas e o seu nível de senioridade observado, calculei a similaridade do coseno entre suas características e as de cada outra pessoa no mesmo nível de senioridade que ela, e identifiquei àquela cujo valor da similaridade de coseno foi o maior. Isto fez com que eu conseguisse parear as características destas duas pessoas, definindo o que elas têm de similar e o que têm de diferente. Os resultados dessa análise podem ser observados na tabela abaixo.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code}\n# extraindo o identificador de cada pessoa respondente na base de treino no qual o modelo\n# conseguiu prever corretamente o seu nivel de senioridade\nids_acertos_treino <- modelo_treinado %>% \n  # extraindo o pipeline com o modelo ja treinado\n  extract_workflow() %>% \n  # adicionando as previsões do modelo à base de treino\n  augment(new_data = training(split_dos_dados)) %>% \n  # filtrando apenas as pessoas respondentes onde o modelo acertou o nível de senioridade\n  filter(P2_g == .pred_class) %>% \n  # extraindo o identificador único da pessoa respondente\n  pull(P0)\n\n# extraindo o identificador de cada pessoa respondente na base de teste no qual o modelo\n# errous a previsçao do nivel de senioridade\nids_erros_teste <- modelo_treinado %>% \n  # extraindo o pipeline com o modelo ja treinado\n  extract_workflow() %>% \n  # adicionando as previsões do modelo à base de teste\n  augment(new_data = testing(split_dos_dados)) %>% \n  # filtrando apenas as pessoas respondentes onde o modelo errou o nível de senioridade\n  filter(P2_g != .pred_class) %>% \n  # extraindo o identificador único da pessoa respondente\n  pull(P0)\n\n# extraindo os dados que utilizaremos para implementar o algoritmo de vizinhos mais proximos\n# esta base deve conter a informação das features já pré-processadas para as pessoas respondentes\n# em que o modelo acertou a previsão do nível de senioridade na base de treino e para àquelas\n# em que o modelo errou a previsão\ndf_comparacao <- modelo_treinado %>% \n  # extraindo as etapas de pre-processamento já treinados a partir do pipeline do modelo\n  extract_recipe() %>% \n  # aplicando o pre-processamento à toda a base analítica\n  bake(new_data = df_base_analitica) %>% \n  # corrigindo os valores da coluna de identificador único de cada pessoa, que vira NA para\n  # as instâncias na base de teste - como as duas bases estão alinhadas, basta sobre-escrever\n  # a coluna (isto ocorre pois o id é convertido é um fator, e como os identificadores na \n  # base de teste não estão contempladas como níveis conhecidos do fator, eles são convertidos\n  # em NA)\n  mutate(P0 = df_base_analitica$P0) %>% \n  # pegando apenas as pessoas respondentes que estejam na base de treino e tenham tido seu nível\n  # de senioridade previsto corretamente pelo modelo ou àquelas pessoas que estavam na base de\n  # teste e tiveram seu nível de senioridade previsto de forma errada pelo modelo\n  filter(P0 %in% c(ids_acertos_treino, ids_erros_teste)) %>% \n  # parseando o tibble para um data.frame, de forma a podermos dar nomes à cada uma de suas linhas\n  data.frame %>% \n  # colocando o identificador único de cada pessoa como o nome das linhas do data.frame - \n  # isso facilitará a indexação na função de vizinhos mais próximos que aplicaremos\n  `row.names<-`(value = .$P0)\n\n# criando funcao para calcular os vizinhos mais próximos de uma pessoa respondente da \n# base de teste que tenha sido classificada de forma errada com base nas pessoas da \n# base de treino que tenham sido classificadas corretamente\npessoa_mais_proxima <- function(pessoa) {\n  # pegando a informação da classe à qual a pessoa respondente no input está associada\n  classe_observada <- as.character(df_comparacao[pessoa, 'P2_g'])\n  \n  # pegando todas as instâncias da base de treino que foram classificadas corretamente\n  # e que também estão associadas à classe da pessoa respondente que serviu de input\n  # para a função - dropando o identificador e a classe das outras pessoas respondentes \n  # uma vez que essa informação é irrelevante para calcular a similaridade entre elas\n  instancias_na_classe <- df_comparacao %>%\n    filter(P0 %in% ids_acertos_treino, P2_g == classe_observada) %>%\n    select(-P0, -P2_g)\n  \n  # replicando a instância relacionada à pessoa respondente no input dessa função tantas\n  # vezes quantas forem as pessoas classificadas corretamente na base de treino. Esta \n  # estrutura de dados resultará em um data.frame com a linha da instância da pessoa do\n  # input da função replicada tantas vezes quanto houverem linhas em instancias_na_classe\n  instancia_errada <- df_comparacao[rep(x = pessoa, times = nrow(instancias_na_classe)),] %>%\n    select(-P0, -P2_g)\n  \n  # calculando a similaridade do coseno entre todas as pessoas classificadas na classe\n  # correta na base de treino e a pessoa classificada na classe errada na base de teste\n  # o resultado desta operação é um vetor nomeado, onde os nomes são os identificadores\n  # das outras pessoas naquela classe da pessoa input e que foram classificadas corretamente\n  # na base de treino, e os valores são a similaridade do coseno entre cada uma delas e\n  # as features da pessoa utilizada no input\n  similaridade_coseno <- rowSums(instancias_na_classe * instancia_errada) / \n    (sqrt(rowSums(instancias_na_classe ^ 2)) * sqrt(sum(instancia_errada[pessoa, ] ^ 2)))\n  \n  # passando o vetor nomeado para um tibble, onde uma coluna é o identificador de cada\n  # pessoa respondente classificada corretamente naquela classe na base de treino e a\n  # outra coluna é a similaridade entre ela e a pessoa respondente utilizada no input\n  enframe(x = similaridade_coseno, name = 'P0_acerto', value = 'similaridade') %>% \n    # retendo aquela pessoa que teve maior similaridade com a pessoa utilizada no input\n    slice_max(n = 1, order_by = similaridade) %>% \n    # adicionando uma coluna para codificar o identificador da pessoa respondente\n    # utilizada como input da função - i.e., àquela classificada na classe errada\n    mutate(P0_errado = pessoa) %>% \n    # reorganizando as colunas do tibble antes de retornar os resultados\n    relocate(P0_errado, .before = P0_acerto)\n}\n\n# aplicando a função aos dados, de forma a identificar quem é a pessoa classificada \n# corretamente na base de treino que é mais similar àquela classificada de forma errada\n# na base de teste\ndf_vizinhos <- map_dfr(.x = ids_erros_teste, .f = pessoa_mais_proxima) \n\n# criando uma versão mais longa da base analítica onde mapearemos as respostas de cada pessoa\n# respondente às perguntas feitas linha a linha, e utilizaremos estas informações para poder\n# identificar mais facilmente as perguntas cujas respostas diferem entre a pessoa que o modelo\n# errou a previsão na base de teste e a pessoa mais similar à ela mas que o modelo acertou a\n# previsão na base de treino\ndf_base_analitica_longa <- df_base_analitica %>% \n  # passando todas as colunas de inteiro para caracter, de forma a conseguirmos pivotear a \n  # base para o formato longo - se não fizermos isso, o `pivot_longer` explode uma mensagem\n  # de erro pois estaríamos misturando os data types\n  mutate(across(where(is.integer), as.character)) %>%\n  # passando a base para o formato longo, onde colocaremos as perguntas e as respostas de cada\n  # pessoa respondente nas linhas ao invés de nas colunas - isso aumentará muito a quantidade \n  # de linhas na base, mas precisaremos disso para fazer uns joins mais abaixo\n  pivot_longer(cols = -c(P0, P2_g), names_to = 'feature', values_to = 'valor') %>% \n  # ajustando o conteúdo de algumas colunas\n  mutate(\n    # ajustando o significado das features criadas uma vez que elas não constam no dicionario\n    feature_tidy = case_when(\n      feature == 'has_role' ~ 'Tem o cargo de DS',\n      feature == 'tamanho_empresa' ~ 'Tamanho da empresa',\n      feature == 'instrucao' ~ 'Nível de instrução',\n      feature == 'tipo_industria' ~ 'Tipo de indústria',\n      TRUE ~ feature\n    ),\n    # passando os valores 0 e 1 das dummies para o seu significado real quando for o caso, ou\n    # usando o valor da resposta quando relevante\n    valor_tidy = case_when(\n      valor == '1' ~ 'Sim',\n      valor == '0' ~ 'Não',\n      TRUE ~ valor\n    ),\n    # criando uma coluna de resposta, que conterá o nome da feature e o valor da resposta para\n    # cada linha - isso nos ajudará depois a mapear que resposta diferiu entre as duas pessoas\n    # mais facilmente\n    resposta = paste0(feature_tidy, ': ', valor_tidy)\n  ) %>% \n  select(-valor)\n\n# levantando os nomes das features que estão entre os top 10 dos maiores coeficientes do elasticnet\n# utilizado para modelar a senioridade. Aproveitaremos para remover as features relacionadas à \n# quantidade de respostas marcadas para as atuações gerais e específicas (uma vez que elas obviamente\n# vão variar entre as duas pessoas e esta não é uma alavanca acionável muito inteligente), além\n# de remover a parte do string que específica a o nível da categoria para as variáveis categóricas\n# que contém múltiplos níveis. Esse objeto guardará este resultado como um vetor, onde o texto com\n# o nome das features estará escrito de forma similar aquele da coluna `feature` na base analítica\n# que criamos no formato longo acima\ntop_features <- df_coeficientes %>%\n  filter(!term %in% c('respostas_geral', 'respostas_especificas')) %>% \n  pull(term) %>% \n  unique %>% \n  str_replace(pattern = '(?<=instrucao|tamanho_empresa|tipo_industria).+', replacement = '') %>%\n  unique\n\n# consolidando todos os contra-factuais para cada uma das pessoas respondentes em um data.frame só\ndf_contra_exemplos <- df_vizinhos %>% \n  # adicionando as features relacionadas às respostas dadas pela pessoa que o modelo errou a previsão\n  # na base de teste aos resultados da análise de vizinhança\n  left_join(y = df_base_analitica_longa, by = c('P0_errado' = 'P0')) %>% \n  # adicionando as features relacionadas às respostas dadas pela pessoa que o modelo acertou a previsão\n  # na base de treino aos resultados da análise de vizinhança\n  left_join(y = select(df_base_analitica_longa, -P2_g), \n            by = c('P0_acerto' = 'P0', 'feature'), suffix = c('_errado', '_acerto')) %>% \n  # adicionando a informação do nível de senioridade predito pelo modelo para as pessoas às quais\n  # a previsão este errada\n  left_join(y = modelo_treinado %>% \n              extract_workflow() %>% \n              augment(new_data = testing(x = split_dos_dados)) %>% \n              select(P0, .pred_class),\n            by = c('P0_errado' = 'P0')\n  ) %>% \n  # juntando o significado de cada pergunta à base de dados\n  left_join(y = select(dicionario, pergunta_id, texto), by = c('feature' = 'pergunta_id')) %>% \n  # adicionando novas informações à base de dados para ajudar na visualização\n  mutate(\n    # adicionando um indicador para sinalizar se a feature em questão está entre àquelas 10 \n    # features mais importantes identificadas pelo elasticnet entre todos os níveis de senioridade\n    top_feature = feature %in% top_features,\n    # codificando o tipo geral do erro de previsão: ou o modelo previu que pessoa performa acima \n    # do nível no qual ela tem (i.e., 'Júnior que atua como Sênior') ou abaixo dela (i.e., 'Sênior\n    # que atua como Júnior')\n    direcao_erro = case_when(\n      P2_g == 'Júnior' & .pred_class %in% c('Sênior', 'Pleno') ~ 'Júnior classificado como Sênior',\n      P2_g == 'Pleno' & .pred_class == c('Sênior') ~ 'Júnior classificado como Sênior',\n      P2_g == 'Sênior' & .pred_class %in% c('Júnior', 'Pleno') ~ 'Sênior classificado como Júnior',\n      P2_g == 'Pleno' & .pred_class == c('Júnior') ~ 'Sênior classificado como Júnior',\n    ),\n    # tratado o texto de descrição da feature - removendo whitespace do início e do fim do string,\n    # adicionando o nome tratado da feature quando ele estiver ausente (i.e., para as features que\n    # derivamos) e truncando o string de forma que apenas os 50 caracteres de cada um apareça\n    texto = str_squish(string = texto),\n    texto = ifelse(test = is.na(texto), yes = feature_tidy_acerto, no = texto),\n    resumo = str_trunc(string = texto, width = 60)\n  ) %>% \n  # removendo as features relacionadas à quantidade de atuações gerais e específicas, uma\n  # vez que essa não é uma alavanca acionável de forma inteligente\n  filter(!texto %in% c('respostas_geral', 'respostas_especificas'))\n\n# colocando os resultados da análise de vizinhança para cada pessoa na qual o modelo errou a\n# previsão na base de teste em uma tabela interativa para a análise\ndf_contra_exemplos %>%\n  # retendo apenas os registros de cada par de pessoas no qual existe uma diferença entre as \n  # respostas dadas para uma mesma pergunta\n  filter(resposta_acerto != resposta_errado) %>% \n  select(P0_errado, P2_g, .pred_class, resumo, valor_tidy_errado, valor_tidy_acerto, top_feature) %>%\n  mutate(top_feature = ifelse(test = top_feature, yes = 'Sim', no = 'Não')) %>%\n  reactable(\n    groupBy = 'P0_errado', \n    columns = list(\n      P0_errado         = colDef(name = 'Pessoa'),\n      P2_g              = colDef(name = 'Senioridade da pessoa', aggregate = 'unique'),\n      .pred_class       = colDef(name = 'Senioridade prevista', aggregate = 'unique'),\n      resumo            = colDef(name = 'Pergunta'),\n      valor_tidy_errado = colDef(name = 'Resposta foi...'),\n      valor_tidy_acerto = colDef(name = 'Resposta deveria mudar para...'),\n      top_feature       = colDef(name = 'Feature entre top 10?', defaultSortOrder = 'desc')\n    ),\n    showPageSizeOptions = TRUE, defaultPageSize = 3, borderless = TRUE, striped = TRUE, \n    highlight = TRUE, compact = TRUE, defaultSorted = 'top_feature', \n    style = list(fontSize = '12px')\n  )\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"reactable html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-142c8e44354d79c30301\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-142c8e44354d79c30301\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"P0_errado\":[\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"zo6oav14153wvnfzmqmzo6oav19cql86\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"xumrdoahwo5qcavzjpksrqzxumrdoahp\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"vqvp9ar6i820vqvhq2su04womayd3dy5\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"ubm4v4s04jwpyty85ubm42w1j22k11xc\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"sls45ph0dz4d53sls4i1z3oxg0kdoqrz\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"2l6as57wp5aey4ielsfl2l6asd54joiw\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"rycmgq4er3n4dlakrycmawa1j2pn1ekq\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p9l7jdj5g2hzhpbmi22p9l7jd5y4u675\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"p6n805f19y0fsp6n8jgon5ykcxhnakgj\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"oxpc371sd7chqoed0l6v6oxpc3afe2ur\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"nouqj1wtq3pm55lelwgnouqj1q1w5kpi\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"m0fgjpi58sovf00ka7m0fgamj0n4vxc0\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"vwzyyvgeh1f65w0ob6xuutonvwzyyvge\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"ivextum28kfro26psoaivexpf3s2mhzj\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"hvtqceh0xi0shvtqlzqdp5sg7zpz4ndd\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"e8467b6udwym52l3ke8467egommtgfed\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"deq36fzc8jyp2rf0udeq36kr1kki0qhs\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"cgxw8ytyiwte7ffbbzrdzicgxw8yprb8\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"agranqn7j1033eagoltgolmlynkydr3p\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"ac2nhqoluhr1s0trmonkac2nhqobwkkc\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9qkc088b04v8z199csf9qkc066iweuc4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9l7h20mqlw53geak9l7hmm0lobaz7jr4\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"9ggwzf3gojs1pwuu1h9ggwzf32kqjx1l\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"78v4857hqfswtksl28rsv78v48el2dmf\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"0iyvkyiz6zmkwd0in6t1yqvxhnmtq1h7\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"4dadski72h1ozwmfr2l4dadskiuufjbs\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3nrnuzyn3ci7tiz5c3nrnuzyis4t4rtc\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"3i0geo89xgb2ekjvd1a3i0gedfu1kj29\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2zovklif9muobp2o2zoxf5o2bgbaf8nn\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"2q48ngln0i2dyecymik1h2q48n86akjw\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1wz151ik0t2hvx6b1ryal1wz150bnd0l\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"1o29uxryr1rme5jtjhbikfamg1o29uxr\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"185ixaw25zxtvjqrlm0za185ixatdzny\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"0p3ft70e0126d03wp1s0p3ft709q4u7s\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\",\"hmvgeuzd7vfn0r7hmvwdx434azlozrpr\"],\"P2_g\":[\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\"],\".pred_class\":[\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Sênior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Júnior\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\",\"Pleno\"],\"resumo\":[\"Tamanho da empresa\",\"Dados relacionais estruturados em bancos SQL\",\"Imagens\",\"Textos/Documentos\",\"SQL\",\"PostgreSQL\",\"SQLite\",\"Grafana\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Sou responsável por criar e manter a infra que meus model...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Utilizo métodos de Visão Computacional\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Entrando em contato com os times de negócio para definiçã...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Dados relacionais estruturados em bancos SQL\",\"Dados armazenados em bancos NoSQL\",\"Planilhas\",\"Presto\",\"Microsoft PowerBI\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Plataformas de Data Apps Streamlit, Shiny, Plotly Dash etc\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Planilhas\",\"R\",\"Java\",\"Javascript\",\"Hive\",\"Metabase\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Desenvolvo sistemas de recomendação RecSys\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Utilizo cadeias de Markov ou HMMs para realizar análises ...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Ferramentas de AutoML Datarobot, H2O, Auto-Keras etc\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Entrando em contato com os times de negócio para definiçã...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Dados relacionais estruturados em bancos SQL\",\"SQL\",\"MySQL\",\"Snowflake\",\"Amazon Web Services AWS\",\"Azure Microsoft\",\"Microsoft PowerBI\",\"Tableau\",\"SAP Business Objects\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Nível de instrução\",\"Planilhas\",\"Dados georeferenciados\",\"Python\",\"Julia\",\"MySQL\",\"Oracle\",\"SQL SERVER\",\"SAP\",\"SAP HANA\",\"Azure Microsoft\",\"Servidores On Premise/Não utilizamos Cloud\",\"Cloud Própria\",\"Metabase\",\"Alteryx\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Entrando em contato com os times de negócio para definiçã...\",\"Colocando modelos em produção, criando os pipelines de da...\",\"Tamanho da empresa\",\"Nível de instrução\",\"Tipo de indústria\",\"Python\",\"Javascript\",\"MySQL\",\"PostgreSQL\",\"Amazon Web Services AWS\",\"Servidores On Premise/Não utilizamos Cloud\",\"Grafana\",\"Pentaho\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo modelos de Detecção de Churn\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Tem o cargo de DS\",\"Nível de instrução\",\"Dados georeferenciados\",\"R\",\"MariaDB\",\"Cloud Própria\",\"Metabase\",\"Alteryx\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Colocando modelos em produção, criando os pipelines de da...\",\"Nível de instrução\",\"Textos/Documentos\",\"MongoDB\",\"S3\",\"PostgreSQL\",\"Databricks\",\"Google Cloud GCP\",\"Tableau\",\"Grafana\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo métodos de Visão Computacional\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Feature Store Feast, Hopsworks, AWS Feature Store, Databr...\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Imagens\",\"R\",\"MySQL\",\"Amazon Athena\",\"Hive\",\"Tableau\",\"Google Data Studio\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Desenvolvo sistemas de recomendação RecSys\",\"Utilizo cadeias de Markov ou HMMs para realizar análises ...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo métodos de Visão Computacional\",\"Feature Store Feast, Hopsworks, AWS Feature Store, Databr...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"MongoDB\",\"S3\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Desenvolvo sistemas de recomendação RecSys\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento na nuvem Google Colab, AWS S...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Planilhas\",\"R\",\"Oracle\",\"SQL SERVER\",\"S3\",\"Hive\",\"Amazon Web Services AWS\",\"Azure Microsoft\",\"Servidores On Premise/Não utilizamos Cloud\",\"Tableau\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo cadeias de Markov ou HMMs para realizar análises ...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo modelos de Detecção de Churn\",\"Ambientes de desenvolvimento na nuvem Google Colab, AWS S...\",\"Ferramentas de AutoML Datarobot, H2O, Auto-Keras etc\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Plataformas de Data Apps Streamlit, Shiny, Plotly Dash etc\",\"Colocando modelos em produção, criando os pipelines de da...\",\"Nível de instrução\",\"Dados relacionais estruturados em bancos SQL\",\"Dados armazenados em bancos NoSQL\",\"MySQL\",\"S3\",\"Amazon Athena\",\"Amazon Web Services AWS\",\"Azure Microsoft\",\"Tableau\",\"Não utilizo nenhuma ferramenta de BI no trabalho\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Plataformas de Machine Learning TensorFlow, Azure Machine...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Nível de instrução\",\"Dados relacionais estruturados em bancos SQL\",\"Planilhas\",\"Java\",\"Javascript\",\"Microsoft Access\",\"Azure Microsoft\",\"Servidores On Premise/Não utilizamos Cloud\",\"Microsoft PowerBI\",\"SAP Business Objects\",\"Pentaho\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Não utilizo nenhuma dessas ferramentas no meu dia a dia.\",\"Nível de instrução\",\"Planilhas\",\"MySQL\",\"Amazon Redshift\",\"Tableau\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Não utilizo nenhuma ferramenta de BI no trabalho\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Utilizo modelos de Detecção de Churn\",\"Planilhas Excel, Google Sheets etc\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Planilhas\",\"R\",\"SQL SERVER\",\"S3\",\"Sybase\",\"Amazon Athena\",\"Azure Microsoft\",\"Tableau\",\"Google Data Studio\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Utilizo ferramentas avançadas de estatística como SAS, SP...\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de estatística avançada como SPSS, SAS, etc.\",\"Entrando em contato com os times de negócio para definiçã...\",\"Realizando construções de dashboards em ferramentas de BI...\",\"Tem o cargo de DS\",\"Tamanho da empresa\",\"Nível de instrução\",\"Tipo de indústria\",\"Dados relacionais estruturados em bancos SQL\",\"Dados armazenados em bancos NoSQL\",\"C/C++/C#\",\"Visual Basic/VBA\",\"SQL SERVER\",\"Databricks\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Sou responsável por criar e manter a infra que meus model...\",\"Entrando em contato com os times de negócio para definiçã...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Cuidando da manutenção de modelos de Machine Learning já ...\",\"Tem o cargo de DS\",\"Dados armazenados em bancos NoSQL\",\"Textos/Documentos\",\"Javascript\",\"MySQL\",\"PostgreSQL\",\"DB2\",\"Microsoft PowerBI\",\"Metabase\",\"SAS Visual Analytics\",\"TIBCO Spotfire\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo ferramentas avançadas de estatística como SAS, SP...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo métodos de Visão Computacional\",\"Planilhas Excel, Google Sheets etc\",\"Ferramentas de estatística avançada como SPSS, SAS, etc.\",\"Entrando em contato com os times de negócio para definiçã...\",\"Cuidando da manutenção de modelos de Machine Learning já ...\",\"Nível de instrução\",\"Dados relacionais estruturados em bancos SQL\",\"Planilhas\",\"SQL\",\"R\",\"SQL SERVER\",\"Azure Microsoft\",\"Servidores On Premise/Não utilizamos Cloud\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Entrando em contato com os times de negócio para definiçã...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"R\",\"Amazon Athena\",\"Amazon Web Services AWS\",\"Google Data Studio\",\"Sou responsável por entrar em contato com os times de neg...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Utilizo ferramentas avançadas de estatística como SAS, SP...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento na nuvem Google Colab, AWS S...\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Dados georeferenciados\",\"R\",\"SQLite\",\"Redis\",\"Presto\",\"Hive\",\"Grafana\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Feature Store Feast, Hopsworks, AWS Feature Store, Databr...\",\"Nível de instrução\",\"Planilhas\",\"SAP\",\"PostgreSQL\",\"Microsoft Access\",\"SAP HANA\",\"Microsoft PowerBI\",\"Pentaho\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Sou responsável por criar e manter a infra que meus model...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Utilizo cadeias de Markov ou HMMs para realizar análises ...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo métodos de Visão Computacional\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Ferramentas de estatística avançada como SPSS, SAS, etc.\",\"Entrando em contato com os times de negócio para definiçã...\",\"Colocando modelos em produção, criando os pipelines de da...\",\"Realizando construções de dashboards em ferramentas de BI...\",\"Planilhas\",\".NET\",\"Visual Basic/VBA\",\"Oracle\",\"SQL SERVER\",\"Databricks\",\"Hive\",\"Azure Microsoft\",\"Tableau\",\"Sou responsável por entrar em contato com os times de neg...\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Sou responsável por criar e manter a infra que meus model...\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Ambientes de desenvolvimento na nuvem Google Colab, AWS S...\",\"Plataformas de Machine Learning TensorFlow, Azure Machine...\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Plataformas de Data Apps Streamlit, Shiny, Plotly Dash etc\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Entrando em contato com os times de negócio para definiçã...\",\"MySQL\",\"SQL SERVER\",\"Amazon Aurora ou RDS\",\"S3\",\"PostgreSQL\",\"Amazon Redshift\",\"Azure Microsoft\",\"Metabase\",\"Amazon QuickSight\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Criando e dando manutenção em ETLs, DAGs e automações de ...\",\"Tem o cargo de DS\",\"Nível de instrução\",\"R\",\"Java\",\"Javascript\",\"MongoDB\",\"Hive\",\"Google Cloud GCP\",\"Não utilizo nenhuma ferramenta de BI no trabalho\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Cuido da manutenção de modelos de Machine Learning já em ...\",\"Utilizo ferramentas avançadas de estatística como SAS, SP...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Utilizo modelos de Detecção de Churn\",\"Plataformas de Machine Learning TensorFlow, Azure Machine...\",\"Ferramentas de estatística avançada como SPSS, SAS, etc.\",\"Nível de instrução\",\"Textos/Documentos\",\"SQL SERVER\",\"S3\",\"PostgreSQL\",\"Google BigQuery\",\"Amazon Web Services AWS\",\"Azure Microsoft\",\"Microsoft PowerBI\",\"Qlik View/Qlik Sense\",\"Google Data Studio\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Sou responsável por entrar em contato com os times de neg...\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Realizo construções de dashboards em ferramentas de BI co...\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo métodos de Visão Computacional\",\"Utilizo modelos de Detecção de Churn\",\"Planilhas Excel, Google Sheets etc\",\"Plataformas de Machine Learning TensorFlow, Azure Machine...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Entrando em contato com os times de negócio para definiçã...\",\"Tamanho da empresa\",\"Nível de instrução\",\"Tipo de indústria\",\"R\",\"Python\",\"C/C++/C#\",\".NET\",\"Java\",\"Javascript\",\"Sybase\",\"Microsoft PowerBI\",\"TIBCO Spotfire\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Não utilizo nenhuma ferramenta de BI no trabalho\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Não utilizo nenhuma dessas ferramentas no meu dia a dia.\",\"MySQL\",\"SQL SERVER\",\"S3\",\"Databricks\",\"Amazon Web Services AWS\",\"Azure Microsoft\",\"Microsoft PowerBI\",\"Tableau\",\"Metabase\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo modelos de Detecção de Churn\",\"Planilhas Excel, Google Sheets etc\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"PostgreSQL\",\"Hive\",\"Amazon Web Services AWS\",\"IBM\",\"Servidores On Premise/Não utilizamos Cloud\",\"IBM Analytics/Cognos\",\"Fazemos todas as análises utilizando apenas Excel ou plan...\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo modelos de Detecção de Churn\",\"Feature Store Feast, Hopsworks, AWS Feature Store, Databr...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Nível de instrução\",\"R\",\"MySQL\",\"Databricks\",\"Google Cloud GCP\",\"IBM\",\"Tableau\",\"IBM Analytics/Cognos\",\"Não utilizo nenhuma ferramenta de BI no trabalho\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Realizo previsões através de modelos de Séries Temporais ...\",\"Utilizo modelos de Reinforcement Learning aprendizado por...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo métodos de Visão Computacional\",\"Utilizo modelos de Detecção de Churn\",\"Ambientes de desenvolvimento na nuvem Google Colab, AWS S...\",\"Plataformas de Machine Learning TensorFlow, Azure Machine...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Dados armazenados em bancos NoSQL\",\"Textos/Documentos\",\"R\",\"Java\",\"SAS/Stata\",\"MongoDB\",\"MariaDB\",\"Firebase\",\"Google BigQuery\",\"Google Firestore\",\"Databricks\",\"Firebird\",\"Google Data Studio\",\"Sou responsável pela coleta e limpeza dos dados que uso p...\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Utilizo redes neurais ou modelos baseados em árvore para ...\",\"Desenvolvo sistemas de recomendação RecSys\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Feature Store Feast, Hopsworks, AWS Feature Store, Databr...\",\"Plataformas de Data Apps Streamlit, Shiny, Plotly Dash etc\",\"Nível de instrução\",\"Planilhas\",\"IBM\",\"Desenvolvo modelos de Machine Learning com o objetivo de ...\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Planilhas Excel, Google Sheets etc\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Entrando em contato com os times de negócio para definiçã...\",\"Desenvolvendo modelos de Machine Learning com o objetivo ...\",\"Nível de instrução\",\"Dados georeferenciados\",\"S3\",\"PostgreSQL\",\"Amazon Redshift\",\"Redash\",\"Alteryx\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\",\"Sistemas de controle de versão Github, DVC, Neptune, Gitl...\",\"Nível de instrução\",\"Dados relacionais estruturados em bancos SQL\",\"Imagens\",\"Planilhas\",\"SQL\",\"S3\",\"PostgreSQL\",\"Amazon Web Services AWS\",\"Servidores On Premise/Não utilizamos Cloud\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Sou responsável por criar e manter a infra que meus model...\",\"Utilizo modelos de regressão linear, logística, GLM\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Ambientes de desenvolvimento local R-studio, JupyterLab, ...\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Tamanho da empresa\",\"Nível de instrução\",\"Tipo de indústria\",\"SAS/Stata\",\"SQL SERVER\",\"Amazon Web Services AWS\",\"IBM\",\"Estudos Ad-hoc com o objetivo de confirmar hipóteses, rea...\",\"Utilizo ferramentas avançadas de estatística como SAS, SP...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo métodos estatísticos clássicos Testes de hipótese...\",\"Utilizo cadeias de Markov ou HMMs para realizar análises ...\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de estatística avançada como SPSS, SAS, etc.\",\"Coletando e limpando os dados que uso para análise e mode...\",\"Nível de instrução\",\"Planilhas\",\"MongoDB\",\"Amazon Athena\",\"Sou responsável por colocar modelos em produção, criar os...\",\"Utilizo métodos estatísticos Bayesianos para analisar dados\",\"Utilizo técnicas de NLP Natural Language Processing para ...\",\"Desenvolvo técnicas de Clusterização K-means, Spectral, D...\",\"Utilizo modelos de Machine Learning para detecção de fraude\",\"Utilizo modelos de Detecção de Churn\",\"Ferramentas de BI PowerBI, Looker, Tableau, Qlik etc\",\"Ferramentas de ETL Apache Airflow, NiFi, Stitch, Fivetran...\"],\"valor_tidy_errado\":[\"Pequeno porte\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Graduação/Bacharelado\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sem diploma\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Pós-graduação\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Graduação/Bacharelado\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Médio porte\",\"Graduação/Bacharelado\",\"Serviços\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Mestrado\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Doutorado ou Phd\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Graduação/Bacharelado\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Mestrado\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Pós-graduação\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sem diploma\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Graduação/Bacharelado\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Pequeno porte\",\"Mestrado\",\"Serviços\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Mestrado\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Graduação/Bacharelado\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Pós-graduação\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sem diploma\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Pós-graduação\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Médio porte\",\"Graduação/Bacharelado\",\"Serviços\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Mestrado\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Mestrado\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Mestrado\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Graduação/Bacharelado\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Graduação/Bacharelado\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Mestrado\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Microempresa\",\"Mestrado\",\"Indeterminado\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Graduação/Bacharelado\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\"],\"valor_tidy_acerto\":[\"Grande porte\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sem diploma\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Graduação/Bacharelado\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sem diploma\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sem diploma\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Grande porte\",\"Pós-graduação\",\"Indústria\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sem diploma\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sem diploma\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Pós-graduação\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Doutorado ou Phd\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Graduação/Bacharelado\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Pós-graduação\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Mestrado\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Grande porte\",\"Sem diploma\",\"Indústria\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Pós-graduação\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Mestrado\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Mestrado\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Pós-graduação\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Graduação/Bacharelado\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Grande porte\",\"Doutorado ou Phd\",\"Indeterminado\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sem diploma\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sem diploma\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Graduação/Bacharelado\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Pós-graduação\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sem diploma\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Graduação/Bacharelado\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Grande porte\",\"Pós-graduação\",\"Serviços\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Mestrado\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\"],\"top_feature\":[\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Não\",\"Não\",\"Sim\",\"Sim\",\"Não\",\"Não\",\"Não\",\"Não\",\"Não\",\"Sim\",\"Não\",\"Sim\",\"Sim\",\"Sim\"]},\"columns\":[{\"id\":\"P0_errado\",\"name\":\"Pessoa\",\"type\":\"character\"},{\"id\":\"P2_g\",\"name\":\"Senioridade da pessoa\",\"type\":\"character\",\"aggregate\":\"unique\"},{\"id\":\".pred_class\",\"name\":\"Senioridade prevista\",\"type\":\"factor\",\"aggregate\":\"unique\"},{\"id\":\"resumo\",\"name\":\"Pergunta\",\"type\":\"character\"},{\"id\":\"valor_tidy_errado\",\"name\":\"Resposta foi...\",\"type\":\"character\"},{\"id\":\"valor_tidy_acerto\",\"name\":\"Resposta deveria mudar para...\",\"type\":\"character\"},{\"id\":\"top_feature\",\"name\":\"Feature entre top 10?\",\"type\":\"character\",\"defaultSortDesc\":true}],\"groupBy\":[\"P0_errado\"],\"defaultSorted\":[{\"id\":\"top_feature\",\"desc\":true}],\"defaultPageSize\":3,\"showPageSizeOptions\":true,\"highlight\":true,\"borderless\":true,\"striped\":true,\"compact\":true,\"style\":{\"fontSize\":\"12px\"},\"dataKey\":\"24a35ed32ddcd9a5a85ec3756ba0593f\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nA partir desses resultados, resolvi então determinar a frequência com a qual cada uma das características utilizadas pelo modelo preditivo deveriam mudar de forma que àquelas pessoas fossem mapeadas corretamente ao seu nível de senioridade. Este exercício forneceu alguns insights, apresentados na figura abaixo, e que nos mostram que:\n\n+ Diferenças no nível de instrução foram o principal motivo para a classificação incorreta do nível de senioridade. Isto foi o caso independente se estamos falando de uma pessoa de nível Júnior que foi classificada como Sênior ou vice-versa;\n+ Existe um número expressivo de características diferentes entre cada par de pessoas, mas a maior parte delas não teve muito peso para a previsão de sua senioridade (_i.e._, ou seus coeficientes eram zero ou não estavam entre as 10 variáveis mais importantes por nível de senioridade que extraímos do modelo; barras brancas abaixo). Alguns exemplos destas características são o conhecimento em Cloud na Azure ou na AWS quando o problema foi errar a senioridade de pessoas de nível Júnior, ou utilizar o R quando o caso foi as pessoas de nível Sênior. Isto sugere que a diversidade de atuações dentro de um mesmo nível de senioridade é relevante, de forma que você não precisa ser um clone exato de uma pessoa para estar no nível correto ou ir para o próximo nível - basta não perder de vista o que talvez seja o essencial; e,  \n+ Existe um número maior de características que impactam mais fortemente a previsão do nível de senioridade (_i.e._, barras cinzas abaixo) quando falamos dos erros nos quais a pessoa de nível Sênior foi classificada como de nível Júnior do que o contrário. Isto parece sugerir que existem mais formas do modelo errar o nível de senioridade de quem é Sênior do que de quem é Júnior.\n\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\n# criando a figura para mostrar as variáveis que apareceram com mais frequência como as mais\n# diferentes entre cada par de pessoas\ndf_contra_exemplos %>% \n  # retendo apenas os registros de cada par de pessoas no qual existe uma diferença entre as \n  # respostas dadas para uma mesma pergunta\n  filter(resposta_acerto != resposta_errado) %>% \n  # contando quantas vezes cada variável aparece associada à cada tipo de erro\n  count(direcao_erro, resumo, sort = TRUE) %>% \n  # juntando a informação da quantidade de pessoas associadas à cada tipo geral de erro\n  # que estavam na base de teste\n  left_join(\n    y = df_contra_exemplos %>% \n      distinct(P0_errado, direcao_erro) %>% \n      count(direcao_erro, name = 'n_pessoas'),\n    by = 'direcao_erro'\n  ) %>% \n  # juntando a informação do tipo de variável associada à cada tipo geral de erro\n  left_join(\n    y = df_contra_exemplos %>% \n      distinct(direcao_erro, resumo, top_feature),\n    by = c('direcao_erro', 'resumo')\n  ) %>% \n  # transformando os dados para ajudar na plotagem\n  mutate(\n    # calculando a frequencia com a qual uma dada variável apareceu como diferente entre a pessoa\n    # na base de teste em que o modelo errou e a pessoa mais similar à ela na base de treino que \n    # o modelo acertou - este valor representa a frequência de ocorrência de uma diferença naquela\n    # variável nos dados que estamos analisando\n    proporcao = n / n_pessoas,\n    # limitando o tamanho do texto por linha de forma a não deixas os textos em cada eixo muito longos\n    resumo = str_wrap(string = resumo, width = 30),\n    # ordenando as variáveis dentro de cada tipo de erro de acordo com a sua frequencia de ocorrencia\n    # isso nos ajudará a colocar as barras em ordem na figura abaixo\n    texto_ordenado = reorder_within(x = resumo, by = proporcao, within = direcao_erro)\n  ) %>% \n  # agrupando pelo tipo geral de erro, de forma a aplicar o filtro seguinte dentro de cada grupo\n  group_by(direcao_erro) %>% \n  # pegando as 10 variáveis mais frequentemente diferentes entre a pessoa na base de teste e à mais\n  # similar à ela na base de treino\n  slice_max(order_by = proporcao, n = 10, with_ties = FALSE) %>% \n  # criando a figura per se\n  ggplot(mapping = aes(x = proporcao, y = texto_ordenado, fill = top_feature)) +\n  facet_wrap(~ direcao_erro, scales = 'free') +\n  geom_col(color = 'black', show.legend = FALSE) +\n  geom_text(mapping = aes(label = paste0(round(x = proporcao, digits = 2) * 100, '%')), nudge_x = 0.07) +\n  scale_y_reordered() +\n  scale_x_continuous(breaks = seq(from = 0, to = 0.85, by = 0.2), limits = c(0, 0.9), \n                     labels = label_percent(), expand = c(0, 0)) +\n  scale_fill_manual(values = c('white', 'grey50')) +\n  labs(\n    title    = 'Quais as 10 características mais frequentemente associadas às diferenças entre os pares de pessoas?',\n    subtitle = 'Diferenças no nível de instrução foram a principal causa do erro de classificação do nível de senioridade das pessoas por parte do modelo, independentemente do tipo de\\nerro. Além disso, grande parte dos erros de classificação das pessoas de nível Sênior esteve associada à diferenças nas variáveis com maior peso sobre a classificação',\n    caption  = 'As barras na cor cinza representam as variáveis com maior peso para a classificação do nível de senioridade por parte do modelo dentro de cada tipo geral de erro.',\n    x        = 'Frequência de ocorrência'\n  ) +\n  theme(\n    axis.title.y = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_diferencas_frequentes-1.png){width=1248}\n:::\n:::\n\n\nResolvi ir um pouco mais no detalhe nos padrões apresentados nessa última figura, e olhar que tipo de mudança deveria ocorrer no nível de instrução para o modelo acertar a classificação de senioridade. O resultado mais interessante é que, tanto no caso da pessoa de nível Júnior classificada como Sênior quanto no contrário, é provável que o modelo já tivesse acertado a sua senioridade se ela tivesse o nível de instrução abaixo ou acima do qual ela já possui. Por exemplo, o fato de uma pessoa de nível Júnior ter um mestrado fez com que ela fosse classificada como Sênior, quando bastava que ela só tivesse a Graduação/Bacharelado para que ela tivesse melhor alinhada ao nível de Júnior. Outro exemplo é o de uma pessoa de nível Sênior que foi classificada como Júnior pois ela só tinha Graduação/Bacharelado - e, talvez, bastava que ela tivesse uma pós-graduação ou mestrado para que ela tivesse sido classificada corretamente. Finalmente, algo que vale a pena salientar é que muitas das pessoas de nível Júnior classificadas incorretamente pelo modelo já tem um mestrado atualmente, e acredito que isso conversa bastante com alguns movimentos de pessoas indo da academia para o mercado que temos visto.\n\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code}\n# criando a base que usaremos para criar as figuras com as mudanças de resposta maisfrequentes que deveriam \n# ser feitas para que uma pessoa respondente tivesse sido classificada corretamente no seu nível de senioridade\ndf_fig_instrucao <- df_contra_exemplos %>% \n  # pegando apenas os casos relacionados à diferenças no nível de instrução das pessoas\n  filter(texto == 'Nível de instrução') %>%\n  # retendo apenas os registros de cada par de pessoas no qual existe uma diferença entre as \n  # respostas dadas para uma mesma pergunta\n  filter(resposta_acerto != resposta_errado) %>% \n  # contando quantas vezes, dentro de cada tipo de erro, aparece cada par de mudança da resposta observada\n  # para a resposta que precisava ser dada\n  count(direcao_erro, valor_tidy_errado, valor_tidy_acerto, name = 'ocorrencias', sort = TRUE) %>%\n  # adicionando um identificador único à cada linha, que será usada para criar o alluvial plot\n  rownames_to_column(var = 'id') %>% \n  # passando a base para o formato longo, de forma a termos o par de respostas observadas vs que precisava\n  # ser dada nas linhas ao invés de nas colunas para cada pessoa\n  pivot_longer(cols = c(valor_tidy_errado, valor_tidy_acerto), names_to = 'resposta', values_to = 'valor') %>% \n  # reordenando o nível do tipo de resposta e da resposta que precisava ser dada para fazer com que\n  # o gradiente fique mais claro na figura que vamos criar\n  mutate(\n    resposta = fct_relevel(.f = resposta, 'valor_tidy_errado'),\n    valor = factor(x = valor, \n                   levels = c('Sem diploma', 'Graduação/Bacharelado', 'Pós-graduação', 'Mestrado', 'Doutorado ou Phd'))\n  )\n\n# figura para o caso do junior classificado como sênior\nfig1 <- df_fig_instrucao %>% \n  filter(direcao_erro == 'Júnior classificado como Sênior') %>%\n  ggplot(mapping = aes(x = resposta, stratum = valor, alluvium = id, y = ocorrencias, fill = valor, label = valor)) +\n  geom_flow(width = 0.2, show.legend = FALSE) +\n  geom_stratum(width = 0.2, show.legend = FALSE) +\n  geom_text_repel(\n    mapping = aes(label = ifelse(as.numeric(resposta) == 1, as.character(valor), NA)),\n    stat = 'stratum', size = 4, nudge_x = -0.5\n  ) +\n  geom_text_repel(\n    mapping = aes(label = ifelse(as.numeric(resposta) == 2, as.character(valor), NA)),\n    stat = \"stratum\", size = 4, direction = \"y\", nudge_x = 0.5\n  ) +\n  scale_fill_viridis_d() +\n  labs(\n    title    = 'O que me diferencia se sou um profissional de nível Júnior?',\n    subtitle = 'Profissionais de nível Júnior classificados como Sênior'\n  ) +\n  theme(\n    plot.title   = element_text(size = 12),\n    axis.line    = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text    = element_blank()\n  )\n\n# figura para o caso do sênior classificado como júnior\nfig2 <- df_fig_instrucao %>% \n  filter(direcao_erro == 'Sênior classificado como Júnior') %>%\n  ggplot(mapping = aes(x = resposta, stratum = valor, alluvium = id, y = ocorrencias, fill = valor, label = valor)) +\n  geom_flow(width = 0.2, show.legend = FALSE) +\n  geom_stratum(width = 0.2, show.legend = FALSE) +\n  geom_text_repel(\n    mapping = aes(label = ifelse(as.numeric(resposta) == 1, as.character(valor), NA)),\n    stat = 'stratum', size = 4, nudge_x = -0.5\n  ) +\n  geom_text_repel(\n    mapping = aes(label = ifelse(as.numeric(resposta) == 2, as.character(valor), NA)),\n    stat = \"stratum\", size = 4, direction = \"y\", nudge_x = 0.5\n  ) +\n  scale_x_discrete(labels = c('Qual é o nível de\\ninstrução da pessoa?', 'Qual deveria ter sido\\npara o modelo acertar?')) +\n  scale_fill_viridis_d() +\n  labs(\n    title = 'Como eu poderia me diferenciar já sendo um profissional Sênior?',\n    subtitle = 'Profissionais de nível Sênior classificados como Júnior'\n  ) +\n  theme(\n    plot.title   = element_text(size = 12),\n    axis.line    = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y  = element_blank(),\n    axis.text.x  = element_text(size = 12, face = 'bold')\n  )\n\n# compondo a figura\n(fig1 / fig2) +\n  plot_annotation(\n    title = 'Quais as diferenças mais comuns no nível de instrução entre os pares de pessoas?',\n    theme = theme(plot.title = element_text(size = 14))\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig_instrucao-1.png){width=864}\n:::\n:::\n\n\nAcredito que podemos encerrar por aqui as nossas análises. No entanto, se você quiser ir um pouco mais fundo e explorar outras diferenças entre os pares de pessoas ou até mesmo usar o modelo que treinamos para se escorar, vou deixar tudo isso salvo aqui.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# objeto contendo o pipeline de pre-processamento dos dados e o modelo treinado\nwrite_rds(x = extract_workflow(x = modelo_treinado), file = 'outputs/modelo_treinado.rds')\n# base que utilizamos para treinar o modelo\nwrite_rds(x = training(x = split_dos_dados), file = 'outputs/base_de_treino.rds')\n# base que utilizamos para testar o modelo\nwrite_rds(x = testing(x = split_dos_dados), file = 'outputs/base_de_teste.rds')\n# base com os pares de pessoas e as respostas à cada uma das perguntas feitas pela pesquisa\nwrite_rds(x = df_contra_exemplos, file = 'outputs/pares_de_pessoas.rds')\n# dicionario para mapear o identificador das perguntas ao seu texto\nwrite_rds(x = dicionario, file = 'outputs/dicionario_de_perguntas.rds')\n```\n:::\n\n\n# Conclusões\n\nNesse post eu foquei em entender a diferença existente entre pessoas de nível Júnior, Pleno e Sênior na área de dados, dando uma atenção especial à ciência de dados. Ao contrário de outras discussões, minha idéia aqui foi a de tentar enxergar estas diferenças de alguma forma através dos dados, e obter algum tipo de insight que pudesse contribuir com a comunidade. Isso foi possível graças à pesquisa _State of Data Brasil 2021_, que mapeou as atuações gerais e específicas da área e da ciência de dados, além de outras informações relacionadas ao contexto de trabalho no qual cada pessoa está inserida. \n\nUma análise preliminar feita sobre a amostra de pessoas que atuam com ciência de dados e responderam à pesquisa revelou uma grande heterogeneidade de atuações dentro e entre aqueles três níveis de senioridade. Observamos que existe um _core_ de atuações que praticamente toda pessoa cientista de dados parece ter mas, por outro lado, é maior ainda a quantidade de atuações e conhecimentos que se substituem entre as pessoas e que parecem caracterizar nichos de atuação. Apesar disto retratar a diversidade de backgrounds e experiências, tais padrões também mostram a dificuldade que pode ser em determinar os critérios que o mercado de trabalho brasileiro está usando para estabelecer o perfil e expectativas sobre os profissionais de acordo com o seu nível de senioridade.\n\nA forma como optei por abordar este problema foi utilizar uma regressão multinomial para prever o nível de senioridade de cada pessoa cientista de dados, utilizando então os betas da regressão para delinear as características mais frequentemente associadas (ou não) à cada nível de senioridade. Através desta abordagem conseguimos observar alguns padrões interessantes, muitos dos quais conversam bastante com o senso comum. O exemplo mais claro disso está na contribuição do nível de instrução para prevermos a senioridade da pessoa: a trajetória acadêmica da graduação à uma pós-graduação _stricto sensu_ foi representada na transição dos níveis de Júnior ao Sênior, passando pelo nível de Pleno. Não é a minha intenção fazer uma inferência causal sobre este padrão, mas não seria estranho assumir que quanto mais estudamos e buscamos nos especializar, maiores as chances de aprendermos coisas novas, ganharmos mais experiência e conseguir trazer um maior impacto para o negócio no qual estamos inseridos. Assim, seja através da educação formal ou informal (ou até algum tipo de co-relação ou _confounding_ entre elas _e.g._, quem gosta de estudar acaba recorrendo a educação formal e informal), o fato é que buscar mais qualificação é uma alavanca importante para chegar à níveis de atuação mais altos.\n\nOutro padrão interessante que encontramos está relacionado à evolução na bagagem que as pessoas acumulam ao longo de sua carreira. Como pudemos observar, é incomum que pessoas de nível Júnior tenham muitas atuações gerais da área de dados e específicas da área de ciência de dados. Por outro lado, pessoas de nível Pleno já tiveram maior afinidade por ter certas atuações muito relacionadas ao ciclo de vida dos projetos de ciência de dados, enquanto pessoas de nível Sênior pareceram estar mais associadas à resolução de problemas de negócio específicos (_e.g._, detecção de _churn_) e a ter uma bagagem maior de conhecimentos na área de dados em si. É razoável assumir que estes sejam os padrões esperados quando pensamos no nosso próprio ciclo de amadurecimento e evolução de carreira, e é interessante a possibilidade de que isto possa estar representado de alguma forma nos dados.\n\nA abordagem que optei seguir capturou o perfil 'médio' de cada nível de senioridade mas, como bem sabemos, o comportamento da média pode não descrever a variação dentro da amostra. Assim, dada a heterogeneidade de atuações entre pessoas observada na análise exploratória, não foi surpreendente que o modelo tenha errado a classificação do nível de senioridade de algumas pessoas. Por conta disso, busquei entender o que estaria mais frequentemente associado a esses erros, contrastando as atuações de cada uma das pessoas dentro desta subamostra com àquelas pessoas mais similares à elas e que foram classificadas corretamente pelo modelo. Este exercício demonstrou que a principal fonte de erro do modelo parece vir do nível de instrução que cada pessoa disse ter: pessoas de nível Júnior mais qualificadas do que o esperado (_e.g._, possuem uma pós-graduação _stricto sensu_ e/ou _lato sensu_) ou pessoas de nível Sênior menos qualificadas do que o esperado (_e.g._, possuem apenas a graduação). Eu achei interessante esse resultado, e é difícil para mim não deixar de pensar se isso não retrata um padrão que tenho visto das pessoas saindo da academia e aceitando cargos de nível Júnior como uma forma de entrar no mercado de trabalho - esse foi um pouco do meu caminho, e conheço um bocado de gente que tem percorrido ele também. \n\nTodos esses padrões que observamos aqui são baseados em uma amostra pequena de pessoas cientistas de dados que estão atuando no mercado de trabalho brasileiro, e precisamos ter em mente que não sabemos o quanto que esses resultados e padrões podem ser generalizados para a comunidade como um todo. Além disso, é importante não perder de vista que as informações que utilizei para chegar até aqueles resultados deixam de lado aspectos importantes da atuação das pessoas que também contribuem para essa discussão, tais como o grau de impacto que a pessoa tem no negócio, a qualidade de suas entregas, o quanto que ela sabe de fato e tudo o mais. Contudo, é difícil que uma pesquisa tão grande e complexa como o _State of Data Brasil 2021_ consiga mapear todos esses aspectos de forma robusta, comparativa e justa entre as pessoas, e é certo que existem alguns pontos nos quais a pesquisa poderia melhorar (_e.g._, reduzir a redundância entre algumas opções ou perguntas e ser menos abrangente na quantidade de opções para outras - como no caso dos tipos de bancos de dados). Ainda assim, os resultados dessa pesquisa trazem um retrato da nossa área que já têm muito valor para a comunidade, e exercícios como esse aqui e muitos outros que estão sendo gerados vão revelar padrões e fatos interessantes que têm todo o potencial de redirecionar algumas das discussões que normalmente nos involvemos.\n\nDúvidas, sugestões ou críticas? É só me procurar que a gente conversa!\n\n# Possíveis extensões\n\nTive algumas idéias e testei algumas coisas enquanto olhava os dados e ia fazendo as análises exploratórias, mas não consegui priorizar tudo à tempo. Então fica aqui o registro para não esquecer (ou para você tocar também!):\n\n+ Não temos como combinar todos os respondentes neste mesmo tipo de análise pois as perguntas relacionadas às atuações específicas foram apresentadas para as pessoas dependendo da atuação que elas selecionaram, de forma que _e.g._ uma pessoa cientista de dados não viu as mesmas perguntas de uma engenheira de dados. Por conta disso, eu optei aqui por focar só nas respostas das pessoas que tem a atuação de cientista de dados em seu dia a dia, mas todas as análises apresentadas podem ser replicadas às demais atuações - inclusive, eu cheguei a fazer um pouco disso na análise exploratória que fiz antes de escrever esse post, mas achei que ia ficar complexo demais escrever sobre todos;  \n+ Durante a análise exploratória que fiz antes de escrever este post, reparei que não havia diferença na heterogeneidade dentro e entre os níveis de senioridade caso usássemos as atuações gerais ou só as atuações específicas. Ainda assim, resolvi por considerar todas elas para ter uma visão mais completa da área de ciência de dados. De toda forma, talvez fosse interessante repetir as análises focando em um só tipo de atuação (inclusive, se focássemos só nas atuações gerais, poderíamos comparar as pessoas entre todas as atuações);  \n+ Reparei que existe alguma co-dependência entre as respostas de algumas perguntas, que podem acabar gerando uma heterogeneidade onde na realidade ela não é tão grande. Por exemplo, reparei que as respostas 'Utilizo modelos de Detecção de Churn' (_i.e._, `P8_b_m`) e 'Utilizo modelos de Machine Learning para detecção de fraude' (_i.e._, `P8_b_k`) tendem a co-ocorrer com uma frequência bem alta, de forma que a natureza da atuação contida na resposta à uma pergunta já seria informativa da outra. Outro tipo de caso no qual isso deve ocorrer é quando falamos das tecnologias: uma pessoa que usa a Cloud da AWS certamente acabará marcando muitas opções de tecnologias associadas à ela (_e.g._, S3, Amazon Aurora, Amazon Redshift, Amazon Quicksight), de forma que o excesso de atuações pode não ser tão representativo assim de uma caixa de ferramentas tão diversa. Nesse sentido, acredito que uma coisa legal de se fazer seria criar algo como um _embedding_ dessas atuações e usar essa representação nas análises;\n+ Ainda seguindo a linha da idéia anterior, poderíamos usar aquele _embedding_ para responder à outra pergunta: quantos tipos diferentes de pessoas cientistas de dados existem? (quem sabe não foco nisso na próxima pesquisa?);  \n+ Uma outra idéia que tive era tentar entender (1) que tipo de fatores estão associados à pessoa estar aberta à participar de entrevistas de emprego e (2) o que também estaria associado à ela decidir mudar ou não de emprego. Acredito que muitas das coisas que desenvolvemos aqui poderiam ser reaproveitadas de alguma forma para responder à essas duas perguntas, embora eu também acredite que precisaríamos racionalizar um pouco mais a preparação da base para a modelagem, a parte da criação das features, as premissas e limitações da abordagem.   ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/core-js-2.5.3/shim.min.js\"></script>\r\n<script src=\"../../site_libs/react-18.2.0/react.min.js\"></script>\r\n<script src=\"../../site_libs/react-18.2.0/react-dom.min.js\"></script>\r\n<script src=\"../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\r\n<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\r\n<link href=\"../../site_libs/reactable-0.4.4/reactable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/reactable-binding-0.4.4/reactable.js\"></script>\r\n<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}