[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "De volta ao topo"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Bio\nAprendi a viver e a fazer ciência ao longo da minha trajetória profissional, através de experimentos e estudos observacionais, publicando artigos científicos, contribuindo em grupos de pesquisa e coordenando projetos. Além disso, também atuei como professor colaborador, ministrando disciplinas orientadas à análise de dados e programação. Foi durante esta trajetória percebi que não precisava permanecer na academia para continuar fazendo ciência, e passei a atuar como cientista (de dados) na iniciativa privada. Tenho uma vontade enorme de aprender e gosto de atuar em projetos onde as perguntas não estão bem definidas e as soluções fogem do convencional. Além de estudar e programar, estão entre minhas paixões a minha família, a corrida e os jogos de tabuleiro.\n\n\nFormação\n\nSuperior\nUNIVERSIDADE FEDERAL DO RIO DE JANEIRO | Rio de Janeiro, RJ\n\nPós-Doutorado Ecologia | Programa de Pós-Graduação em Ecologia | Março 2016 - Junho 2019\n\nEfeito das Mudanças Climáticas sobre as Interações Ecológicas\nBolsista PNPD/CAPES\n\n\nPh.D. Ecologia | Programa de Pós-Graduação em Ecologia | Março 2012 - Fevereiro 2016\n\nO efeito de predadores na estrutura e funcionamento de cadeias traficas e sua interação com as mudanças climáticas\nBolsista CAPES\nPrêmio de Melhor Tese de Doutorado defendida no Programa em 2016\n\n\nM.Sc. Ecologia | Programa de Pós-Graduação em Ecologia | Agosto 2009 - Outubro 2011\n\nEstruturação da comunidade de macroinvertebrados aquáticos em bromélias-tanque\nBolsista CAPES\n\n\nB.Sc. Ciências Biológicas | Especialização em Ecologia | Agosto 2005 - Julho 2009\n\nBromélias como ilhas: Influência das características do habitat sobre a riqueza de macroinvertebrados aquáticos\nBolsista FAPERJ\n\n\n\n\n\nComplementar\n\nDATACAMP\n\nData Scientist: Customer Channel/Marketing (Basic, Intermediate & Advanced) | 2021\nData Scientist: Risk (Basic) | 2021\nInteractive Data Visualization with R | 2020\nUnsupervised Machine Learning with R | 2020\nSupervised Machine Learning with R | 2020\nMachine Learning Specialist with R | 2020\nMachine Learning Fundamentals with R | 2020\nPython Programmer | 2019\nData Scientist with Python | 2019\nData Analyst with Python | 2019\nShiny Fundamentals with R | 2019\n\n\nDATA SCIENCE ACADEMY\n\nFormação Inteligência Artificial | Em andamento\nPython Fundamentos para a Análise de Dados | 2020\nMicrosoft Power BI para Data Science | 2020\nBig Data Fundamentos | 2020\nIntrodução à Inteligência Artificial | 2020\nWeb Scrapping e Análise de Dados | 2020\nSoft Skills: Desenvolvendo Suas Habilidades Comportamentais | 2020\n\n\nCURSO-R\n\nDashboards | 2021\nWeb Scrapping | 2021\nDeploy | 2021\nRelatórios e Visualização de Dados | 2021\nDeep Learning | 2020\n\n\nCOURSERA\n\nData Science | Johns Hopkins University | Em andamento\n\nReproducible Research | 2021\nExploratory Data Analysis | 2021\nGetting and Cleaning Data | 2021\nR Programming | 2021\nThe Data Scientist’s Toolbox | 2021\n\nNLP Specialization | Em andamento\n\nNatural Language Processing with Sequence Models | Em andamento\nNatural Language Processing with Probabilistic Models | 2022\nNatural Language Processing with Classification and Vector Spaces | 2022\n\nDeep Learning Specialization | 2022\n\nSequence Models | 2022\nConvolutional Neural Networks | 2022\nStructuring Machine Learning Projects | 2022\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization | 2022\nNeural Networks and Deep Learning | 2022\n\nIntroduction to Mathematical Thinking | 2021\nLearn to Program: Crafting Quality Code | 2021\nMathematics for Machine Learning\n\nPrincipal Component Analysis | 2022\nMultivariate Calculus | 2021\nLinear Algebra | 2021\n\n\n\nCLOUD\n\nAWS Cloud Practitioner | 2022\n\n\n\n\n\nExperiência\n\nAccenture | Cientista de Dados | Julho 2019 - Presente\n\nData Science Principal | Abril 2021 - Atual\nData Science Manager | Janeiro 2021 - Março 2021\nData Science Consultant | Julho 2019 - Dezembro 2020\n\n\nPrograma de Pós-Graduação em Ecologia/UNICAMP | Professor Colaborador | Q1 2019\n\nIntrodução ao Manejo e Visualizacao de Dados\n\n\nPrograma de Pós-Graduação em Ecologia/UFRJ | Professor Colaborador | Março 2016 - Junho 2019\n\nManejo, Visulizacao e Compartilhamento de Dados | 2018\n\nRevisão Sistemática e Meta-Análise | 2016 - 2019\nDelineamento Experimental e Estatística | 2016 - 2019\nIntrodução à Linguagem R | 2016 - 2019\n\n\nLaboratório de Limnologia/UFRJ | Pesquisador | Maio 2005 - Junho 2019\n\nPrograma de Monitoramento Integrado dos Igarapés da FLONA Saracá-Taquera | 2009 - 2020\nPrograma de Monitoramento das Lagoas do Norte Fluminense - ECOLagoas | 2005 - 2009\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "backlog.html",
    "href": "backlog.html",
    "title": "Backlog",
    "section": "",
    "text": "Web Scraping\n\nAnúncios do ZapImóveis;\n\nOpiniões dos funcionários do Glassdoor;\nColaboradores da empresa no LinkedIn;\nPerfil do LinkedIn;\nPostagens do LinkedIn.\n\n\n\nShinyApp\n\nRecomendação de jogos de tabuleiro;\n\n\n\nAnálises\n\nState of Data 2023;\n\n\n\nEnsaios\n\nReview sobre os cursos curtos em Gen AI do DeepLearning.ai;\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Quais as associações entre as cartas de Gwent nos decks existentes?\n\n\nEu tenho jogado Gwent: the Witcher Card Game há algum tempo, e é impressionante a quantidade de combos e sinergias que podem haver entre as cartas de acordo com o deck que você monta. Neste post, eu tento identificar as combinações de cartas que aparecem com maior frequência através de uma análise das regras de associação entre elas.\n\n\n\n\nboardgames\n\n\narules\n\n\nanalise\n\n\nr\n\n\n \n\n\n\n\n8 de jan. de 2022\n\n\n2 minutos\n\n\n\n\n\n\n  \n\n\n\n\nConvertendo coordenadas através da calculadora geográfica do INPE\n\n\nNesse post eu mostro a solução que propus para resolver um problema: converter coordenadas de uma projeção e datum qualquer para SIRGAS2000. Como não encontrei um bom suporte para a conversão no R, tive que recorrer à calculadora geográfica do INPE, criando uma automação para interagir com ela e realizar esta tarefa.\n\n\n\n\nweb scraping\n\n\nselenium\n\n\npython\n\n\n \n\n\n\n\n23 de dez. de 2021\n\n\n18 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a biblioteca de decks de Gwent\n\n\nGwent é um jogo de cartas que nasceu dentro do universo de The Witcher e, dada a popularidade da franquia, chegou aos smartphones. A comunidade de jogadores é bastante ativa, e existe uma biblioteca de decks contribuídos que está disponível dentro do site oficial do jogo. Meu objetivo neste post será obter os dados desta biblioteca e de seus decks. Isto servirá para montar uma base de dados para fazermos outras análises posteriormente.\n\n\n\n\nweb scraping\n\n\nr\n\n\nboardgames\n\n\n \n\n\n\n\n30 de nov. de 2021\n\n\n32 minutos\n\n\n\n\n\n\n  \n\n\n\n\nPrevisão de acidentes com os dados da Fórmula 1\n\n\nHá algum tempo atrás escrevi um post para tentar entender se e de que forma os tempos de conclusão das provas de Fórmulas 1 vêm evoluindo ao longo das temporadas. Neste post eu mudo o foco, e tento entender e determinar a probabilidade de ocorrência de acidentes nas provas da Fórmula 1.\n\n\n\n\nweb scraping\n\n\nr\n\n\nmachine learning\n\n\nanalise\n\n\n \n\n\n\n\n15 de nov. de 2021\n\n\n59 minutos\n\n\n\n\n\n\n  \n\n\n\n\nQuão similares são as notas dos jogos de tabuleiro entre os portais especializados?\n\n\nMeu principal objetivo neste post é analisar as notas dadas aos jogos de tabuleiros nos rankings do portal da Ludopedia e do portal do BoardGameGeek para determinar quão similares são as notas dadas aos títulos nas mesmas posições entre os dois rankings. Isto é, será que a nota dada ao título na i-ésima posição no ranking da Ludopedia é parecida com a nota dada ao título na mesma posição no ranking do BoardGameGeek?\n\n\n\n\nestatistica\n\n\nboardgames\n\n\nanalise\n\n\nr\n\n\n \n\n\n\n\n12 de nov. de 2021\n\n\n44 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a Página do Ranking da Ludopedia\n\n\nEu já havia raspado a página do ranking do portal do BoardGameGeek, e agora eu vou repetir a tarefa focando no ranking do portal da Ludopedia. Meu objetivo com isso é criar a base para que, mais tarde, possamos fazer análises comparando os jogos entre os dois portais.\n\n\n\n\nweb scraping\n\n\nboardgames\n\n\nr\n\n\n \n\n\n\n\n24 de out. de 2021\n\n\n17 minutos\n\n\n\n\n\n\n  \n\n\n\n\nEntendendo os Padrões de Duração das Provas da Fórmula 1\n\n\nA Fórmula 1 é um dos esportes de velocidade mais famosos do mundo, com provas ocorrendo desde o início da década de 50 até os dias de hoje. Muita coisa mudou nestes 70 anos, especialmente os carros: cada vez mais bonitos, mais seguros e mais rápidos. Mas será que isso também se traduziu em provas cada vez mais curtas também? Neste post eu examino de que forma a duração das provas da Fórmula 1 têm evoluído ao longo das temporadas.\n\n\n\n\nanalise\n\n\n \n\n\n\n\n8 de out. de 2021\n\n\n47 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a página do ranking do BoardGameGeek\n\n\nNeste post eu faço a raspagem da tabela do ranking dos jogos de tabuleiro do BoardGameGeek. Essa tarefa foi necessária para que eu conseguisse interagir da melhor forma possível com a API XML que o site oferece.\n\n\n\n\nweb scraping\n\n\nr\n\n\nboardgames\n\n\n \n\n\n\n\n17 de set. de 2021\n\n\n18 minutos\n\n\n\n\n\n\n  \n\n\n\n\nPrevendo o preço de apartamentos em Niterói/RJ\n\n\nA previsão de preços de imóveis é uma tarefa muito comum em ciência de dados, existingo até Hello World para esta prática - o Ames Housing, com informações sobre o preço e outros metadados de imóveis na cidade de Ames em Iowa. Neste post, sigo esta idéia e utilizo um conjunto de dados reais sobre os apartamentos disponíveis para a venda no município de Niterói/RJ. Buscarei entender e prever a variação no preço destes imóveis de acordo com as informações contidas nos anúncios com a ajuda de um modelo de Machine Learning.\n\n\n\n\nmachine learning\n\n\n \n\n\n\n\n27 de ago. de 2021\n\n\n52 minutos\n\n\n\n\n\n\nNenhum item correspondente\n\n De volta ao topo"
  },
  {
    "objectID": "posts/2021-08-27_niteroi-housing-prices/index.html",
    "href": "posts/2021-08-27_niteroi-housing-prices/index.html",
    "title": "Prevendo o preço de apartamentos em Niterói/RJ",
    "section": "",
    "text": "Nota\n\n\n\nEste post foi o meu trabalho de conclusão do curso Relatórios e visualização de dados que fiz em julho de 2021 na Curso-R Curso-R, que foi escolhido como um dos três melhores trabalhos da turma."
  },
  {
    "objectID": "posts/2021-08-27_niteroi-housing-prices/index.html#footnotes",
    "href": "posts/2021-08-27_niteroi-housing-prices/index.html#footnotes",
    "title": "Prevendo o preço de apartamentos em Niterói/RJ",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nhttps://pt.wikipedia.org/wiki/Niter%C3%B3i↩︎\nShapefile obtido a partir de https://geo.niteroi.rj.gov.br/civitasgeoportal/↩︎\nNão mostro aqui mas, de fato, a relação entre muitas delas não é linear.↩︎\nDei uma lida no texto de descrição dos anúncios e parece que todos os casos que narrei aqui são erros de imputação de informação mesmo. Assim, acredito que esta solução não seja um problema.↩︎\nSe você for uma pessoa curiosa, eu acabei imputando um NA a 583 linhas da base↩︎\nNão existe uma função no fastshap para criar essa figura, então tentei emular uma figura similar que existente dentro do pacote shap.↩︎"
  },
  {
    "objectID": "posts/2021-09-17_scrapper-boardgamegeek/index.html",
    "href": "posts/2021-09-17_scrapper-boardgamegeek/index.html",
    "title": "Raspando a página do ranking do BoardGameGeek",
    "section": "",
    "text": "Sempre joguei os jogos de tabuleiro mais tradicionais, como Banco Imobiliário, Scotland Yard e War. Esses são jogos muito populares, apesar de cada partida ser muito repetitiva e eles demandarem uma quantidade razoável de jogadores para que tenham graça - e, no meio de uma pandemia, se já acabava sendo chato jogar um deles, a coisa passou a ser impossível. Mas será que não existem alternativas (mais divertidas, inclusive) para continuar com a distração num momento tão difícil como esse? Como eu bem descobri, a resposta estava nos próprios jogos de tabuleiro - mais precisamente, na reinvenção que eles sofreram nas últimas décadas.\nExistem inúmeros jogos de tabuleiro disponíveis atualmente e um número crescente de pessoas que os curtem. Dada esta diversidade de novos títulos, inúmeros portais têm focado em criar e manter essa cultura, trazendo reportagens, fóruns, marketplaces, reviews, rankings e fichas técnicas de cada um deles. Dois exemplos destes sites são o BoardGameGeek e a Ludopedia: ambos possuem praticamente o mesmo conteúdo, mas o primeiro é um portal americano e o segundo é brasileiro. Outro ponto interessante é que o consumo de informações desses portais não precisa ocorrer pelo browser, uma vez que ambos fornecem uma API. A Ludopedia oferece uma API REST bastante intuitiva1, enquanto o BoardGameGeek usa uma API XML que eu acabei achando meio complicada de usar. Mas o que isto tudo tem haver com dados?\nBom, logo que descobri esse hobby, acabei ficando muito perdido sobre quais são os títulos mais legais para se jogar. São tantas as possibilidades e informações disponíveis sobre cada jogo, que eu me peguei navegando entre inúmeras páginas naqueles portais para tentar encontrar aquilo que eu estava buscando. Assim, acabei tendo a ideia de compilar essas informações e colocar tudo dentro de uma linguagem de programação, a fim de deixar a análise de dados me ajudar a encontrar os jogos que mais combinavam com aquilo que eu estava buscando. Para isso, tive a ideia de pegar as informações dos jogos do BoardGameGeek (BGG daqui em diante) através de sua API, tabular tudo o que estava buscando e partir para o abraço. Mas nada é tão simples quanto parece.\nA parede que encontrei é bem chatinha: o request da API XML do BoardGameGeek funciona muito melhor quando usamos o código numérico de identificação do jogo. Quando passamos o nome do jogo para o request, ele precisa estar grafado igual à como está na base do BGG, caso contrário ele pode falhar em trazer o que você está buscando ou trazer todos os títulos que tenham um match parcial com aquele que você buscou (daí para a frente é só caos). Outra ressalva aqui é que essa API não oferece nenhum tipo de método através do qual podemos pegar um de-para dos IDs numéricos para os nomes dos jogos, e o código numérico deles também não é sequencial. Logo, não dá para fazer uma busca gulosa e loopar os IDs de 1 até n. A solução mais simples para o problema é montar a nossa própria base de-para, catando o nome dos títulos e o seu ID numérico de algum lugar do site do BGG - e esse lugar é a página que contém o ranking dos jogos de tabuleiro no site.\nNeste post eu vou mostrar como raspar a página do ranking do BGG, usando como base o fluxo do Web Scrapping que a galera da Curso-R criou (Lente (2018)), e muito bem ilustrada na figura abaixo.\n\nCódigoknitr::include_graphics(path = 'images/web_scrapping_cycle_curso_r.png')\n\n\n\nFluxo do Web Scrapping de acordo com o Lente (2018). Figura copiada de https://blog.curso-r.com/posts/2018-02-18-fluxo-scraping/."
  },
  {
    "objectID": "posts/2021-09-17_scrapper-boardgamegeek/index.html#footnotes",
    "href": "posts/2021-09-17_scrapper-boardgamegeek/index.html#footnotes",
    "title": "Raspando a página do ranking do BoardGameGeek",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nEssa API ainda está em desenvolvimento, e devo escrever sobre o consumo de informações através dela em outro post↩︎"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html",
    "href": "posts/2021-10-18_formula-1/index.html",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "",
    "text": "A Fórmula 1 é um dos esportes de velocidade mais famosos do mundo, com provas ocorrendo desde o início da década de 50 até os dias de hoje. Com um histórico tanto grande desses, com provas tão intrincadas, tantas montadoras, pilotos e acontecimentos, é de se imaginar também que uma enorme quantidade de dados tenham sido gerados em cada prova. Seguindo esta ideia, existe um site1 que hospeda este registro histórico, e que foi o tema de um #TidyTuesday2 recentemente. Achei os dados disponíveis bastante interessantes e, como (toda) pessoa interessada em dados, logo comecei a pensar sobre os tipos de perguntas que poderiam ser respondidos olhando para eles. Mas antes de seguir essa estória, vamos baixar os dados do repositório oficial do TidyTuesday (ou usá-los localmente se você já o tiver baixado).\n\n# carregando os pacotes necessários\nlibrary(tidyverse) # core\nlibrary(fs) # manipular paths\nlibrary(lubridate) # trablhar com datas\nlibrary(ggforce) # extender o ggplot2\nlibrary(broom) # wrangling dos resultados da regressão\nlibrary(metafor) # para a meta-análise\nlibrary(reactable) # tabelas reativas\nlibrary(sparkline) # embedar widgets\nlibrary(patchwork) # compor figuras\nlibrary(tidytuesdayR) # ler os arquivos do tidytuesday\n\n# carregando todos os dados a partir do github do tidytuesday\n# se você quiser baixar os dados direto da fonte é só descomentar \n# como existe um limite de requests que podem ser feitos ao site, resolvi\n# deixar aqui só para referência mesmo\n# tt_dataset &lt;- tt_load(x = 2021, week = 37)\n\n# carregando a copia local dos dados\n## extraindo os paths das copias locais\npaths_copias_locais &lt;- dir_ls(path = 'data/')\n\n## criando vetor de nomes dos arquivos\nnomes_arquivos &lt;- paths_copias_locais %&gt;% \n  path_file() %&gt;% \n  path_ext_remove()\n\n## carregando os arquivos em uma lista\ntt_dataset &lt;- map(.x = paths_copias_locais, .f = read_rds)\n\n## renomeando os elementos da lista\nnames(tt_dataset) &lt;- nomes_arquivos\n\nUma das primeiras coisas que me ocorreram é que ao longo do tempo a tecnologia do setor automobilístico avançou bastante, levando ao desenvolvimento de motores cada vez mais performáticos e potentes. Não só os motores em si devem ter melhorado, mas todas as outras partes dos carros sofreram alterações no intuito de melhorar a sua aerodinâmica e fornecer mais vantagens aos pilotos que os conduzem - seja em termos de manobrabilidade, velocidade, aceleração ou segurança. Neste contexto, eu esperaria que as provas da Fórmula 1 ficassem cada vez mais curtas com o passar dos anos, certo? Então, a primeira coisa que fiz foi montar um histórico com a evolução da duração das provas ao longo dos anos.\nComo primeiro passo para construir este histórico precisei combinar duas tabelas que estão disponíveis junto aos dados - tt_dataset$results e tt_dataset$status. Fiz isso pois a primeira tabela traz o tempo de conclusão de prova de cada piloto, enquanto a segunda tabela é um de-para que nos permite mapear quais pilotos concluíram ou não cada uma delas. Usei esta informação para filtrar a primeira tabela e extrair o tempo de prova (em minutos) de cada piloto que as concluiu. Com base nessa informação, então, calculei o tempo de duração de cada prova como a média do tempo entre todos os pilotos, bem como o desvio padrão deste valor e a quantidade de observações nas quais se baseiam estas estimativas.\n\n\nCódigo\n## adicionando o dicionario com o de-para do statusId\nresultados &lt;- left_join(x = tt_dataset$results,\n                        y = tt_dataset$status,\n                        by = 'statusId')\n\n# criando base com o minimo, media e maximo dos tempos de cada prova\ntempos_de_prova &lt;- resultados %&gt;% \n  # pegando apenas os pilotos que concluiram a prova\n  filter(status == 'Finished') %&gt;% \n  # removendo qualquer valor na coluna milliseconds que não contenha pelo menos um número\n  filter(str_detect(string = milliseconds, pattern = '[0-9]')) %&gt;% \n  # parseando a coluna de milliseconds para numerico\n  mutate(\n    milliseconds = parse_number(milliseconds),\n    # calculando a quantidade de tempo em horas\n    minutos      = (milliseconds / 1000) / 60\n  ) %&gt;% \n  # agrupando pela prova\n  group_by(raceId) %&gt;% \n  # pegando o valor minimo, medio e maximo dos tempo de prova\n  summarise(\n    media  = mean(minutos, na.rm = TRUE),\n    erro   = sd(minutos, na.rm = TRUE),\n    obs    = n()\n  )\n\n## criando a tabela para a visualização\ntempos_de_prova %&gt;% \n  # passando para caracter só para facilitar o plot da tabela\n  mutate(raceId = as.character(raceId)) %&gt;% \n  reactable(striped = TRUE, highlight = TRUE, compact = TRUE, \n            columns = list(\n              media = colDef(name = 'Duração média (min)', format = colFormat(digits = 2)),\n              erro  = colDef(name = 'Desvio padrão', format = colFormat(digits = 2)),\n              obs = colDef(name = 'Observações')\n            ),\n            defaultColDef = colDef(align = 'center',\n                                   footer = function(values) {\n                                     if (!is.numeric(values)) return()\n                                     sparkline(values, type = \"box\", width = 100, height = 30)\n                                   })\n  )\n\n\n\n\n\n\n\nComo podemos ver na figura abaixo, existe uma tendência forte de queda no tempo de duração das provas até a década de 70 e, então, uma desaceleração desta tendência. Além disso, parece haver uma certa variância nesta série temporal ao longo da última década, inclusive com um aparente aumento nos tempos de prova. Isso acabou me surpreendendo, uma vez que a minha expectativa era de que as provas estariam ficando mais curtas. Mas o que será que poderia estar ocorrendo?\n\n\nCódigo\ntempos_de_prova %&gt;% \n  # juntando com as informacoes da data de ocorrência de cada prova\n  left_join(y = select(tt_dataset$races, raceId, date), \n            by = 'raceId') %&gt;% \n  mutate(\n    # parseando a data para date\n    data   = as_date(x = date),\n    # extraindo o ano do objeto de data\n    year   = year(x = data),\n    # calculando a decada onde ocorreu cada prova\n    decada = (year %/% 10) * 10,\n    # passando a decada para caracter, pois quero que o mapeamento de cores\n    # seja feito usando uma escala discreta, e não contínua\n    decada = as.character(decada)\n  ) %&gt;% \n  # criando a figura do historico de tempos de prova\n  ggplot(mapping = aes(x = data, y = media)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = decada), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('1950-01-01'), to = as.Date('2021-12-01'), by = '5 years'),\n               labels = seq(from = 1950, to = 2020, by = 5)) +\n  scale_fill_viridis_d() +\n  labs(\n    title    = 'Série histórica da duração média das provas',\n    subtitle = 'A linha vermelha representa a tendência geral de duração das provas no histórico, enquanto os pontos representam\\na duração de cada uma das provas',\n    x        = 'Período',\n    y        = 'Duração Média (minutos)'\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\n\nEu esperaria que a duração das provas permanecessem relativamente estáveis em pelo menos duas condições. A primeira delas é no caso dos carros não estarem ficando mais rápidos ao longo dos anos. Não dá para avaliar isso tão bem com os dados que estão disponíveis, uma vez esta informação (i.e., fastestLapSpeed) só passou a ser registrado de forma consistente a partir de 2004 (painel A na figura abaixo). Esta série até mostra que a velocidade andou baixando um pouquinho na última década, mas precisávamos mesmo é ver como eram as velocidades lá para trás, quando a série temporal dos tempos de conclusão passou a ficar mais flat.\nA segunda coisa que me ocorreu seria o caso onde houvesse uma mudança sistemática na quantidade de voltas em cada prova, que refletisse aquele mesmo padrão da duração das provas. Embora o painel B da figura abaixo mostre que existe alguma semelhança entre as duas séries temporais, podemos ver que a variância na quantidade de voltas pareceu ficar bem mais tamponada ao longo do tempo. Além disso, a queda observada ao longo das primeiras décadas não é tão forte quanto àquela observada na outra série temporal. Finalmente, ainda que a mesma quantidade de voltas em cada prova sejam dadas hoje e no passado, não me parece razoável acreditar que a velocidade dos carros não variou em nada neste mesmo período. Claro, pode sempre ter alguma regra da FIA que defina alguns padrões que segurassem aqueles comportamentos, mas algo não parece fechar.\n\n\nCódigo\n## mapeando a velocidade maxima por prova\nvelocidades_por_prova &lt;- resultados %&gt;%\n  # parseando a velocidade para numerico\n  mutate(fastestLapSpeed = parse_number(fastestLapSpeed)) %&gt;% \n  # extraindo a velocidade maxima por prova\n  group_by(raceId) %&gt;% \n  filter(fastestLapSpeed == max(fastestLapSpeed, na.rm = TRUE)) %&gt;% \n  ungroup %&gt;% \n  # garantindo que temos valores unicos por prova\n  distinct(raceId, fastestLapSpeed)\n\n## calculando a quantidade de voltas em cada prova\nvoltas_por_prova &lt;- resultados %&gt;% \n  # considerando apenas os pilotos que concluiram cada prova\n  filter(status == 'Finished') %&gt;% \n  # selecionando as colunas de interesse\n  select(raceId, laps) %&gt;% \n  # pegando o valor maximo da quantidade de voltas por prova\n  group_by(raceId) %&gt;% \n  summarise(laps = max(laps))\n\n## mapeando cada circuito à uma prova\nprovas &lt;- left_join(x = tt_dataset$races,\n                    y = tt_dataset$circuits,\n                    by = 'circuitId') %&gt;% \n  # removendo URL da wikipedia\n  select(-contains('url'), -circuitRef, -circuitId, -round, -time) %&gt;%\n  # renomeando o nome do GP e do circuito\n  rename(gp = name.x, circuit = name.y, data = date)\n\n## juntando informacoes\nfeatures_por_prova &lt;- provas %&gt;% \n  ## juntando de voltas por prova\n  left_join(y = voltas_por_prova, by = 'raceId') %&gt;% \n  ## juntando velocidades por prova\n  left_join(y = velocidades_por_prova, by = 'raceId') %&gt;% \n  ## adicionando decada à tabela\n  mutate(decada = (year %/% 10) * 10)\n\n## criando figura do historico de velocidade por prova\nfig1 &lt;- features_por_prova %&gt;% \n  select(data, decada, fastestLapSpeed) %&gt;% \n  drop_na() %&gt;% \n  ggplot(mapping = aes(x = data, y = fastestLapSpeed)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = as.character(decada)), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('2004-01-01'), to = as.Date('2021-01-01'), by = '4 years'),\n               labels = seq(from = 2004, to = 2020, by = 4)) +\n  scale_fill_viridis_d() +\n  labs(\n    title = '(A) Série histórica da velocidade máxima nas provas',\n    x     = 'Período',\n    y     = 'Velocidade máxima (Km/h)'\n  ) +\n  theme(legend.position = 'none')\n\n## criando figura do historico de voltas por prova\nfig2 &lt;- features_por_prova %&gt;% \n  ggplot(mapping = aes(x = data, y = laps)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = as.character(decada)), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('1950-01-01'), to = as.Date('2021-12-01'), by = '10 years'),\n               labels = seq(from = 1950, to = 2020, by = 10)) +\n  scale_fill_viridis_d() +\n  labs(\n    title = '(B) Série histórica da quantidade de voltas por prova',\n    x     = 'Período',\n    y     = 'Quantidade de voltas'\n  ) +\n  theme(legend.position = 'none')\n\n## criando composição\nfig1 / fig2\n\n\n\n\n\nFoi daí que me ocorreu que essa não era uma série temporal convencional. Isto porquê cada corrida dentro de uma temporada vem de uma prova ocorrida em um dado circuito, e poderia ser o caso que o roster de circuitos tivesse mudado bastante entre as temporadas. Mais importante, dada tantas peculiaridades ligadas aos circuitos em si, não faria mais sentido tentar entender o quanto as provas dentro do mesmo circuito têm ou não ficado mais rápidas entre as temporadas? Isto é, será que nossa unidade básica de previsão para entender os padrões de duração das provas não seria o circuito, ao invés de cada prova em si? Afinal, observações vindas de um mesmo circuito não são independentes entre si.\nCom isto em mente, isolei cada circuito e fiz um levantamento do período no qual cada um deles esteve no roster das temporadas. A primeira coisa legal que vi com isso é que nenhum circuito esteve em todas as temporadas da Fórmula 1 (i.e., nenhum segmento é contínuo de ponta à ponta na figura abaixo). O segundo padrão interessante é que são poucos os circuitos que se mantiveram por um período longo de temporadas (i.e., os circuito estão ordenados de cima para baixo, daqueles com o maior volume de provas para o menor volume de provas). Por fim, outro padrão que me saltou aos olhos foi o fato de que alguns circuitos só acabaram estando em uma única temporada mesmo (i.e., os últimos circuitos na figura abaixo) ou, ainda, participado de forma muito intermitente do roster (i.e., segmentos descontínuos e/ou pontos isolados). Em essência, (1) temos uma série temporal sequência de valores com a duração das provas da Fórmula 1 ao longo das temporadas, (2) estes valores não pertencem sempre as mesmas entidades e, (3) quando estas entidades se repetem, pode ser que estejam bem distantes uma das outras no tempo.\n\n\nCódigo\nfeatures_por_prova %&gt;% \n  # pegando as ocorrências únicas de cada circuito em cada temporada\n  distinct(circuit, data) %&gt;% \n  # extraindo o ano a partir da data de ocorrência da corrida\n  mutate(ano = year(data)) %&gt;% \n  # organizando a base de acordo com os anos dentro de cada circuito\n  arrange(circuit, ano) %&gt;% \n  # agrupando pelo circuito\n  group_by(circuit) %&gt;%\n  mutate(\n    # criando uma dummy que será 1 caso a diferença entre o ano de ocorrência\n    # de corridas sucessivas dentro de um mesmo circuito seja 1 ou caso seja\n    # o primeiro registro de prova naquele circuito\n    recorrencia       = (ano - lag(ano)) != 1 | is.na(ano - lag(ano)),\n    # acumulando a dummy de recorrencia, de forma a criar grupos que sinalizem\n    # anos sucessivos onde houve uma prova naquele circuito\n    grupo_recorrencia = cumsum(recorrencia),\n    # calculando a quantidade total provas registradas em cada circuito\n    n_provas          = n()\n  ) %&gt;% \n  # adicionando a recorrencia ao group_by\n  group_by(grupo_recorrencia, .add = TRUE) %&gt;% \n  summarise(\n    # extraindo o ano de inicio de cada fase sucessiva de ocorrência de provas\n    # em cada circuito\n    inicio   = min(ano),\n    # extraindo o ano de fim de cada fase sucessiva de ocorrência de provas\n    # em cada circuito\n    fim      = max(ano),\n    # extraindo a quantidade total de provas em cada circuito\n    n_provas = max(n_provas),\n    # dropando os grupos\n    .groups = 'drop'\n  ) %&gt;% \n  mutate(\n    # reordenando os níveis do circuito de acordo com a quantidade total de provas\n    circuit = fct_reorder(.f = circuit, .x = n_provas, .fun = sum)\n  ) %&gt;% \n  # criando figura para ver o período e/ou ano de ocorrências das provas em cada circuito\n  ggplot() +\n  geom_segment(mapping = aes(x = inicio, xend = fim, y = circuit, yend = circuit)) +\n  geom_point(mapping = aes(x = inicio, y = circuit), size = 2,\n             shape = 21, color = 'black', fill = 'white') +\n  geom_point(mapping = aes(x = fim, y = circuit), size = 2,\n             shape = 21, color = 'black', fill = 'grey70') +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 10)) +\n  labs(\n    title    = 'Ocorrência das provas em cada circuito ao longo dos anos',\n    subtitle = 'Os segmentos representam o intervalo de anos sucessivos nos quais cada circuito esteve no roster',\n    x        = 'Período'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\n\nParece que não tem muito o que fazer para abordarmos analiticamente a evolução da duração das provas da Fórmula 1. Por um lado, a sequência temporal está bem estacionária há uns 50 anos - fato que parece estar associado à constância na quantidade de voltas em cada prova e, mais recentemente, à mudança bem pequena na velocidade dos carros. Por outro lado, parece que parte do que estamos buscando entender pode estar sendo mascarado pelo fato das provas ocorrerem em diferentes circuitos em cada temporada. Se pudéssemos analisar a série temporal de cada circuito, isto nos daria mais insights sobre a real evolução da duração das provas, uma vez que controlamos o efeito do circuito; todavia, como vimos, temos um sashimi de séries temporais e, portanto, não daria para ajustar um modelo estatístico mais tradicional. Como então atacar esse problema?"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#footnotes",
    "href": "posts/2021-10-18_formula-1/index.html#footnotes",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nhttps://ergast.com/mrd/↩︎\nhttps://github.com/rfordatascience/tidytuesday↩︎\nAlém de ser a função utilizada para realizar os cálculos dos tamanhos do efeito quando estamos usando este pacote para fazer uma meta-análise.↩︎\na função calcula o inverso deste argumento internamente, mas é possível passar o inverso da variância já calculada através do argumento W↩︎\nAlgumas pessoas também chamam de modelo de painel.↩︎\nE isto seria algo que poderia nos motivar a buscar entender o porquê disso, também através de uma meta-análise, utilizando moderadores - que é a forma como chamamos as variáveis preditoras na área.↩︎\nEm essência, vamos estimar um intercepto e um slope fixo para todos os circuitos, além de um intercepto aleatório para cada circuito↩︎"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-i",
    "href": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-i",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Abordagem Meta-Analítica I",
    "text": "Abordagem Meta-Analítica I\nO sucesso de uma meta-análise está logo na sua primeira etapa: a definição de, pelo menos, 2 de 4 informações sobre o que queremos fazer:\n\nPopulação: o objeto básico do nosso estudo. Nesta seção, a nossa população será a sequência de provas ocorridas em um circuito;\n\nIntervenção: o tratamento, intervenção ou variável independente que temos interesse em relacionar com a população. Nesta seção, isso será o tempo - mais especificamente, uma sequência de provas ocorrendo de forma ininterrupta ao longo dos anos;\n\nComparação: aquilo que queremos contrastar. Não faremos uso dessa informação nesta seção, mas na próxima;\n\nOutcome: aquilo que quantifica o impacto da intervenção sobre a população - isto é, o que servirá, direta ou indiretamente, para medir o efeito da intervenção na população. Nesta seção, isto será a taxa de variação da duração das provas ao longo dos anos para um circuito.\n\nE como trabalhamos isto na prática? O outcome que vamos buscar nada mais é, nesse caso, do que o slope de uma regressão entre a duração da prova e o tempo (i.e., os anos) para cada um dos circuitos. Se o slope dessa relação for negativo, então é sinal de que as provas estão ficando curtas ao longo das temporadas; caso contrário, as provas estão ficando mais longas. Um detalhe que não podemos perder de vista é o fato de que muitas vezes um circuito sai do roster e depois retorna. Assim, precisaremos ajustar uma regressão para cada uma das sequências ininterruptas de anos nos quais aquele circuito esteve no roster. Como não queremos errar muito na mão e ajustar uma regressão com poucos pontos, vamos colocar uma restrição para só considerarmos dentro da nossa população as sequências de provas que tenham ocorrido pelo menos durante 5 anos sucessivos em cada circuito. O pedaço de código abaixo dá conta de identificar as provas que atendem à essa restrição em cada circuito, e já prepara o dataframe que vamos usar para ajustar as regressões.\n\n## pegando as provas que ocorrem em sequencia\nprovas_alvo &lt;- df %&gt;% \n  # pegando só as informações do id da prova, circuito e ano\n  select(raceId, circuit, year) %&gt;% \n  # organizando a base de acordo com os anos dentro de cada circuito\n  arrange(circuit, year) %&gt;% \n  # agrupando a base pelo circuito\n  group_by(circuit) %&gt;% \n  mutate(\n    # criando uma dummy que será 1 caso a diferença entre o ano de ocorrência\n    # de corridas sucessivas dentro de um mesmo circuito seja 1 ou caso seja\n    # o primeiro registro de prova naquele circuito\n    recorrencia = (year - lag(year)) != 1 | is.na(year - lag(year)),\n    # acumulando a dummy de recorrencia, de forma a criar grupos que sinalizem\n    # anos sucessivos onde houve uma prova naquele circuito\n    grupo_recorrencia = cumsum(recorrencia)\n  ) %&gt;% \n  ungroup %&gt;% \n  # contando quantas vezes cada grupo dentro de cada circuito aparece - i.e., calculando\n  # o tamanho de cada uma das sequências ininterruptas de provas dentro de cada circuito\n  add_count(circuit, grupo_recorrencia, name = 'ocorrencias_continuas') %&gt;% \n  # removendo toda as sequências compostas por menos de 4 anos consecutivos\n  filter(ocorrencias_continuas &gt;= 5) %&gt;% \n  # pegando só o id da prova e o grupo de recorrencia de cada uma\n  select(raceId, grupo_recorrencia)\n\n## criando dataframe com as provas que vamos usar\ndf_regs &lt;- provas_alvo %&gt;% \n  # filtrando as provas que de fato podemos usar\n  left_join(y = df, by = 'raceId')\nglimpse(x = df_regs)\n\nRows: 849\nColumns: 17\n$ raceId            &lt;dbl&gt; 651, 639, 628, 616, 601, 587, 570, 554, 538, 522, 50…\n$ grupo_recorrencia &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ media             &lt;dbl&gt; 102.77994, 91.07974, 89.98567, 89.59983, 89.63631, 5…\n$ erro              &lt;dbl&gt; 0.8427061623, 0.6348897167, 0.4935152496, 0.64426457…\n$ obs               &lt;int&gt; 3, 7, 8, 6, 6, 7, 7, 5, 3, 5, 7, 6, 2, 4, 4, 5, 1, 2…\n$ year              &lt;dbl&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978…\n$ gp                &lt;chr&gt; \"Austrian Grand Prix\", \"Austrian Grand Prix\", \"Austr…\n$ data              &lt;date&gt; 1970-08-16, 1971-08-15, 1972-08-13, 1973-08-19, 197…\n$ circuit           &lt;chr&gt; \"A1-Ring\", \"A1-Ring\", \"A1-Ring\", \"A1-Ring\", \"A1-Ring…\n$ location          &lt;chr&gt; \"Spielberg\", \"Spielberg\", \"Spielberg\", \"Spielberg\", …\n$ country           &lt;chr&gt; \"Austria\", \"Austria\", \"Austria\", \"Austria\", \"Austria…\n$ lat               &lt;dbl&gt; 47.2197, 47.2197, 47.2197, 47.2197, 47.2197, 47.2197…\n$ lng               &lt;dbl&gt; 14.7647, 14.7647, 14.7647, 14.7647, 14.7647, 14.7647…\n$ alt               &lt;dbl&gt; 678, 678, 678, 678, 678, 678, 678, 678, 678, 678, 67…\n$ laps              &lt;dbl&gt; 60, 54, 54, 54, 54, 29, 54, 54, 54, 54, 54, 53, 53, …\n$ fastestLapSpeed   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ decada            &lt;dbl&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970…\n\n\nComo podemos ver, das 1047 provas que tínhamos originalmente, somente 849 atendem ao pré-requisito de terem ocorrido dentro daquela sequência contínua de anos em cada circuito. A figura abaixo tenta mostrar o que queremos fazer até aqui: ajustar uma regressão para cada sequência de anos dentro de cada circuito, extrair os slopes destas regressões e os erros associado à cada um deles.\n\n\nCódigo\n## visualizando regressões\ndf_regs %&gt;% \n  ggplot(mapping = aes(x = year, y = media, color = circuit, group = grupo_recorrencia)) +\n  facet_wrap(~ circuit, scales = 'free') +\n  geom_point(color = 'black', fill = 'grey70', shape = 21) +\n  geom_smooth(color = 'black', linetype = 1, method = 'lm', se = TRUE) +\n  labs(\n    title = 'Regressões aplicadas à cada período dentro de cada circuito',\n    x     = 'Ano',\n    y     = 'Duração Média (minutos)'\n  )\n\n\n\n\n\nCom isto em mente, é hora de ajustar das regressões! Vamos fazer isso de forma tidy, aninhando o dataframe e deixando de fora apenas o circuito e o identificador da sequência de anos dentro de cada um deles. Na sequência, vamos usar a função purrr::map para ajustar uma regressão para prever a duração das provas de acordo com uma sequência de anos. Um ponto importante é que diferenças nos valores do intervalo de anos para cada regressão influenciam diretamente a estimativa do slope de cada uma delas. Portanto, vamos padronizar a variável independente ano dentro de cada recorte, de forma que os slopes estejam livres do confounding do valor dos anos utilizados para ajustá-los. Neste contexto, os slopes vão representar a mudança na duração das provas para cada unidade de desvio padrão do tempo (i.e., anos).\n\ndf_regs &lt;- df_regs %&gt;% \n  # aninhando o dataframe e deixando de fora apenas o circuito e o identificador da \n  # sequencia de anos dentro de cada circuito\n  nest(data = -c(circuit, grupo_recorrencia)) %&gt;% \n  mutate(\n    # padronizando o ano dentro de cada recorte\n    data      = map(.x = data, .f = mutate, year_scaled = (year - mean(year)) / sd(year)),\n    # ajustando uma regressão para cada recorte\n    modelo    = map(.x = data, .f = ~ lm(media ~ year_scaled, data = .x)),\n    # extraindo os coeficientes da regressão\n    tidyed    = map(.x = modelo, .f = tidy),\n    # extraindo o ano maximo dentro de cada recorte\n    ano_max   = map_dbl(.x = data, .f = ~ pull(.x, 'year') %&gt;% max),\n    # extraindo o ano minimo dentro de cada recorte\n    ano_min   = map_dbl(.x = data, .f = ~ pull(.x, 'year') %&gt;% min),\n    # extraindo a quantidade de anos que cada intervalo compreende\n    intervalo = ano_max - ano_min\n  )\n\nRegressões ajustadas, vamos olhar o que conseguimos extrair dos dados. A figura abaixo mostra que os slopes (i.e., estimate) não estão muito relacionados às estimativas de erro, o ano de fim da estimativa e nem o intervalo de anos. Por outro lado e, como era de se esperar, as estimativas de erro parecem ser menores tanto maior forem a quantidade de pontos que usamos para ajustar as regressões. Tirando isso, nada de muito surpreendente nos dados.\n\n\nCódigo\ndf_regs %&gt;% \n  # desempacotando a coluna com as estimativas de cada regressão\n  unnest(tidyed) %&gt;% \n  # pegando só os slopes\n  filter(term == 'year_scaled') %&gt;% \n  # plotando a figura\n  ggplot() +\n  geom_autopoint(shape = 21, color = 'black', fill = 'grey70', alpha = 0.5) +\n  geom_autodensity(color = 'black', fill = 'grey70') +\n  facet_matrix(vars(estimate, std.error, ano_max, intervalo), layer.diag = 2)\n\n\n\n\n\nVamos fazer uma breve pausa para entender o próximo passo da meta-análise. O outcome que estávamos buscando é o slope das regressões da duração das provas vs o tempo para cada sequência ininterrupta de anos nos quais elas ocorreram em cada circuito. Dentro do contexto da meta-análise, utilizaremos estes slopes como uma medida do tamanho do efeito (i.e., effect size): a informação sobre a magnitude e o sinal de uma intervenção sobre a população estudada. É esta a medida que será combinada através do modelo meta-analítico. Aqui estamos usando o slope das regressões como métrica de tamanho do efeito, mas qualquer métrica quantitativa pode ser usada para tal em uma meta-análise: o valor de uma média, uma métrica relacionada a um modelo (e.g., coeficiente de determinação, coeficiente de correlação,…) a um tipo de problema de negócio (e.g., acurácia, AUC,…), informações extraídas de uma tabela de contingência, comparações entre médias (veremos um exemplo deste na próxima seção). O importante é que o tamanho do efeito seja caracterizado por uma métrica quantitativa, comum a todos os estudos na população que estamos estudando. A função escalc do pacote metafor fornece uma visão bastante detalhada sobre as diferentes métricas de tamanho do efeito e os seus casos de uso3.\nUma outra coisa que precisamos é uma estimativa da incerteza ao redor do tamanho do efeito. Isto é importante pois nem todos os estudos possuem a mesma precisão ao estimar as relações que estamos querendo investigar e, se queremos combiná-los, devemos levar em consideração que estimativas mais precisas devem ter um peso maior na nossa análise do que àquelas com maior erro. Esta medida de incerteza normalmente é dada como o inverso da variância do tamanho do efeito, e é utilizada no modelo meta-analítico para ponderar cada estudo. Logo, nesse contexto, um modelo meta-analítico pode ser pensado como um tipo de regressão ponderada. Um curiosidade importante: você só pode chamar uma meta-análise como tal caso esta medida de incerteza seja utilizada para ajustar o modelo; caso contrário (i.e., todos os tamanhos do efeito têm o mesmo peso), a análise feita pode ser chamada apenas de síntese.\nCom esta visão em mente, vamos extrar os slopes de cada um dos modelos ajustados anteriormente, bem como a estimativa do erro associado à cada um deles. Para chegarmos à variância do slope, basta então elevar o valor desse momento ao quadrado.\n\n## desempacotando os resultados das regressoes\ndf_regs &lt;- df_regs %&gt;% \n  # removendo a list column de data e a coluna com o objeto dos modelos ajustados\n  select(-data, - modelo) %&gt;% \n  # desaninhando a list column com os coeficientes das regressões\n  unnest(cols = tidyed) %&gt;% \n  # pegando apenas o slope das regressões\n  filter(term == 'year_scaled') %&gt;% \n  # dropando a coluna com a string do slope\n  select(-term) %&gt;% \n  # calculando a variância do slope, elevando o erro da estimativa ao quadrado\n  mutate(\n    variance = std.error ^ 2\n  )\nrmarkdown::paged_table(x = df_regs)\n\n\n\n  \n\n\n\nTemos tudo pronto, agora é só ajustar o modelo meta-analítica. Para isso, vou utilizar a função rma.mv do pacote metafor para ajustar um modelo meta-analítico de efeitos aleatórios. Este modelo vai combinar os tamanhos do efeito (i.e., slopes) obtidos a partir de cada regressão em um efeito global, ponderando cada observação através de sua variância4. Outro ponto importante é que como alguns circuitos contribuem com mais de uma medida, é necessário considerar esta fonte de não-independência na análise. Fazemos isso especificando o argumento random, e passando uma fórmula que especificará que os tamanhos do efeito estão agrupados através dos níveis da variável circuit. Posto de outra forma, este modelo funciona de forma muito parecida com um modelo de efeitos aleatórios5 ponderado, onde estimamos um intercepto fixo para todas as observações e um intercepto aleatório para cada circuito. Para mais informações, o help dessa função está muito bem documentada.\n\nmodelo_ma_slopes &lt;- rma.mv(yi = estimate, V = variance, random = ~ 1 | circuit, data = df_regs)\nmodelo_ma_slopes\n\n\nMultivariate Meta-Analysis Model (k = 61; method: REML)\n\nVariance Components:\n\n             estim    sqrt  nlvls  fixed   factor \nsigma^2    12.1997  3.4928     42     no  circuit \n\nTest for Heterogeneity:\nQ(df = 60) = 617.5129, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub     \n -1.8680  0.5979  -3.1245  0.0018  -3.0398  -0.6962  ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO output da análise traz algumas informações muito importantes:\n\nO valor em sigma^2 quantifica a variância entre estudos, neste caso mostrando que existe uma variabilidade substancial no slope das regressões entre circuitos;\n\nIsto também é representado através da estatística Q, que é utilizada para testar a hipótese nula de que todas as observações representam uma amostra aleatória homogênea da população de estudo (i.e., os slopes são homogêneos entre os circuitos). O teste de hipótese baseado nesta estatística segue uma distribuição do Qui-Quadrado com K-1 graus de liberdade (onde K é o número de observações) e, nesse caso, sugere que devemos rejeitar a hipótese nula, em favor da hipótese de que os slopes são de fato diferentes entre os circuitos6.\nFinalmente, temos o resultado do modelo, que nos mostra: o valor da estimativa global estimada pelo modelo (estimate), o erro associado à esta estimativa (se), os intervalos de confiança inferior e superior da estimativa (ci.lb e ci.ub, respectivamente), bem como um teste estatístico da hipótese nula de que o efeito global estimado não difere de 0 (i.e., o slope na realidade é flat). Neste caso, podemos ver que rejeitamos esta hipótese nula, o que também fica claro olhássemos só a estimativa do intervalo de confiança.\n\nMas beleza…o que esse último resultado quer nos dizer? Ele mostra que, entre todas as sequências ininterruptas de provas entre os circuitos, a evidência sugere que para cada mudança de um desvio padrão entre os anos (i.e., para cada três anos), as provas ficam cerca de 1.868 minutos mais curtas, com um intervalo de confiança de 3.04 à 0.696 minutos. Isto é, quando levamos em consideração diferenças entre os circuitos, existe evidência para dizer que as provas estão ficando mais curtas sim - mas é tão pouco que chega à dar dó: de 40 segundos à 3 minutos.\nPara fechar essa seção, também podemos ter acesso aos valores dos interceptos aleatórios estimados pelo modelo através da função ranef (i.e., a diferença circuito-específico da estimativa global do modelo). O 0 na figura abaixo representa a estimativa global do modelo, e podemos ver que alguns circuitos têm um desvio consistente para cima (i.e., circuitos nos quais o slope tende a ser mais positivo do que o estimado - círculos azuis) e outros para baixo (i.e., circuitos nos quais o slope tende a ser mais negativo do que o estimado - círculos vermelhos), mas a maioria deles não difere muito do slope global estimado (i.e., círculos vazios).\n\n\nCódigo\nranef(object = modelo_ma_slopes) %&gt;% \n  pluck('circuit') %&gt;% \n  rownames_to_column(var = 'circuit') %&gt;% \n  mutate(\n    circuit  = fct_reorder(.f = circuit, .x = intrcpt, .fun = mean),\n    efeito   = case_when(intrcpt &gt; 0 & pi.lb &gt; 0 ~ 'pos',\n                         intrcpt &lt; 0 & pi.ub &lt; 0 ~ 'neg',\n                         TRUE ~ 'none')\n  ) %&gt;% \n  ggplot(mapping = aes(y = circuit, x = intrcpt)) +\n  geom_vline(xintercept = 0, color = 'grey50') +\n  geom_errorbar(mapping = aes(xmin = pi.lb, xmax = pi.ub), \n                width = 0, size = 0.5, color = 'grey50') +\n  geom_point(mapping = aes(fill = efeito), shape = 21, size = 2.5, color = 'black') +\n  scale_fill_manual(values = c('indianred3', 'white', 'dodgerblue3')) +\n  labs(\n    title    = 'Diferença no tamanho do efeito associado à cada circuito',\n    subtitle = 'Este figura demonstra o quanto cada circuito se afasta da estimativa do tamanho de\nefeito global. Neste contexto, a linha horizontal centrada em zero representaria aquele\nefeito, e a diferença no eixo horizontal é o quanto que uma instância pertencer à cada\ncircuito o modifica.',\n    x        = 'Diferença no tamanho do efeito'\n  ) +\n  theme(\n    legend.position = 'none',\n    axis.title.y    = element_blank()\n  )\n\n\n\n\n\nCom estes resultados, já temos a nossa resposta: sim, as provas estão ficando mais curtas ao longo das temporadas…mas bem pouquinho. Mas…o quanto será que este efeito varia de acordo com a janela de tempo que estamos usando para a comparação?"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-ii",
    "href": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-ii",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Abordagem Meta-Analítica II",
    "text": "Abordagem Meta-Analítica II\nVamos mudar um pouco a abordagem agora, e tentar entender se e o quanto a duração de cada prova tem mudado ano a ano. Isto é, vamos responder à pergunta do quão diferente é o tempo de prova ‘hoje’ quando comparada à cada um dos anos anteriores: se as provas estão ficando mais curtas, então esperamos que a diferença entre o agora e o passado fique cada vez maior tanto mais distante for esse passado. Para isso, vamos mudar um pouco a forma como definimos àquelas informações sobre o passo inicial de uma meta-análise. Nesta seção temos:\n\nPopulação: definida como as provas ocorridas em um circuito. Apesar de parecer ser a mesma população da seção anterior, agora consideraremos todas as provas, não só àquelas que ocorrem em uma sequência ininterrupta de anos;\n\nIntervenção: continuaremos olhando o tempo, mas aqui ele será representado como cada um dos anos nos quais uma prova ocorreu em cada circuito (e, novamente, independentemente destas provas terem ocorrido em uma sequência ininterrupta de anos ou não);\n\nComparação: em alguns casos, queremos fazer uma comparação entre dois ou mais níveis de uma intervenção - e.g. tratamento vs controle. No nosso caso, vamos comparar a duração da prova mais recente em cada circuito com cada um dos tempos nos anos anteriores para aquele mesmo circuito.\nOutcome: como nossa ideia é comparar a duração das provas entre dois anos, vamos focar na diferença entre as médias destes tempos entre os dois anos para cada circutio.\n\nUm ponto importante aqui é que como nosso foco é fazer uma comparação, precisaremos sintetizar esta diferença de alguma forma. Neste contexto, vamos utilizar uma métrica de tamanho de efeito bastante popular em meta-análise: o log response ratio (novamente, o arquivo de ajuda da função metafor::escalc traz muita informação sobre essa métrica e suas aplicações). De forma muito breve, esta métrica é calculada através do logaritimo da razão entre as duas médias e, no nosso caso, será implementado para cada circuito x como:\n\\[\nLRR = log(\\frac{\\bar{x_i}}{\\bar{x_j}})\n\\] onde \\[\\bar{x_i}~=~média~da~duração~da~prova~no~circuito~x~no~ano~i~(i = max(i)),\\] \\[\\bar{x_j}~=~média~da~duração~da~prova~no~mesmo~circuito~x~no~ano~j~(j~&lt;~i)\\]\nDesta forma, se as provas tiverem ficado mais curtas quando comparamos o presente ao passado, então os valores desta métrica serão negativos (e o contrário quando as provas estiverem ficando mais longas). Além disso, essa métrica é de fácil interpretação, o que a torna bastante atraente também (e.g., um LRR = -0.10 representa uma redução de cerca de 10% na duração da prova do ano i quando comparado ao ano j). Como em toda meta-análise, também precisamos calcular a incerteza ao redor da estimativa do tamanho do efeito do LRR (i.e., a variância do LRR). Ele é implementado no código abaixo, onde também calculamos o LRR e extraímos algumas outras informações dos dados.\n\ndf_diffs &lt;- df %&gt;% \n  # selecionando apenas as colunas que vamos usar\n  select(raceId:circuit) %&gt;% \n  # removendo qualquer valor faltante na duração média e erro padrão associado\n  drop_na(media, erro) %&gt;% \n  # organizando a base em ordem decrescente de anos dentro de cada circuito\n  arrange(circuit, -year) %&gt;% \n  # agrupando a base por circuito\n  group_by(circuit) %&gt;% \n  mutate(\n    # calculando o log response ratio para cada observação - log(atual / ti)\n    lrr           = log(first(media) / media),\n    # calculando a estimativa geral de variância de cada observação\n    variancia     = (erro ^ 2) / (obs * (media ^ 2)), \n    # calculando a variância do log response ratio\n    lrr_var       = first(variancia) + variancia,\n    # calculando o intervalo em anos da comparação\n    intervalo     = first(year) - year,\n    # categorizando a variável em torno de intervalos de 5 anos\n    bin_intervalo = as.factor((intervalo %/% 5) * 5)\n  ) %&gt;% \n  # desagrupando o dataframe\n  ungroup %&gt;% \n  # removendo a primeira observação (_i.e._, atual vs atual)\n  filter(intervalo &gt; 0)\nrmarkdown::paged_table(x = df_diffs)\n\n\n\n  \n\n\n\nComo pode ser notado acima, a nossa estratégia foi calcular o tamanho do efeito comparando sempre a duração da prova mais recente disponível em cada circuito vs cada uma das provas ocorridas nos anos anteriores naquele mesmo circuito. Neste sentido, se a prova mais recente disponível para o circuito x tiver ocorrido no ano de 2015 e houverem outras duas provas em 2000 e 1990, então teremos uma instância com o LRR para a comparação 2015 vs 2000 e uma outra para a comparação 2015 vs 1990. Portanto, cada circuito contribui com N - 1 comparações (onde N é o número de provas para aquele circuito), e precisaremos considerar esta fonte de variação e não-independência dos dados modelo.\nOutra informação que extraímos no código foi a diferença de tempo entre o ano mais recente e cada um dos outros anos na comparação, que utilizaremos como moderador (i.e., variável preditora) no modelo. Só para lembrar, a nossa ideia principal nessa seção é averiguar se tanto mais para o passado realizarmos àquela comparação, tanto maior será a diferença observada no LRR. Desta forma, quando observamos uma comparação associada a um intervalo de 5 anos, estamos considerando todas as comparações que envolvam uma prova e àquelas ocorridas até 5 anos antes, e assim sucessivamente. Devido à natureza não-linear da relação entre o intervalo de tempo e o LRR, resolvi por discretizar o intervalo de anos em buckets de 5 anos para facilitar a análise. No entanto, deixei uma linha comentada em que é possível usar esta variável em seu formato contínuo.\nO modelo funciona de forma bastante similar à versão da seção anterior. A única diferença é que agora especificamos o argumento mods, passando o right-hand side da fórmula. Este modelo é conhecido com um modelo meta-analítico de efeitos mistos, pois temos uma variável fixa representada pelo moderador e uma variável aleatória representada pela identidade do circuito7.\n\nmodelo_ma_diffs &lt;- rma.mv(yi = lrr, V = lrr_var, \n                          # mods = ~ poly(x = intervalo, degree = 2, raw = TRUE), \n                          mods = ~ bin_intervalo,\n                          random = ~ 1 | circuit, data = df_diffs)\nmodelo_ma_diffs\n\n\nMultivariate Meta-Analysis Model (k = 946; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2    0.0316  0.1777     62     no  circuit \n\nTest for Residual Heterogeneity:\nQE(df = 931) = 8921303.0919, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:15):\nQM(df = 14) = 2991114.1249, p-val &lt; .0001\n\nModel Results:\n\n                 estimate      se        zval    pval    ci.lb    ci.ub      \nintrcpt           -0.0656  0.0226     -2.9089  0.0036  -0.1099  -0.0214   ** \nbin_intervalo5     0.0487  0.0003    190.9061  &lt;.0001   0.0482   0.0492  *** \nbin_intervalo15    0.0672  0.0003    223.8302  &lt;.0001   0.0666   0.0678  *** \nbin_intervalo20    0.0762  0.0003    249.2053  &lt;.0001   0.0756   0.0768  *** \nbin_intervalo25    0.0576  0.0004    149.2714  &lt;.0001   0.0569   0.0584  *** \nbin_intervalo30    0.0387  0.0004     89.5131  &lt;.0001   0.0378   0.0395  *** \nbin_intervalo10    0.0648  0.0003    224.3725  &lt;.0001   0.0642   0.0653  *** \nbin_intervalo35    0.0340  0.0005     67.5145  &lt;.0001   0.0330   0.0349  *** \nbin_intervalo40   -0.0550  0.0005   -103.1802  &lt;.0001  -0.0561  -0.0540  *** \nbin_intervalo45   -0.0243  0.0006    -41.4670  &lt;.0001  -0.0254  -0.0231  *** \nbin_intervalo50   -0.2137  0.0005   -445.0941  &lt;.0001  -0.2146  -0.2128  *** \nbin_intervalo55   -0.3905  0.0007   -577.1681  &lt;.0001  -0.3918  -0.3892  *** \nbin_intervalo60   -0.4647  0.0005   -883.3099  &lt;.0001  -0.4657  -0.4636  *** \nbin_intervalo65   -0.6387  0.0005  -1164.8388  &lt;.0001  -0.6398  -0.6376  *** \nbin_intervalo70   -0.5334  0.0016   -339.1343  &lt;.0001  -0.5365  -0.5303  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTemos algumas novidades quanto ao output do modelo:\n\nA estatística Q agora é particionada entre QE e QM: o primeiro é a mesma coisa que a definição da estatística Q dada na seção anterior, enquanto o segundo testa a significância estatística do modelo conforme representado pelos efeitos fixos - i.e., a significância do moderador. Neste caso, podemos ver que os tamanhos dos efeitos não representam uma amostra aleatória homogênea da população de estudo (i.e., existem diferenças entre circuitos em termos do LRR; p valor do QE é menor que 0.05) e que o moderador contribui para explicar a diferença no LRR entre os circuitos (i.e., p valor do QM é menor que 0.05);\n\nO model results traz a estimativa não só para o intercepto, mas também para todos os outros níveis do moderador que utilizamos. Todavia, o intercepto aqui representa o primeiro nível do moderador - i.e., provas ocorridas até 5 anos antes da prova mais recente na base para cada circuito. Portanto, este valor de estimate representa o LRR global para este subconjunto de provas, enquanto todos os outros estimates representam a diferença entre os níveis seguintes e este primeiro. Em outras palavras, eles nos informam se existe e qual o tamanho da diferença entre o LRR global para provas ocorridas até 5 anos antes vs provas ocorridas entre 5 à 9 anos antes, vs 10 à 14 anos antes e etc. Explicando assim, fica difícil de entender, então vamos partir para a prática.\n\n\n\nCódigo\n## construindo matriz identidade para traçar o contraste\nmatriz_contraste &lt;- diag(x = 1, nrow = length(unique(df_diffs$bin_intervalo)))\n\n## colocando 1 na primeira coluna para que todo calculo de efeito seja\n## baseado no valor do intercepto + nivel\nmatriz_contraste[, 1] &lt;- 1\n\n## calculando o efeito de cada nível\ncontrastes &lt;- anova(modelo_ma_diffs, L = matriz_contraste) \ncontrastes\n\n\n\nHypotheses:                                  \n1:                    intrcpt = 0 \n2:   intrcpt + bin_intervalo5 = 0 \n3:  intrcpt + bin_intervalo15 = 0 \n4:  intrcpt + bin_intervalo20 = 0 \n5:  intrcpt + bin_intervalo25 = 0 \n6:  intrcpt + bin_intervalo30 = 0 \n7:  intrcpt + bin_intervalo10 = 0 \n8:  intrcpt + bin_intervalo35 = 0 \n9:  intrcpt + bin_intervalo40 = 0 \n10: intrcpt + bin_intervalo45 = 0 \n11: intrcpt + bin_intervalo50 = 0 \n12: intrcpt + bin_intervalo55 = 0 \n13: intrcpt + bin_intervalo60 = 0 \n14: intrcpt + bin_intervalo65 = 0 \n15: intrcpt + bin_intervalo70 = 0 \n\nResults:\n    estimate     se     zval   pval \n1:   -0.0656 0.0226  -2.9089 0.0036 \n2:   -0.0170 0.0226  -0.7517 0.4523 \n3:    0.0016 0.0226   0.0704 0.9439 \n4:    0.0105 0.0226   0.4664 0.6410 \n5:   -0.0080 0.0226  -0.3550 0.7226 \n6:   -0.0270 0.0226  -1.1942 0.2324 \n7:   -0.0009 0.0226  -0.0386 0.9692 \n8:   -0.0317 0.0226  -1.4041 0.1603 \n9:   -0.1207 0.0226  -5.3464 &lt;.0001 \n10:  -0.0899 0.0226  -3.9835 &lt;.0001 \n11:  -0.2793 0.0226 -12.3768 &lt;.0001 \n12:  -0.4562 0.0226 -20.2064 &lt;.0001 \n13:  -0.5303 0.0226 -23.4955 &lt;.0001 \n14:  -0.7043 0.0226 -31.2040 &lt;.0001 \n15:  -0.5990 0.0226 -26.4830 &lt;.0001 \n\nOmnibus Test of Hypotheses:\nQM(df = 15) = 2991119.6531, p-val &lt; .0001\n\n\nA matriz de contraste acima traz alguns resultados muito interessantes e importantes para fecharmos essa estória. Ela mostra que se compararmos as provas mais recentes em cada circuito com àquelas ocorridas:\n\naté 4 anos antes naquele mesmo circuito (intrcpt), existe uma tendência das provas serem ~6% mais curtas;\n\nentre 35 à 70 anos atrás (intrcpt + bin_intervalo35 à intrcpt + bin_intervalo70, respectivamente), também podemos observar que as provas também estão mais curtas. Todavia, essa tendência varia bastante ao longo daquele intervalo: comparadas a ele, as provas estão entre ~3% (e.g., vs 30 anos atrás) à ~70% mais curtas (e.g., vs 65 anos através); e,\n\nentre 5 e 34 anos atrás (intrcpt + bin_intervalo5 à intrcpt + bin_intervalo30, respectivamente), não existe evidência de que as provas estejam ficando mais curtas. Podemos tirar essa conclusão principalmente pelo fato do intervalo de confiança da estimativa (i.e., estimate \\(\\pm\\) 1.96 \\(\\times\\) se) cruzar o valor de 0.\n\nO que estes resultados mostram é que é muito claro que as provas estão mais curtas hoje do que há 4 décadas atrás. Todavia, embora as provas estejam ligeiramente mais curtas quando olhamos o seu passado muito recente (até 4 anos), elas ainda sim têm durações semelhantes àquelas de 1 à 3 décadas atrás. Se, assim como eu, você prefere entender estes resultados visualmente, basta olhar o gráfico abaixo. Um tanto curioso este padrão, não?\n\n\nCódigo\ncontrastes %&gt;% \n  unclass %&gt;% \n  keep(names(.) %in% c('hyp', 'Xb', 'se', 'pval')) %&gt;% \n  map(.f = as.data.frame) %&gt;% \n  bind_cols() %&gt;% \n  as_tibble() %&gt;% \n  set_names(nm = c('intervalo', 'estimate', 'se', 'pval')) %&gt;% \n  mutate(\n    intervalo   = str_extract(string = intervalo, pattern = '(?&lt;=bin_intervalo)[0-9]{1,2}'),\n    intervalo   = as.numeric(intervalo),\n    significant = pval &lt;= 0.05\n  ) %&gt;% \n  replace_na(replace = list(intervalo = 0)) %&gt;% \n  ggplot(mapping = aes(x = intervalo, y = estimate)) +\n  geom_hline(yintercept = 0, color = 'grey40', linetype = 2) +\n  geom_errorbar(mapping = aes(ymin = estimate - 1.96 * se, ymax = estimate + 1.96 * se),\n                width = 1) +\n  geom_point(mapping = aes(fill = significant), shape = 21, size = 2, color = 'black') +\n  scale_x_continuous(breaks = seq(from = 0, to = 70, by = 5)) +\n  scale_y_continuous(breaks = seq(from = -1, to = 0.2, by = 0.2), limits = c(-0.9, 0.2)) +\n  scale_fill_manual(values = c('grey70', 'white')) +\n  labs(\n    title    = 'Diferença entre o tempo de prova mais recente e os anteriores',\n    subtitle = 'As provas ficaram cerca de 10% à 60% mais curtas quando comparadas àquelas mesmas provas quando ocorriam 50\nanos atrás. As provas ficam um pouco mais rápidas quando comparadas ao histórico mais recente (até 5 anos antes),\nmas a duração não muda muito quando comparada à 10 ou 35 anos atrás.',\n    x        = 'Anos de Diferença',\n    y        = 'Log Response Ratio'\n  ) +\n  theme(\n    legend.position = 'none',\n    panel.grid      = element_blank(),\n    axis.line       = element_line()\n  )"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "",
    "text": "Há algum tempo atrás eu explorei o caminho para raspar os dados do ranking do BoardGameGeek1, e consolidei o passo-a-passo nesse post e script. Meu principal interesse naquele momento era que eu precisava obter o código numérico identificador de cada título a fim de poder usar esse valor quando fosse interagir com a API XML do BGG. Como o único lugar em que encontrei essa informação foi no hyperlink para a página de cada título na tabela do ranking, resolvi criar aquele scrapper.\nUma outra fonte de informação sobre jogos de tabuleiro é o site brasileiro da Ludopedia. Este portal tem muita coisa em comum com o BGG, inclusive uma API e uma página de ranking. Todavia, diferente do equivalente gringo, a Ludopedia oferece (1) uma REST API e (2) um meio mais fácil de obter o código identificador de cada título a partir da própria API. De toda forma, no momento em que escrevo este post, ainda não é possível obter as informações da página do ranking diretamente pela API. Desta forma, aqui também existe a possibilidade de exercitar um pouco o web scrapping para a extração dessa informação.\nVou aproveitar esta oportunidade para continuar construindo uma trilha a partir da qual construiremos uma base de dados que nos permitirá responder muitas outras perguntas interessantes, e aplicar técnicas bastante legais de Machine Learning. Falo mais sobre essas idéias ao final desse post."
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#footnotes",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#footnotes",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nBGG daqui para a frente.↩︎\nEssa paginação não estará evidente na primeira vez que você visitar essa página. Entretanto, se você avançar para a próxima página e depois voltar, verá que ela aparecerá na url.↩︎\nEssa informação estava dentro de um atributo chamado Last Page em uma tag div, tornando a extração da informação bem fácil.↩︎\nHavíamos raspado apenas 5 páginas do ranking do BGG, mas cada página contém informações sobre 100 jogos. Portanto, dado que cada página do ranking da Ludopedia contém as informações de 50 jogos, tivemos que raspar 10 páginas.↩︎"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#identificar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#identificar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Identificar",
    "text": "Identificar\nA primeira coisa aqui é navegar até a página-alvo e entender como funciona a sua paginação e onde está o conteúdo que queremos raspar. A figura abaixo mostra um print da primeira página do ranking, onde podemos ver a url que precisaremos visitar bem como constatar que a paginação funciona incrementando a contagem da página (i.e., pagina=1, pagina=2,…)2.\nOutro ponto importante é que as informação que queremos parecem estar em uma tabela, como foi no caso do BGG. Além disso, cada página contém 50 jogos ordenados de forma sequencial de acordo com a sua posição no ranking.\n\nCódigoinclude_graphics(path = 'images/imagem_1.jpg')"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#navegar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#navegar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Navegar",
    "text": "Navegar\nO próximo passo é olhar o fluxo de informação da página a partir da aba Network, acessível através da ferramenta Inspecionar do navegador. Podemos ver que o conteúdo que queremos raspar não é produzido a partir de nenhuma API nem nada parecido, mas totalmente disponível a partir do código HTML mesmo. Além disso, podemos ver que o conteúdo não está organizado dentro de tags de tabela em HTML, mas sim dentro de várias tags div associadas à classe pad-top. Isto já torna o parser deste scrapper diferente daquele do BGG, onde foi bastante simples tabular as informações a partir do código HTML.\n\nCódigoinclude_graphics(path = 'images/imagem_2.jpg')"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#replicar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#replicar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Replicar",
    "text": "Replicar\nVamos então tentar fazer um request da primeira página do ranking e ver o que conseguimos. Isso é feito de forma bem simples, passando apenas a url base para acessar a página e deixando o valor correspondente à página como algo a ser determinado separadamente. Faremos isso usando a função GET do pacote httr.\n\nCódigo## url base do ranking\nbase_url &lt;- 'https://www.ludopedia.com.br/ranking?pagina='\n\n# fazendo o GET\nresultado &lt;- GET(url = str_glue(base_url, 1))\nresultado\n\nResponse [https://ludopedia.com.br/ranking?pagina=1]\n  Date: 2024-02-10 01:56\n  Status: 200\n  Content-Type: text/html; charset=UTF-8\n  Size: 160 kB\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml...\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"\n      xmlns:og=\"http://ogp.me/ns#\"\n      xmlns:fb=\"https://www.facebook.com/2008/fbml\" \n      lang=\"pt-BR\"\n&gt;\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximu...\n    \n...\n\n\nApesar da forma como o conteúdo está disponível nesta página ser diferente daquele do BGG, o request em si parace também ser bem simples!"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#parsear",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#parsear",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Parsear",
    "text": "Parsear\nComo vimos anteriormente, as informações que queremos não estão formatadas e organizadas dentro de tags de tabela em HTML. Portanto, precisaremos identificar e parsear cada uma das informações que queremos usando os respectivos xpath. Para começar, podemos ver que temos acesso ao hyperlink que leva à imagem da capa do jogo se extrairmos o atributo src a partir da classe img-capa dentro da tag img. Isto pode ser uma informação legal se, depois, e.g. quisermos plotar essa imagem como uma célula em uma tabela do reactable.\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando a imagem da capa\n  xml_find_all(xpath = '//img[@class=\"img-capa\"]') %&gt;% \n  # pegando o url\n  xml_attr(attr = 'src') %&gt;% \n  # pegando a primeira observação\n  head(1) %&gt;% \n  # plotando a imagem de uma capa\n  magick::image_read() %&gt;% \n  # aumentando a resolução da imagem\n  magick::image_scale(geometry = '300')\n\n\n\n\nOutra informação legal de buscar é o hyperlink para a página de cada jogo no domínio da Ludopedia. Esta informação está dentro da tag que contém o nome do título (i.e., classe media-heading dentro do header h4), e pode ser obtida extraindo o atributo href de dentro da tag a. Como já conheço a API REST da Ludopedia, sei que essa informação pode ser útil para e.g. raspar o campo de descrição completa do jogo, a fim de utilizar esse texto em alguma análise.\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando todos os links\n  xml_find_all(xpath = 'a') %&gt;% \n  # extraindo o atributo dos hiperlinks\n  xml_attr(attr = 'href') %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \"https://ludopedia.com.br/jogo/brass-birmingham\" \n[2] \"https://ludopedia.com.br/jogo/gaia-project\"     \n[3] \"https://ludopedia.com.br/jogo/terraforming-mars\"\n[4] \"https://ludopedia.com.br/jogo/terra-mystica\"    \n[5] \"https://ludopedia.com.br/jogo/gloomhaven\"       \n[6] \"https://ludopedia.com.br/jogo/brass-lancashire\" \n\n\nA posição do ranking também pode ser extraída a partir da classe media-heading dentro do header h4, olhando a classe rank dentro da tag span…\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o ranking\n  xml_find_all(xpath = 'span[@class=\"rank\"]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \"1º\" \"2º\" \"3º\" \"4º\" \"5º\" \"6º\"\n\n\n…enquanto o nome do jogo pode ser extraído a partir do atributo title dentro da tag a…\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o nome do jogo\n  xml_find_all(xpath = 'a[@title]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \"Brass: Birmingham\" \"Projeto Gaia\"      \"Terraforming Mars\"\n[4] \"Terra Mystica\"     \"Gloomhaven\"        \"Brass: Lancashire\"\n\n\n…o ano de lançamento de cada título vêm do atributo small…\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o ano de lançamento do jogo\n  xml_find_all(xpath = 'small') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \" (2018)\" \" (2017)\" \" (2016)\" \" (2012)\" \" (2017)\" \" (2017)\"\n\n\n…enquanto, finalmente, todas as informações relacionadas às notas podem ser extraídas a partir da classe rank-info dentro da tag div.\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando as notas do jogo\n  xml_find_all(xpath = '//div[@class=\"rank-info\"]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head() %&gt;% \n  # tirando um pouco o excesso de whitespace\n  str_squish()\n\n[1] \"Nota Rank: 9.04 | Média: 9.15 | Notas: 1124 | Sua Nota: -\"\n[2] \"Nota Rank: 9.03 | Média: 9.12 | Notas: 1253 | Sua Nota: -\"\n[3] \"Nota Rank: 9.00 | Média: 9.03 | Notas: 3153 | Sua Nota: -\"\n[4] \"Nota Rank: 8.99 | Média: 9.03 | Notas: 2566 | Sua Nota: -\"\n[5] \"Nota Rank: 8.98 | Média: 9.09 | Notas: 1074 | Sua Nota: -\"\n[6] \"Nota Rank: 8.97 | Média: 9.15 | Notas: 619 | Sua Nota: -\" \n\n\nCom isso, temos um sashimi de parsers para pegar todas as informações que queremos a partir da página do ranking. Vamos agora consolidar esse entendimento e validá-lo na segunda página."
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#validar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#validar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Validar",
    "text": "Validar\nPara auxiliar na tarefa de raspar e parsear a segunda página do ranking, vamos definir duas funções abaixo - uma para cada tarefa. A função pega_pagina recebe a url base do ranking e o número da página que queremos raspar, fazendo então o request da página e salvando o HTML resultante em disco, no diretório definido pelo argumento path. A outra função, parser_pagina, recebe como único argumento o path para o arquivo HTML que a função pega_pagina salvou, e faz o que o próprio nome da função já diz. Ela está bem verbosa, mas o objetivo é mesmo deixar claro o que estamos fazendo.\n\nCódigo# função para fazer o GET\npega_pagina &lt;- function(url_base, pagina, save_dir) {\n  ## junta a base url com o numero da pagina e salva no diretorio alvo\n  GET(url = str_glue(url_base, pagina), \n      write_disk(path = sprintf(fmt = '%s/pagina_%03d.html', save_dir, pagina), \n                 overwrite = TRUE)\n  )\n  \n  # esperanando antes de prosseguir\n  Sys.sleep(runif(n = 1, min = 1, max = 5))\n}\n\n# função para parsear uma pagina\nparser_pagina &lt;- function(path_to_html){\n  \n  ## lendo a pagina raspada\n  pagina_raspada &lt;- read_html(x = path_to_html)\n  \n  ## infos do heading\n  media_head &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//h4[@class=\"media-heading\"]')\n  \n  ## link para a imagem da capa\n  links_da_capa &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//img[@class=\"img-capa\"]') %&gt;% \n    xml_attr(attr = 'src')\n  \n  ## link para a pagina do jogo\n  link_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'a') %&gt;% \n    xml_attr(attr = 'href')\n  \n  ## posicao do ranking de cada titulo\n  posicao_ranking &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'span[@class=\"rank\"]') %&gt;% \n    xml_text()\n  \n  ## nome do jogo\n  titulo_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'a[@title]') %&gt;% \n    xml_text()\n  \n  ## ano de lancamento do jogo\n  ano_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'small') %&gt;% \n    xml_text()\n  \n  ## informacoes gerais das notas\n  notas_jogo &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//div[@class=\"rank-info\"]') %&gt;% \n    xml_text()\n  \n  ## colocando rsultados numa tibble\n  tibble(\n    ranking   = posicao_ranking, \n    titulo    = titulo_jogo, \n    ano       = ano_jogo, # \n    notas     = notas_jogo,\n    link_capa = links_da_capa,\n    link_jogo = link_jogo\n  )\n}\n\n\nCom as funções definidas, agora é hora de utilizá-las! Primeiro, vamos pegar a segunda página e salvá-la em disco…\n\nCódigo# criando uma pasta para colocar os arquivos caso ela nao exista\nif(!dir_exists(path = 'temp/')){\n  dir_create(path = 'temp/')\n}\n\n# pegando a segunda pagina do ranking\npega_pagina(url_base = base_url, pagina = 2, save_dir = 'temp/')\n\n# checando para ver se o html foi baixado\ndir_ls(path = 'temp/', regexp = '.html')\n\ntemp/pagina_002.html\n\n\n…agora vamos parsear a página a partir do arquivo salvo em disco.\n\nCódigoparser_pagina(path_to_html = dir_ls(path = 'temp/', regexp = '.html'))\n\n# A tibble: 50 × 6\n   ranking titulo                                ano   notas link_capa link_jogo\n   &lt;chr&gt;   &lt;chr&gt;                                 &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 51º     Ticket to Ride: Europa                \" (2… \"\\r\\… https://… https://…\n 2 52º     Agricola (Edição Revisada)            \" (2… \"\\r\\… https://… https://…\n 3 53º     As Viagens de Marco Polo              \" (2… \"\\r\\… https://… https://…\n 4 54º     Ticket to Ride: Europa - 15 Anos      \" (2… \"\\r\\… https://… https://…\n 5 55º     On Mars                               \" (2… \"\\r\\… https://… https://…\n 6 56º     Robinson Crusoé: Aventuras na Ilha A… \" (2… \"\\r\\… https://… https://…\n 7 57º     Tiranos da Umbreterna                 \" (2… \"\\r\\… https://… https://…\n 8 58º     World Wonders                         \" (2… \"\\r\\… https://… https://…\n 9 59º     Viticulture - Edição Essencial        \" (2… \"\\r\\… https://… https://…\n10 60º     Mombasa                               \" (2… \"\\r\\… https://… https://…\n# ℹ 40 more rows\n\n\nParece que está tudo ok!"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#iterar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#iterar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Iterar",
    "text": "Iterar\nA ideia agora seria repetir o processo acima, da página 1 até a última página disponível no ranking. Lá no post sobre a raspagem do ranking do BGG vimos que poderíamos descobrir qual o número da última página a partir do próprio código HTML que era raspado3. Faremos algo bem parecido aqui, embora a informação que buscamos não esteja disponível de forma tão clara. Se inspecionarmos o código HTML da página, podemos ver que é possível extrair o número da última página através da url que está em um dos atributos da classe pagination de uma tag ul.\n\nCódigoinclude_graphics(path = 'images/imagem_3.jpg')\n\n\n\n\nPara facilitar nosso trabalho de extração dessa informação aqui, vamos criar e usar a função pega_max_paginas: ela vai olhar dentro daquela classe e extrair o href do atributo title da tag a; a partir daí vamos ter que usar um pouquinho de regex para extrair o número da página em si, uma vez que o resultado original é uma string, e o que desejamos são os números que estão após o padrão pagina=.\n\nCódigo# função para definir o número máximo de páginas para raspar\npega_max_paginas &lt;- function(url_base) {\n  GET(url = str_glue(url_base, 1)) %&gt;% \n    # pegando o conteudo do GET\n    content() %&gt;% \n    # pegando o xpath da paginacao\n    xml_find_all(xpath = '//ul[@class=\"pagination\"]//a[@title=\"Última Página\"]') %&gt;% \n    # pegando o link que contem o numero da pagina maxima\n    xml_attr('href') %&gt;% \n    # pegando o numero da pagina\n    str_extract(pattern = '(?&lt;=pagina=)([0-9]+)') %&gt;% \n    # parseando para numero\n    parse_number()\n}\n\n## definindo qual o numero maximo de paginas para pegar\nultima_pagina &lt;- pega_max_paginas(url_base = base_url)\nultima_pagina\n\n[1] 75\n\n\nComo vimos, temos 75 para raspar, o que pode demorar um pouquinho. No entanto, como a ideia aqui é ser apenas ilustrativo, vou raspar apenas as 10 primeiras páginas e deixarei uma linha comentada com o que deveria ser passado para a função walk caso quiséssemos tudo.\n\nCódigo## pegando as paginas\nwalk(\n  .x = 1:10,\n  # .x = 1:ultima_pagina, # descomentar essa linha se for para raspar tudo\n  .f = pega_pagina,\n  url_base = base_url, save_dir = 'temp/'\n)"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#faxinar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#faxinar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Faxinar",
    "text": "Faxinar\nCom o HTML das páginas, agora devemos organizar e tratar os dados. Para tal, vou extrair o path de todos os arquivos HTML baixados e passá-los para a função map_dfr. Esta função vai se encarregar de aplicar a função parser_pagina ao arquivo HTML associado à cada path e retornar um único tibble com todos os resultados parseados.\n\nCódigo## pegando o path para as paginas\npath_das_paginas &lt;- dir_ls(path = 'temp/', regexp = 'html')\n\n## colocando todas as tabelas em um dataframe so\ndf &lt;- map_dfr(.x = path_das_paginas, .f = parser_pagina)\nrmarkdown::paged_table(x = df)\n\n\n\n  \n\n\n\nJá temos os dados tabulados. Vamos aplicar alguns ajustes a eles: remover o excesso de espaço em branco nas strings, separar as informações sobre as notas em diversas colunas e passar o que for numérico para tal. O código abaixo dá conta disso e nos retorna os dados do ranking tratados.\n\nCódigodf &lt;- df %&gt;% \n  mutate(\n    # parseando o ranking para numerico\n    ranking = parse_number(ranking),\n    # tratando o string titulo do jogo\n    titulo  = str_squish(string = titulo),\n    # parseando o ano para numerico\n    ano     = parse_number(ano),\n    # ajustando a string do campo de nota\n    notas   = str_squish(string = notas),\n  ) %&gt;% \n  # separando a coluna com as informacoes de nota atraves do padrao da barra\n  separate(col = notas, into = c('nota_rank', 'nota_media', 'notas', 'leftover'), sep = '\\\\|') %&gt;% \n  # tratando as informacoes da coluna separada\n  mutate(\n    # nota do ranking\n    nota_rank  = parse_number(nota_rank),\n    # nota dos usuarios\n    nota_media = parse_number(nota_media),\n    # quantidade de notas\n    notas      = parse_number(notas) \n  ) %&gt;% \n  # removendo colunas que nao serao mais necessarias\n  select(-leftover)\nrmarkdown::paged_table(x = df)\n\n\n\n  \n\n\n\nPara concluir, vamos criar uma figura para verificar a relação entre as notas do ranking da Ludopedia, a nota média dada pelos usuários e a quantidade de votos para cada jogo. Essa figura é bastante similar àquela que havíamos criado para o BGG e, inclusive, raspamos 10 páginas neste exemplo aqui justamente para colocar as duas figuras em pé de igualdade4. Apesar desta pequena diferença entre os dois portais, podemos ver padrões similares aqueles já vistos no ranking do BGG:\n\na nota média do jogo de acordo com os usuários parece ser maior do que àquelas do ranking final da Ludopedia;\n\nparece existir uma tendência aos jogos que recebem mais votos também terem maiores notas no ranking; e,\n\nParece que os jogos mais votados são aqueles com menores notas dadas pelos usuários.\n\n\nCódigodf %&gt;% \n  # renomeando as colunas para ficar mais parecido com o plot que fizemos para o BGG\n  rename(nota_ludopedia = nota_rank, nota_usuarios = nota_media, votos = notas) %&gt;% \n  # criando a figura\n  ggplot() +\n  geom_autopoint(alpha = 0.7, shape = 21, fill = 'tomato') +\n  geom_autodensity(mapping = aes(x = .panel_x, y = .panel_y), \n                   fill = 'tomato', color = 'black', alpha = 0.7) +\n  facet_matrix(rows = vars(nota_ludopedia:votos), layer.diag = 2)\n\n\n\n\nNão sei qual era a sua expectativa, mas me surpreende o fato dos padrões serem tão parecidos entre os dois portais dado a diferença que acredito existir entre os públicos brasileiros e estrangeiros."
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "",
    "text": "Uma parte chata de um esporte competitivo e de velocidade como a Fórmula 1 é o risco de ocorrência de um acidente. A história da Fórmula 1 está marcada por estes eventos, e não é preciso ir muito longe para lembrar de casos que marcaram gerações. Em resposta a esse risco, diversas intervenções têm sido implementadas no intuito de reduzir e/ou mitigar a ocorrência de acidentes e suas consequências para os pilotos - conforme informações retiradas do site F1 Insider e resumidas na figura abaixo.\n\nCódigo# carregando os pacotes necessários\nlibrary(tidyverse) # core\n# library(tidytuesdayR) # ler os arquivos do tidytuesday\nlibrary(fs) # manipular paths\nlibrary(lubridate) # trablhar com datas\nlibrary(readxl) # carregar dados de excel\nlibrary(skimr) # descricao do dataframe\nlibrary(ggupset) # para criar um upset plot\nlibrary(patchwork) # para compor figuras\n\n# carregando os dados de dispositivos de segurança na F1\nsafety &lt;- read_excel(path = 'data/f1_safety.xlsx', skip = 1)\n\n# criando a figura principal\nsafety %&gt;% \n  # prepaarando os dados para criar a figura\n  mutate(\n    # imputando o ano atual\n    Now = year(today()),\n    # ordenando as intervencoes po rano\n    Intervention = fct_reorder(.f = Intervention, .x = Year, .fun = min, .desc = TRUE)\n  ) %&gt;% \n  # criando a figura do ano a partir do qual as intervenções foram implementadas\n  ggplot(mapping = aes(x = Year, xend = Now, y = Intervention, yend = Intervention)) +\n  geom_segment() +\n  geom_point(mapping = aes(x = Year), size = 3) +\n  geom_text(mapping = aes(x = Year, label = Year), nudge_x = -4, size = 3) +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 10)) +\n  labs(\n    title    = 'Principais intervenções associadas à segurança dos pilotos ao longo dos anos',\n    subtitle = 'Houve um aumento na quantidade de intervenções relacionadas à segurança entre as décadas de 90\ne o início dos anos 2000.',\n    caption  = 'Fonte: https://f1-insider.com/en/history-of-formula-1-safety/',\n    x        = 'Ano'\n  ) +\n  theme(\n    axis.title.y = element_blank()\n  )\n\n\n\n\nDado o investimento crescente na segurança dos pilotos, uma pergunta que me veio à mente é o quanto isto está relacionado à variação na ocorrência de acidentes entre provas e temporadas. Ao que àquela figura sugere, eu esperaria que a frequência de ocorrência destes eventos estaria em uma decrescente ao longo dos anos, talvez não chegando à zero, mas tornando-os cada vez mais raros. Por sorte, podemos tentar explorar esta ideia utilizando uma base de dados disponível através de um [#TidyTuesday]^(https://github.com/rfordatascience/tidytuesday) que ocorreu há alguns meses atrás. Você pode acessar essa base de dados usando as funções do pacote tidytuesdayR ou apontando diretamente para o arquivo no GitHub; eu optei por baixar uma imagem desses dados e deixá-los disponíveis localmente1.\n\nCódigo# carregando todos os dados a partir do github do tidytuesday\n# se você quiser baixar os dados direto da fonte\n# tt_dataset &lt;- tt_load(x = 2021, week = 37)\n\n# carregando a copia local dos dados\n## extraindo os paths das copias locais\npaths_copias_locais &lt;- dir_ls(path = 'data/', regexp = '.rds')\n\n## criando vetor de nomes dos arquivos\nnomes_arquivos &lt;- paths_copias_locais %&gt;%\n  path_file() %&gt;%\n  path_ext_remove()\n\n## carregando os arquivos em uma lista\ntt_dataset &lt;- map(.x = paths_copias_locais, .f = read_rds)\n\n## renomeando os elementos da lista\nnames(tt_dataset) &lt;- nomes_arquivos\n\n\nAs informações que precisamos para analisar a ocorrência dos acidentes nas provas da Fórmula 1 estão separadas em duas tabelas. A primeira delas, results, contém os resultados e outras informações sobre cada piloto em cada prova. A segunda tabela, status, é uma base de-para que nos ajuda a converter a coluna numérica statusId, comum às duas bases, em um string associado ao que ocorreu com aquele piloto naquela prova. É nesta coluna que está a informação se cada piloto esteve envolvido ou não em um acidente em uma dada prova. Assim, agrupando as observações pelo código identificador da prova, podemos determinar se existe pelo menos um piloto que esteve envolvido em um acidente ou não em cada prova.\nUm ponto importante é que vou considerar apenas os status de Accident e Fatal accident para determinar se um piloto esteve envolvido em um acidente ou não. Até existem outras categorias de status que poderiam caracterizar estes mesmos eventos, tais como, Collision, Injury, Eye injury. No entanto, vou ficar no lado mais seguro, e assumir que apenas àquelas duas primeiras categorias de status é que caracterizam a ocorrência de um acidente. O código abaixo dá conta de juntar as duas tabelas e de definir se houve pelo menos um acidente associado à cada uma das provas.\n\nCódigo## adicionando o dicionario de resultados\nresultados &lt;- left_join(x = tt_dataset$results,\n                        y = tt_dataset$status,\n                        by = 'statusId')\n\n## vetor com as categorias que vou usar como acidente\ncategorias_acidente &lt;- c('Accident', 'Fatal accident')\n\n## juntando padrao de regex para os acidentes\nregex_acidentes &lt;- paste0(categorias_acidente, collapse = '|')\n\n## mapeando os acidentes por prova\nacidentes_por_prova &lt;- resultados %&gt;% \n  # testando se aquele padrão de regex ocorre em cada linha\n  mutate(tem_acidente = str_detect(string = status, pattern = regex_acidentes)) %&gt;% \n  # agrupando as observações pelo identificador da prova\n  group_by(raceId) %&gt;% \n  # testando se existe qualquer linha onde existe algum TRUE\n  summarise(tem_acidente = any(tem_acidente)) \ncount(acidentes_por_prova, tem_acidente, name = 'ocorrencias')\n\n# A tibble: 2 × 2\n  tem_acidente ocorrencias\n  &lt;lgl&gt;              &lt;int&gt;\n1 FALSE                607\n2 TRUE                 441\n\n\nA base de dados sobre a ocorrência de acidentes ao nível das provas é levemente desbalanceada, uma vez que existem 607 provas sem a ocorrência de um acidente contra 441 onde eles ocorreram. Mas com que frequência estes acidentes foram registrados ao longo das temporadas da Fórmula 1 comparado ao aumento nas medidas de segurança para preveni-los?\n\nCódigo# calculando o volume acumulado de medidas de seguranca\nacumulado_medidas &lt;- tibble(\n  # criando uma sequência completa de anos\n  year = seq(from = 1950, to = 2020, by = 1)\n) %&gt;% \n  # juntando com as informacoes das medidas de seguraca por ano\n  left_join(y = safety %&gt;% \n              distinct(Year) %&gt;% \n              mutate(medida = 1), \n            by = c('year' = 'Year')) %&gt;% \n  # adicionando um contador zero para ajudar a fazer a soma acumulada\n  replace_na(replace = list(medida = 0)) %&gt;% \n  # calculando o volume acumulado de medidas de seguranca existentes por temporada\n  mutate(n_medidas = cumsum(medida))\n\n# criando a figura\n## juntando as informacoes da ocorrencia de acidentes por prova com o ano em que a prova ocorreu\nleft_join(x = acidentes_por_prova,\n          y = select(tt_dataset$races, raceId, year),\n          by = 'raceId') %&gt;% \n  # agrupando pela temporada\n  group_by(year) %&gt;% \n  # calculando a proporcao de provas com acidentes por temporada\n  summarise(proporcao = mean(tem_acidente)) %&gt;% \n  # juntando informacoes da quantidade acumulada de medidas de seguranca existentes por temporada\n  left_join(y = acumulado_medidas, by = 'year') %&gt;% \n  # criando a figura\n  ggplot() +\n  geom_line(mapping = aes(x = year, y = n_medidas / 20), color = '#3399E6', linewidth = 1) +\n  geom_line(mapping = aes(x = year, y = proporcao), color = 'tomato1', linewidth = 1) +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 5)) +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ . * 20, \n                                         name = 'Quantidade de medidas de segurança existentes')) +\n  labs(\n    title    = 'Histórico da proporção de provas com acidente e medidas de segurança existentes por temporada',\n    subtitle = 'A frequência de ocorrência de acidentes caiu no final da década de 80 e se manteve assim até o início dos anos 2000, quando ela novamente voltou a aumentar um pouco.\\nNo mesmo período, uma número cada vez mais de medidas de segurança têm sido implementado.',\n    x        = 'Ano/Temporada',\n    y        = 'Proporção de provas com acidente na temporada'\n  ) +\n  theme(\n    axis.title.y.left  = element_text(colour = 'tomato1'),\n    axis.text.y.left   = element_text(colour = 'tomato1', face = 'bold'),\n    axis.title.y.right = element_text(colour = '#3399E6'),\n    axis.text.y.right = element_text(colour = '#3399E6', face = 'bold')\n  )\n\n\n\n\nAtravés da figura acima podemos observar que acidentes eram muito comuns na Fórmula 1 até o final da década de 80, quando eles despencaram2. A partir daí, eles se mantiveram em baixa até meados dos anos 2000, quando eles voltaram a subir, se mantendo em um patamar entre 2003 à 2012 e outro até hoje. Por outro lado, o número de medidas de segurança só têm aumentado, principalmente durante o período entre 1990 e 2005, quando elas praticamente dobraram. Ainda assim, a frequência de ocorrência de acidentes não zerou e, tampouco, continuou apresentando uma tendência de queda com o aumento no número de medidas de segurança nos últimos anos.\nDado os padrões observados, a ocorrência de acidentes parece não estar associada simplesmente à quantidade ou natureza das medidas de segurança implementadas. Isto abre a possibilidade para que outros fatores associados às provas e/ou às temporadas também estejam contribuindo para a variação na frequência destes eventos. Neste contexto, que outros fatores poderiam ser estes? Se nós os conhecermos, seria possível determinar a probabilidade e/ou o potencial de que estes eventos ocorram? São essas as perguntas que eu buscarei abordar aqui[Só para frisar: não é minha intenção prever acidentes! Meu intuito principal aqui será olhar para o histórico dos eventos e tentar entender o conjunto de fatores que parece estar mais frequentemente associado à sua ocorrência. Gerar predições que impeçam estes eventos de ocorrerem está fora do escopo deste post.].[Disclaimer: eu já fui muito fã de Fórmula 1, mas deixei de acompanhar há muito tempo atrás. Neste contexto, é muito provável que eu deixe de fora das minhas análises algumas informações muito valiosas ou de conhecimento geral para os fãs mais assíduos. Assim, peço desculpa de antemão caso isso ocorra e, reforço, que as análises aqui apresentadas estão mais orientadas ao exercício do que algo que seja para valer.]"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#footnotes",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#footnotes",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nUma diferença dos dados que estão disponíveis lá no tidytuesday é que eu acabei criando mais uma tabela com informações adicionais sobre cada prova, que estavam disponíveis na Wikipedia - mas explico sobre isso mais adiante.↩︎\nCoincidentemente ou não, isto parece ter ocorrido um pouco depois que os crash tests foram implementados.↩︎\nUma exceção seria, por exemplo, uma análise orientada à geração de entendimento do fenômeno ou causal, mas aí é outro assunto.↩︎\nE como fazer no caso da primeira prova da temporada, quando não há prova anterior para definir a intensidade da competição? Aqui eu optei por imputar o valor 0, a fim de representar o fato de que a competição estaria em pé de igualdade no início de cada temporada.↩︎\nNo meu uso do tidymodels, acabei vendo que quando passava o 0 ou o 1 como fator e pedia para calcular a sensibilidade, ele estava calculando a especificidade. Por outro lado, quando usava a função que retorna a especificidade, ela me retornava o valor que eu esperava para a sensibilidade. Assim, no código abaixo eu estou invocando a função specificity ou specificity_vec, mas o que ela está me retornando é a métrica de sensibilidade. Examinando os argumentos da função, posteriormente, vi que esse problema seria facilmente resolvido setando alguns dos parâmetros ali, mas optei por não fazer isso e deixar como já estava.↩︎\nSó para lembrar, temos 607 instâncias de provas sem acidentes e 441 provas com acidentes.↩︎\nExistem muito mais hiperparâmetros disponíveis na implementação original do pacote ranger no R e no Scikit-Learn do Python, mas apenas estes três estão disponíveis dentro do wrapper do tidymodels↩︎\ne.g., já ouvi uma palestra uma vez falando da resistência que os desenvolvedores tinham em criar o pacote workflowsets, relacionado ao risco de transformar o tidymodels em algo que você só fica ajustando algoritmo sem pensar no que está fazendo.↩︎"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#considerações-gerais",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#considerações-gerais",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Considerações gerais",
    "text": "Considerações gerais\nDefinir precisamente a pergunta que queremos responder e/ou o objetivo que queremos alcançar é o passo principal para a criação da base analítica correta: é isto que define qual será a variável resposta bem como a sua granularidade. No caso apresentado aqui, buscarei definir a probabilidade de ocorrência de um acidente em cada uma das provas da Fórmula 1 no registro histórico. Como consequência, precisarei montar uma base analítica na qual tenhamos uma observação por prova (i.e., a granularidade) e, associado à elas, um indicador se houve ou não um acidente (i.e., a variável resposta).\nO segundo passo importante é definir quais variáveis utilizar para auxiliar na tarefa de previsão. Meu ponto com isto não está na seleção de variáveis per se, mas sim na criação de variáveis para ajudar a endereçar a pergunta e/ou objetivo proposta. É bem relevante traçar essa diferença, pois é bastante comum encontrar bases de dados prontas para ajustar os algoritmos mas, na vida real, é muito raro encontrar essas coisas já desenhadas para nós. Além disso, eu tenho a impressão de que raramente refletimos sobre o processo de ideação que levou àquelas variáveis à estarem naquela tabela. Isso daria uma discussão longa por si só, mas gostaria de aproveitar a oportunidade para comentar sobre três coisas relevantes nesta tarefa. Todas elas parecerão óbvias mas, novamente, acho que de tempos em tempos é relevante relembrarmos destas coisas.\n\nAs variáveis criadas precisam estar na mesma granularidade da variável resposta. Isto significa que as variáveis que vou utilizar precisam descrever características ao nível de cada prova. Se eu estivesse buscando gerar previsões para a temporada, então faria mais sentido que as variáveis utilizadas descrevessem características relacionadas à cada temporada. Em outras palavras, as variáveis devem sempre descrever aquilo que estamos buscando analisar;\n\nAs variáveis utilizadas para a previsão precisam estar disponíveis antes que o fenômeno analisado ocorra. Isto é, se queremos antecipar a probabilidade de ocorrência de um acidente em uma prova, não podemos utilizar informações que estariam disponíveis apenas após a conclusão daquela prova3. Relacionado a este ponto, também é importante estar atento para não incluir variáveis que estão obviamente confundidas com aquilo que queremos prever. Por exemplo, a ocorrência de acidentes pode colocar o safety car na pista, de forma que a distância percorrida pelos pilotos em um circuito ou o tempo de prova podem acabar aumentando. Neste caso, ambas as informações estariam disponíveis apenas após a conclusão da prova e, também, confundidas com aquilo que queremos prever. Assim, estas duas variáveis teriam um potencial bastante pequeno de fornecer insights úteis para a análise.\nFinalmente, é muito importante que as variáveis criadas estejam bem embasadas em hipóteses, perguntas e expectativas pré-definidas. Em outras palavras, nós não saímos jogando variáveis dentro de uma base analítica só porque podemos fazer isso. É preciso pensar sobre o que elas estão representando, de que forma podem estar contribuindo para o fenômeno analisado e, também, quão razoável é a relação que imaginamos existir. Dificilmente conseguiremos representar uma relação complexa entre um padrão e um processo usando uma única variável; se o objetivo da análise for a predição e você se encontrar em uma situação como essa, talvez valha a pena redesenhar a pergunta ou a abordagem que está sendo seguida.\n\nCom estas considerações em mente, vou criar algumas variáveis que estarão associadas direta ou indiretamente à hipóteses e perguntas relacionadas à ocorrência dos acidentes em cada prova."
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#criação-das-variáveis",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#criação-das-variáveis",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Criação das variáveis",
    "text": "Criação das variáveis\nComeço falando de três hipóteses relacionadas à informações que não temos na base de dados original. Uma hipótese é de que provas que ocorram sob condições do tempo mais chuvosas ou similares têm maior probabilidade de ter um acidente; o porquê disso me parece bem intuitivo, então não vou me ater muito aqui. Uma segunda hipótese é que provas que ocorram fora dos autódromos (e.g., em circuitos de rua ou adaptados) possuem maior chance de ocorrência de um acidente, talvez por não possuírem a infraestrutura necessária para preveni-los ou por apresentarem uma rota muito sinuosa e errática. Finalmente, uma outra hipótese seria que provas em circuitos mais longos teriam maior probabilidade de ocorrência de um acidente, uma vez que e.g. dirigir nestes circuitos seria mais cansativo. Apesar das informações relacionadas à estas hipóteses não estarem na base original, é muito fácil obtê-las a partir da Wikipedia.\nPara extrair àquelas informações da Wikipedia utilizei a url da Wikipedia que está relacionada à cada prova, e disponível na tabela races. Eu criei um scrapper para ir até uma dessas páginas e extrair as informações e metadados sobre cada cada uma das provas. A partir daí eu fiz um parser e montei a base de dados que ajusto abaixo. Você encontra esse scrapper dentro da pasta deste post, neste link.\n\ntempo_durante_prova &lt;- tt_dataset$weather_and_other %&gt;% \n  # removendo a coluna de voltas, pois ja temos ela na base de dados\n  select(-voltas) %&gt;% \n  # agrupando operacao seguinte linha a linha\n  rowwise(raceId) %&gt;% \n  # ajustando features relacionadas ao tempo durante a prova\n  mutate(\n    # flag para quando a informação estiver indisponivel\n    ausente = as.numeric(is.na(sum(c_across(nublado:quente)))),\n    # flag para quando o tempo nao for nenhuma das outras alternativas \n    outro   = as.numeric(sum(c_across(nublado:quente)) == 0)\n  ) %&gt;%\n  # desagrupando a estrutura rowwise da tabela\n  ungroup() %&gt;% \n  # preenchendo os NAs com 0 apenas nas colunas com as flags do tempo durante a prova\n  mutate(across(nublado:outro, \\(x) replace_na(data = x, replace = 0)))\nrmarkdown::paged_table(x = tempo_durante_prova)\n\n\n\n  \n\n\n\nOutra coisa que poderia estar associada à ocorrência de acidentes está na própria intensidade e assimetria da disputa pelo campeonato em cada temporada. Por exemplo, poderia ser o caso de que observemos uma maior frequência de acidentes quando a disputa pelo título da temporada estiver mais intenso. Por outro lado, também seria razoável imaginar que quando os pilotos diferem muito em sua habilidade e, consequentemente, na pontuação obtida ao longo da temporada, maior seria a probabilidade de ocorrência de um acidente. Neste caso, isto ocorreria pois os pilotos menos habilidosos acabariam ‘atrapalhando’ os demais, provocando e/ou os envolvendo em mais acidentes. Finalmente, também é possível que a ocorrência de acidentes esteja associada à combinação entre estes dois fatores: pilotos muito habilidosos e competitivos ousando na pista e pilotos menos habilidosos só atrapalhando\nVou representar estas hipóteses utilizando as informações sobre a pontuação de cada piloto existente na tabela driver_standings. Esta tabela traz o somatório das pontuações dos pilotos em cada prova da temporada e, portanto, nos dá uma ideia da evolução da competição pelo campeonato prova a prova. Com base nestes dados, calculei o coeficiente de variação da pontuação entre os cinco pilotos mais bem posicionados à cada rodada em cada temporada e, também, o coeficiente de variação da pontuação entre todos os pilotos em cada rodada de cada temporada. Em ambos os casos, um coeficiente de variação menor do que 1 sugere que diferenças na pontuação entre pilotos é muito pequena e, portanto, a intensidade da competição é mais forte e simétrica, enquanto valores maiores que 1 indicam o contrário.\nUm detalhe importante aqui é que estas variáveis estariam disponíveis apenas após a conclusão de cada prova. No entanto, a ideia é que estas variáveis sejam utilizadas para representar a intensidade e assimetria da competição antes de cada prova iniciar. Assim, vou utilizar o coeficiente de variação da pontuação até a prova anterior como input para caracterizar a intensidade da competição na prova atual. Isto é, a intensidade e assimetria da competição prova i será caracterizada pela informação disponível até a prova i - 1, garantindo assim que tenhamos acesso à informação antes de cada prova se iniciar4.\n\n## juntando as informacoes das provas com a pontuacao por piloto por prova, round e temporada\ndisputa_por_prova &lt;- left_join(x = tt_dataset$driver_standings,\n                               y = select(tt_dataset$races, raceId, year, round),\n                               by = 'raceId') %&gt;% \n  # organizando pontuação por ano e prova em ordem decrescente, da primeira à última prova\n  arrange(year, round, desc(points)) %&gt;% \n  # agrupando por ano e prova\n  group_by(year, raceId) %&gt;% \n  # criando uma mascara para nos ajuda a calcular as metricas apenas considerando \n  # a pontuacao dos pilotos no topo do ranking da temporada por prova e ano\n  mutate(\n    mask_positions = ifelse(test = row_number() &lt;= 5, yes = 1, no = NA)\n  ) %&gt;%\n  # sumarizando metricas para representar força da disputa à cada prova e ano\n  summarise(\n    # coeficiente de variação da pontuação dos cinco pilotos com mais pontos à \n    # cada prova concluída em cada temporada\n    cv_top_pilots = sd(points * mask_positions, na.rm = TRUE) / mean(points * mask_positions, na.rm = TRUE),\n    # coeficiente de variação da pontuação de todos os pilotos à cada prova\n    # concluída em cada temporada\n    cv_all_pilots = sd(points) / mean(points), \n    .groups = 'drop'\n  ) %&gt;% \n  # garantindo ordenamento por ano e prova\n  arrange(year, raceId) %&gt;% \n  # agrupando por ano\n  group_by(year) %&gt;% \n  # imputando o coeficiente de variacao das metricas da corrida prova anterior \n  # para prever o resultado da corrida atual - não dá para usar o dado de uma\n  # informação obtida após a prova para prever o que \n  # acontecerá durante a prova\n  mutate(cvl_top_pilots = lag(cv_top_pilots), cvl_all_pilots = lag(cv_all_pilots)) %&gt;% \n  # dropando o grupo\n  ungroup %&gt;% \n  # substituindo os NAs por zero - no inicio de cada temporada não houve nenhuma\n  # prova anterior e, portanto, vamos assumir que não há uma competição intensa \n  # neste momento\n  replace_na(replace = list(cvl_top_pilots = 0, cvl_all_pilots = 0)) %&gt;% \n  # dropando a coluna de ano\n  select(-year)\nrmarkdown::paged_table(x = disputa_por_prova)\n\n\n\n  \n\n\n\nTambém aproveito os dados disponíveis para criar um outro conjunto de variáveis que podem estar relacionadas à frequência de ocorrência de acidentes. Mais especificamente, estas variáveis ajudam a respoder algumas perguntas, tais como:\n\nComo a quantidade de participantes (i.e., pilotos e construtores) contribui para a ocorrência de acidentes?\n\nProvas com mais voltas têm maior frequência de acidentes?\n\nA frequência de acidentes está relacionada ao período do ano (i.e., o mês) ou do dia (i.e., manhã ou tarde) em que as provas são realizadas?\n\nExistem diferenças entre os circuitos em termos da probabilidade de ocorrência de um acidente?\n\nO pedaço de código abaixo cria estas variáveis e consolida muitas outras relacionadas à estas mesmas perguntas.\n\n## quantidade de pilotos e construtores por prova\nparticipantes_por_prova &lt;- resultados %&gt;% \n  # pegando os valores unicos das chaves primarias por prova\n  distinct(raceId, driverId, constructorId) %&gt;% \n  # agrupando por prova\n  group_by(raceId) %&gt;% \n  # quantidade de pilotos e construtores por prova\n  summarise(\n    n_pilotos      = n_distinct(driverId),\n    n_construtores = n_distinct(constructorId)\n  )\n\n## calculando a quantidade de voltas em cada prova\nvoltas_por_prova &lt;- resultados %&gt;% \n  # considerando apenas os pilotos que concluiram cada prova\n  filter(status == 'Finished') %&gt;% \n  # selecionando as colunas de interesse\n  select(raceId, laps) %&gt;% \n  # pegando o valor maximo da quantidade de voltas por prova\n  group_by(raceId) %&gt;% \n  summarise(laps = max(laps))\n\n## mapeando cada circuito à uma prova\nprovas &lt;- left_join(x = tt_dataset$races,\n                    y = tt_dataset$circuits,\n                    by = 'circuitId') %&gt;% \n  # removendo URL da wikipedia\n  select(-contains('url'), -circuitRef, -circuitId, -round) %&gt;%\n  # renomeando o nome do GP e do circuito\n  rename(gp = name.x, circuit = name.y) %&gt;% \n  # levantando informacoes de data\n  mutate(\n    data     = paste(date, time),\n    data     = as_datetime(data),\n    mes      = month(data),\n    semana   = isoweek(x = data),\n    turno_pm = pm(data),\n    decada   = (year %/% 10) * 10\n  ) %&gt;% \n  select(-date, -time)\n\nUma vez que tenhamos calculado todas as variáveis para ajudar na modelagem, vou juntá-las em uma tabela só. Farei isso usando um left_join entre todas as bases, com a raceId (i.e., identificador único de cada prova) como chave primária para todas as tabelas.\n\n## juntando informacoes das provas com a quantidade de participantes e construtores\nfeatures_por_prova &lt;- left_join(x = provas,\n                             y = participantes_por_prova,\n                             by = 'raceId') %&gt;% \n  ## juntando de voltas por prova\n  left_join(y = voltas_por_prova, by = 'raceId') %&gt;% \n  ## juntando informacoes da intensidade da disputa\n  left_join(y = disputa_por_prova, by = 'raceId') %&gt;% \n  ## juntando informacoes do tempo em cada prova\n  left_join(y = tempo_durante_prova, by = 'raceId')\nrmarkdown::paged_table(x = features_por_prova)\n\n\n\n  \n\n\n\nPara concluir a preparação da base analítica, vou usar mais um left_join para juntar as observações da variável resposta (i.e., ocorrência ou não de um acidente em cada prova) com as informações das variáveis preditoras (i.e., todas as informações que caracterizam cada uma das provas).\n\ndf &lt;- left_join(x = acidentes_por_prova, y = features_por_prova, by = 'raceId')\nrmarkdown::paged_table(x = df)"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#definição-das-métricas",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#definição-das-métricas",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Definição das métricas",
    "text": "Definição das métricas\nO objetivo aqui é gerar um modelo que seja capaz de definir a probabilidade de ocorrência de um acidente em uma prova. Além disso, gostaria que este modelo fosse tão bom em dizer quando esperar que um acidente ocorra quanto quando ele não ocorra. Finalmente, é importante que este modelo seja capaz de prever corretamente as vezes nas quais um acidente ocorrerá. Dado estes três requisitos requisitos, vou focar em avaliar o modelo através dos valores de AUC. Esta métrica representa o trade-off entre a sensibilidade (i.e., capacidade de prever os acidentes que ocorreram) e a especificidade (i.e., capacidade de prever os acidentes que não ocorreram), e é baseada nas probabilidades que o modelo gera. Esta métrica também pode ser interpretada como a probabilidade de que uma observação amostrada aleatoriamente a partir das previsões do modelo esteja certa ao dizer que haverá um acidente. Também aproveitarei para monitorar o log loss (i.e., representa quão bem calibradas as probabilidades geradas pelo modelo estão) e a sensibilidade (i.e., métrica baseada na classificação binária, obtida após a conversão das probabilidades em uma decisão)5.\n\nCódigo# carregando os pacotes para a analise de dados\nlibrary(tidymodels) # core para ajustar os modelos\nlibrary(finetune) # selecao de hiperparametros\n\n# definindo as tres metricas que vamos monitorar: log loss, AUC e especificidade\nmetricas &lt;- metric_set(roc_auc, mn_log_loss, specificity)"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#separação-dos-dados",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#separação-dos-dados",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Separação dos dados",
    "text": "Separação dos dados\nVou começar separando a base de treino da base de teste, separando 75% dos dados para a primeira e o restante para a segunda. Vou aproveitar e estratificar esta separação, garantindo que haverá a mesma proporção de provas com e sem acidentes entre as duas bases.\n\nCódigo# passando o target para um fator e a variavel de turno para numerico\ndf &lt;- mutate(df, \n             tem_acidente = as.factor(ifelse(test = tem_acidente, yes = 1L, no = 0L)),\n             turno_pm     = as.numeric(turno_pm)\n)\n\n# definindo a seed para a reprodutibilidade\nset.seed(64)\n# fazendo o split da base em treino e teste\ntrain_test_split &lt;- initial_split(data = df, prop = 0.75, strata = tem_acidente)\ntrain_test_split\n\n&lt;Training/Testing/Total&gt;\n&lt;785/263/1048&gt;\n\n\nComo vou usar uma validação cruzada para avaliar a performance do modelo, faço a separação da base de treino em 10 folds abaixo - novamente, estratificando a separação de forma que tenhamos uma proporção similar de provas com e sem acidentes entre os folds.\n\nCódigo# definindo a seed para a reprodutibilidade\nset.seed(128)\n# criando o esquema para a validação cruzada\nkfold &lt;- vfold_cv(data = training(x = train_test_split), v = 10, strata = tem_acidente)\nkfold\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [706/79]&gt; Fold01\n 2 &lt;split [706/79]&gt; Fold02\n 3 &lt;split [706/79]&gt; Fold03\n 4 &lt;split [706/79]&gt; Fold04\n 5 &lt;split [706/79]&gt; Fold05\n 6 &lt;split [707/78]&gt; Fold06\n 7 &lt;split [707/78]&gt; Fold07\n 8 &lt;split [707/78]&gt; Fold08\n 9 &lt;split [707/78]&gt; Fold09\n10 &lt;split [707/78]&gt; Fold10"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-dos-dados-1",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-dos-dados-1",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Preparação dos dados",
    "text": "Preparação dos dados\nO próximo passo é preparar uma receita que vai definir a fórmula do modelo (i.e., variável resposta e preditoras) bem como todas as etapas de pré-processamento dos dados. Os passos principais dessa receita são tratar a coluna raceId como a chave primária da base, criar uma feature de mês a partir da coluna de data, remover colunas que não serão usadas e fazer um one-hot-encoding das variáveis categóricas de identidade do circuito e mês. Nesse último ponto, aproveito também para criar um nível para os níveis não vistos da variável de circuito (útil quando um nível está no fold de treino mas não no de validação) e para agregar os níveis menos frequentes desta variável em um nível único.\n\nCódigopre_processamento &lt;- recipe(tem_acidente ~ ., data = training(x = train_test_split)) %&gt;%\n  # passando a raceId para a role de id da base\n  update_role(raceId, new_role = 'id') %&gt;%\n  # adicionando o mes\n  step_date(data, features = 'month', abbr = FALSE) %&gt;% \n  # removendo as colunas que nao sao necessarias para a modelagem\n  step_rm(c(gp, location:data, semana, distancia, decada, \n            cv_top_pilots, cv_all_pilots, n_construtores, mes)) %&gt;%\n  # criando um step para associar circuitos nao vistos a um novo valor\n  step_novel(circuit) %&gt;%\n  # agrupando circuitos menos frequentes\n  step_other(circuit, threshold = 5, other = 'Other') %&gt;%\n  # fazendo o one hot encoding dos circuitos\n  step_dummy(circuit, data_month, one_hot = FALSE)\npre_processamento"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#definição-do-baseline",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#definição-do-baseline",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Definição do baseline",
    "text": "Definição do baseline\nÉ sempre importante definirmos tantos baselines quantos forem necessários, a fim de verificar se a solução que estamos buscando desenvolver agrega mais valor do que uma regra de negócio ou uma abordagem mais simples. Relacionado a este último ponto, um modelo preditivo em sua versão default pode ser utilizado como um baseline para determinar se o esforço de tunagem de hiperparâmetros está resultando em um ganho de performance ou não. Neste contexto, vou traçar dois baselines aqui antes de implementar a tunagem de hiperparâmetros.\nNaive\nA primeira abordagem será muito simples, e seguirá uma regrinha bastante comum para problemas de classificação quando temos uma base de dados desbalanceada6: prever que todas as instâncias descrevem a categoria majoritária. Para isso, vou desempacotar os dados de cada fold e criar uma coluna toda cheia de zeros (i.e., a representação das provas sem acidente em nossa base de dados); com isso, calcularei então o valor das métricas que serão monitoradas para cada fold e, então, tirar a média destes valores entre todos eles. Os valores obtidos com isso representam o baseline das nossas métricas de classificação se seguíssemos uma regra de negócio que diria que não haverá acidente em nenhuma prova.\n\nCódigokfold %&gt;% \n  # extraindo o target a partir de cada fold\n  mutate(\n    obs_target = map(.x = splits, .f = ~ training(.x) %&gt;% pull(tem_acidente))\n  ) %&gt;% \n  # pegando so o id do fold e a list column com o target\n  select(id, obs_target) %&gt;% \n  # desempacotando os targets\n  unnest(obs_target) %&gt;% \n  # criando colunas com a previsao naive a fim de definir um baseline sem um modelo\n  # nesse caso, vamos sempre prever que ocorrera um acidente em cada prova, independentemente\n  # de qualquer informacao que tenhamos sobre elas\n  mutate(\n    # criando um target numerico para que ele seja usado para calcular o auc e log loss\n    prd_target_num = 0, \n    # criando um target no formato fator para calcular a especificidade\n    prd_target_fct = factor(x = 0L, levels = c(0L, 1L)),\n    # target\n  ) %&gt;% \n  # agrupando pelo id do fold\n  group_by(id) %&gt;% \n  # calculando cada uma das metricas para cada fold\n  summarise(\n    log_loss       = mn_log_loss_vec(truth = obs_target, estimate = prd_target_num),\n    auc            = roc_auc_vec(truth = obs_target, estimate = prd_target_num),\n    sensibilidade  = specificity_vec(truth = obs_target, estimate = prd_target_fct),\n    .groups = 'drop'\n  ) %&gt;% \n  # tirando a media de cada metrica entre todos os folds\n  summarise(\n    across(log_loss:sensibilidade, mean)\n  )\n\n# A tibble: 1 × 3\n  log_loss   auc sensibilidade\n     &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1     20.9   0.5             0\n\n\nModelo\nO segundo baseline que vou definir será através de um algoritmo em sua versão default. Vou focar em um algoritmo de Random Forest neste post, só para praticar mesmo. Este algoritmo é bastante popular e, normalmente, consegue uma performance razoável em problemas de classificação e regressão dado a baixa complexidade envolvida em seu processamento. A ideia geral deste algoritmo é gerar uma previsão com base em um ensemble de árvores de decisão (daí a parte Forest no nome do algoritmo), cada uma das quais é crescida com base em um subconjunto diferente das instâncias e das variáveis disponíveis (daí a parte Random no nome do algoritmo).\nNo código abaixo, instanciamos a Random Forest utilizando a engine do pacote ranger e, então, consolidamos um workflow onde os dados de cada fold serão pré-processados utilizando a receita que havíamos preparado anteriormente. Na sequência, vamos fazer este ajuste e coletar a performance da Random Forest default entre todos os folds da validação cruzada.\n\nCódigo# cria uma instancia do algoritmo com valores default\nalgoritmo_baseline &lt;- rand_forest() %&gt;%\n  set_engine(engine = 'ranger', importance = 'impurity') %&gt;%\n  set_mode(mode = 'classification')\n\n# define um workflow com a receita do pre-processamento e o algoritmo\nwf_baseline &lt;- workflow() %&gt;% \n  add_recipe(recipe = pre_processamento) %&gt;% \n  add_model(spec = algoritmo_baseline)\n\n# setando a seed para a reprodutibilidade\nset.seed(256)\n\n# ajusta o workflow do baseline aos folds\nfit_baseline &lt;- wf_baseline %&gt;% \n  fit_resamples(resamples = kfold, \n                metrics = metricas, \n                control = control_resamples(verbose = TRUE, allow_par = TRUE)\n  )\n\n# extrai as metricas do baseline\ncollect_metrics(x = fit_baseline)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary     0.603    10  0.0105 Preprocessor1_Model1\n2 roc_auc     binary     0.730    10  0.0125 Preprocessor1_Model1\n3 specificity binary     0.597    10  0.0136 Preprocessor1_Model1\n\n\nFica muito claro que o modelo preditivo supera e muito o baseline definido usando a regra de negócio anterior. Posto de outra forma, os valores obtidos aqui sugerem que um modelo de aprendizado de máquina pode agregar valor para tentar antecipar quando existe um maior potencial de ocorrência de acidentes em uma prova. Como este baseline pronto, vamos agora à tunagem dos hiperparâmetros da Random Forest, na tentativa de melhorar um pouco mais a sua performance."
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-do-algoritmo",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-do-algoritmo",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Preparação do algoritmo",
    "text": "Preparação do algoritmo\nVou criar uma instância da Random Forest para que façamos a seleção de hiperparâmetros. Para tal, vou passar o argumento tune() para os três hiperparâmetros disponíveis na implementação do algoritmo no tidymodels7. Além disso, precisamos finalizar o desenho do hiperparâmetro mtry antes de consolidar o algoritmo e a receita de pré-processamento em um workflow.\n\nCódigo# cria uma instancia do algoritmo com hiperparametros a serem tunados\nalgoritmo_tuning &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%\n  set_engine(engine = 'ranger', importance = 'impurity') %&gt;%\n  set_mode(mode = 'classification')\n\n# concluindo a preparacao dos ranges de hiperparametros que serao testados\nhiperparametros &lt;- extract_parameter_set_dials(algoritmo_tuning) %&gt;% \n  update(mtry = mtry(range = c(1L, 25L)))\n\n# define um workflow com a receita do pre-processamento e o algoritmo para ser tunado\nwf_tuning &lt;- workflow() %&gt;% \n  add_recipe(recipe = pre_processamento) %&gt;% \n  add_model(spec = algoritmo_tuning)"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#seleção-de-hiperparâmetros",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#seleção-de-hiperparâmetros",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Seleção de hiperparâmetros",
    "text": "Seleção de hiperparâmetros\nCom tudo pronto, agora é fazer a seleção de hiperparâmetros. Para isso, vou utilizar a técnica de anilhamento simulado (i.e., Simulated Annealing). Este é um algoritmo de otimização estocástica global, que utiliza um pouco de aleatoriedade como parte do processo de busca pela solução final (neste caso específico, àquela que oferecer o maior valor de AUC). Mais especificamente, este algoritmo funciona buscando na vizinhança de uma solução inicial outras alternativas que possam melhorar a sua performance, podendo, inclusive, aceitar soluções sub-ótimas com uma determina probabilidade. Esta probabilidade é alta nas primeiras iterações do algoritmo e vai caindo conforme a busca progride, de forma que esta taxa de decaimento é também um hiperparâmetro do algoritmo de busca que podemos modificar. De toda forma, este procedimento colabora para que o algoritmo localize a região do mínimo global de forma mais rápida, escapando de mínimos locais e passando a explorar outras áreas do espaço de busca que uma busca determinística não faria.\nVou gerar 10 iterações iniciais para alimentar o motor de busca do algoritmo e, então, realizar a busca dos hiperparâmetros que maximizem o AUC através de 50 iterações. Além disso, também vou permitir que o algoritmo teste novas combinações de hiperparâmetros caso nenhuma combinação testada durante 5 iterações consecutivas tragam resultados melhores do que aquele já existente.\n\nCódigo# setando a seed para a reprodutibilidade\nset.seed(512)\n\n# ajusta o workflow do baseline aos folds\nfit_tuning &lt;- wf_tuning %&gt;% \n  tune_sim_anneal(\n    resamples  = kfold,\n    metrics    = metricas,\n    param_info = hiperparametros,\n    iter       = 50,\n    initial    = 10,\n    control    = control_sim_anneal(verbose = TRUE, restart = 5)\n  )\n\n\nPodemos ver de que forma a otimização de hiperparâmetros progrediu ao longo das iterações na figura abaixo. O painel A demonstra que o hiperparâmetro que define o número de árvores tendeu a uma convergência bastante clara ao longo das iterações, enquanto isto não foi tão bem o caso para os hiperparâmetros que definem o número de variáveis preditoras disponíveis para a construção de cada árvore e o número mínimo de instâncias no ramo final das árvores. Já o painel B demonstra que o algoritmo tendeu a convergir rapidamente para um valor de AUC próximo à 0.75, com a performance de alguns folds chegando próximo ao patamar de AUC de 0.78; por outro lado, é possível ver também que a performance dos hiperparâmetros utilizados em algumas iterações foi bem ruim.\n\nCódigo# hiperparametros utilizados por iteracao\nparams &lt;- autoplot(object = fit_tuning, metric = 'roc_auc', type = 'parameters') +\n  labs(y = 'Valor do hiperparâmetro') +\n  theme(axis.title.x = element_blank())\n# evolucao da performance por iteracao\nperf &lt;- autoplot(object = fit_tuning, metric = 'roc_auc', type = 'performance') +\n  labs(x = 'Iteração', y = 'AUC')\n# compondo o plot\n(params / perf) +\n  plot_annotation(title = 'Evolução da seleção de hiperparâmetros por iteração do Simulated Annealing',\n                  tag_levels = 'A')\n\n\n\n\nOs cinco modelos cuja combinação de hiperparâmetros geraram os maiores valores de AUC são apresentados abaixo. Ainda que todos eles apresentem valores de AUC acima daquele obtido para o baseline com as configurações default da Random Forest, podemos ver que existe alguma incerteza em cima da estimativa pontual destes valores - representado pelo std_err associado à cada um deles. Neste contexto, será que os modelos tunados para melhor performance são de fato melhores que o baseline?\n\nCódigoshow_best(x = fit_tuning, metric = 'roc_auc', n = 5)\n\n# A tibble: 5 × 10\n   mtry trees min_n .metric .estimator  mean     n std_err .config .iter\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n1    24   670    38 roc_auc binary     0.754    10  0.0131 Iter41     41\n2    25   818    24 roc_auc binary     0.754    10  0.0123 Iter1       1\n3    24   501    38 roc_auc binary     0.754    10  0.0125 Iter47     47\n4    25   726    34 roc_auc binary     0.753    10  0.0136 Iter43     43\n5    25  1050    32 roc_auc binary     0.753    10  0.0127 Iter30     30\n\n\nPara responder à essa pergunta, utilizei um teste-t para testar a hipótese nula de que a diferença entre os valores de AUC do modelo baseline e do melhor modelo tunado não é diferente de 0. Como podemos ver abaixo, o p-valor deste teste supera o valor crítico de 0.05 e, portanto, não podemos rejeitar àquela hipótese nula. Em outras palavras, isto quer dizer que, na prática, tunar a Random Forest não trouxe benefícios substanciais em termos das métricas de performance avaliadas. Tendo este resultado em vista, vou seguir utilizando aqui o modelo baseline para gerar as probabilidades e tentar entender os padrões que o modelo capturou.\n\nCódigo# pegando o resultado do AUC do baseline\nbaseline &lt;- collect_metrics(x = fit_baseline) %&gt;% filter(.metric == 'roc_auc')\n# pegando as metricas do melhor modelo tunado\ntuned_results &lt;- show_best(x = fit_tuning, metric = 'roc_auc', n = 1)\n\n# pre-calculando o quadrado dos erros padroes\nse_baseline &lt;- baseline$std_err^2\nse_tuned &lt;- tuned_results$mean^2\n\n# extraindo a quantidade de observacoes (igual para os dois)\nn_obs &lt;- baseline$n\n\n# calculando a estatística t da diferenca entre as medias\nestatistica &lt;- (baseline$mean - tuned_results$mean) / sqrt((se_baseline + se_tuned))\n\n# numero de graus de liberdade ajustado\ndf_welch &lt;- ((se_baseline + se_tuned)^2) / ((se_baseline^2/(n_obs - 1)) + (se_tuned^2/(n_obs - 1)))\n\n# calculando o p-valor associado a estatistica calculada, utilizando um teste bicaudal, uma\n# vez que a hipótese nula trata de uma diferenca entre medias que pode ser maior ou menor que 0\np_valor &lt;- 2 * pt(q = estatistica, df = df_welch, lower.tail = TRUE)\n\n# printando o resultado\nsprintf(fmt = 'Estatistica t: %.3f | df: %.2f| p-valor: %5f', estatistica, df_welch, p_valor)\n\n[1] \"Estatistica t: -0.032 | df: 9.00| p-valor: 0.975478\""
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#ajuste-do-modelo-selecionado",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#ajuste-do-modelo-selecionado",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Ajuste do modelo selecionado",
    "text": "Ajuste do modelo selecionado\nFaremos o ajuste do modelo uma última vez, utilizando todos os dados da base de treino para ajustar o algoritmo e os dados da base de teste para gerar as probabilidades que serão usadas para avaliar a sua performance.\n\nCódigo## ajustando o modelo aos dados uma ultima vez\nmodelo_ajustado &lt;- last_fit(object = wf_baseline, split = train_test_split, metrics = metricas)\n\n## pegando as metricas do modelo\ncollect_metrics(x = modelo_ajustado)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 specificity binary         0.550 Preprocessor1_Model1\n2 roc_auc     binary         0.732 Preprocessor1_Model1\n3 mn_log_loss binary         0.598 Preprocessor1_Model1\n\n\nOs valores de AUC deste modelo final ficaram praticamente idênticos aquele obtido pelo modelo baseline. Todavia, temos um valor de log loss menor (i.e., probabilidades mais bem ajustadas) e uma sensibilidade maior (i.e., melhor capacidade do prever os acidentes que ocorreram), algo que acredito ser muito bom também. Mas como será que este modelo performou gerando previsões binárias, i.e. há ou não um acidente associado à cada prova? Como já havíamos antecipado pela métrica de sensibilidade, o modelo preveu corretamente que houve um acidente em uma prova um pouco mais de metade das vezes; por outro lado, ele errou em 34% das vezes em que preveu que haveria um acidente. Por fim, o modelo também não foi tão mal em prever quando não haveria um acidente, acertando em quase 79% dos casos. Para um modelo tão simples e com tão pouca informação, acredito que estes resultados estão bem OK para o esforço empregado.\n\nCódigo# pegando as previsoes do modelo ajustado\ncollect_predictions(x = modelo_ajustado) %&gt;% \n  # tabulando as previsoes e observacoes\n  count(tem_acidente, .pred_class) %&gt;% \n  # ajustando o texto das previsoes e target\n  mutate(across(where(is.factor), \\(x) ifelse(test = x == 1, yes = 'Sim', no = 'Não'))) %&gt;% \n  # recodificando o nivel dos fatores\n  mutate(\n    tem_acidente = fct_relevel(.f = tem_acidente, 'Sim'),\n    .pred_class = fct_relevel(.f = .pred_class, 'Não')\n  ) %&gt;% \n  # criando a figura\n  ggplot(mapping = aes(x = .pred_class, y = tem_acidente, fill = n, label = n)) +\n  geom_tile(color = 'black', show.legend = FALSE) +\n  geom_text(fontface = 'bold', size = 4) +\n  scale_fill_viridis_c(begin = 0.3) +\n  labs(\n    title    = 'Previsões feitas pelo modelo vs ocorrência dos acidentes',\n    subtitle = 'A matriz de confusão apresenta os acertos no modelo na diagonal principal e os\\nerros na diagonal oposta.',\n    x        = 'Previsão de acidente?',\n    y        = 'Ocorreu o acidente?'\n  )"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#explicabilidade",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#explicabilidade",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Explicabilidade",
    "text": "Explicabilidade\nUma vez que cheguei a um modelo preditivo aceitável, vou dar uma olhada agora nos padrões que ele foi capaz de capturar nos dados. A primeira coisa que acho legal olhar é o ranking de importância das variáveis que a Random Forest fornece.\n\nCódigo# pegando o modelo ajustado\nextract_fit_parsnip(x = modelo_ajustado) %&gt;% \n  # extraindo a importancia das variaveis de dentro do objeto do modelo\n  pluck('fit', 'variable.importance') %&gt;% \n  # colocando o vetor nomeado em uma dataframe\n  enframe(name = 'variavel', value = 'importance') %&gt;% \n  # pegando as 15 variaveis mais importantes\n  top_n(n = 15, wt = importance) %&gt;% \n  # reordenando as variaveis para plotar\n  mutate(\n    variavel = str_remove(string = variavel, pattern = 'circuit_'),\n    variavel = str_replace_all(string = variavel, pattern = '(_|\\\\.)', replacement = ' '),\n    variavel = str_to_title(string = variavel, locale = 'pt_BR'),\n    variavel = fct_reorder(.f = variavel, .x = importance, .desc = FALSE)\n  ) %&gt;% \n  # criando a figura\n  ggplot(aes(x = importance, y = variavel)) +\n  geom_col(aes(fill = importance), color = 'black') +\n  geom_text(aes(label = round(importance, 2)), nudge_x = 3) +\n  scale_fill_viridis_c(begin = 0.1) +\n  scale_x_continuous(breaks = seq(from = 0, to = 50, by = 10)) +\n  labs(title    = 'Importância das variáveis para o modelo',\n       subtitle = 'As 15 variáveis mais importantes de acordo com o critério de impureza de Gini',\n       x        = 'Impureza de Gini') +\n  theme(\n    legend.position = 'none',\n    axis.title.y = element_blank()\n  )\n\n\n\n\nPodemos ver que o ano em que a prova foi realizada foi, de longe, a variável mais importante para gerar as previsões de ocorrência dos acidentes. Na sequência, outras 5 variáveis pareceram importantes: o coeficiente de variação na pontuação dos pilotos até a prova anterior (cvl_all_pilots), a extensão do circuito, o coeficiente de variação na pontuação dos 5 pilotos com mais pontos até a prova anterior (cvl_top_pilots), a quantidade de voltas e a quantidade de pilotos participando na prova. Todas essas variáveis estão bastante relacionadas com algumas das hipóteses que defini. Todavia, ainda não sei se a contribuição destas variáveis é similar àquela que eu havia hipotetizado ou não, uma vez que este gráfico só mostra quais variáveis são importantes.\nPara entender de que forma estas variáveis estão relacionadas à probabilidade de ocorrência de um acidente em uma prova, vou utilizar um plot de dependência parcial. Para criar este plot é necessário extrair as previsões da probabilidade de ocorrência de um acidente para cada instância na base de treino, fixando-se os valores de todas as variáveis preditoras a não ser àquela que estamos interessados. Com isso, geramos algo parecido com um profile sobre a forma pela qual a previsão do modelo para àquela instância varia apenas em função da variável analisada. Uma vez que tenhamos esse profile calculado para cada instância, sumarizamos o padrão obtido entre todas as instâncias para cada valor xi da variável analisada, de forma a obter a curva ‘média’ que descreve a relação daquela variável preditora com a variável resposta.\nPara facilitar a criação dos plots de dependência parcial, vou carregar os pacotes DALEX e DALEXtra, criar uma instância de um explicador do tidymodels e passar três argumentos para ela: o workflow ajustado, os dados de treino do modelo e o target (convertido para um número inteiro).\n\nCódigo# carregando o pacote\nlibrary(DALEX)\nlibrary(DALEXtra)\n\n# criando instancia do DALEX para explicar o modelo\nexplainer &lt;- explain_tidymodels(\n  extract_workflow(x = modelo_ajustado), \n  data = training(train_test_split) %&gt;% select(-tem_acidente),\n  y = as.integer(training(train_test_split)$tem_acidente)\n)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  workflow  (  default  )\n  -&gt; data              :  785  rows  35  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  785  values \n  -&gt; predict function  :  yhat.workflow  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package tidymodels , ver. 1.1.1 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0336633 , mean =  0.4202269 , max =  0.9253802  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  0.3696457 , mean =  1.000155 , max =  1.687087  \n  A new explainer has been created!  \n\n\nAgora que temos esta instância, podemos gerar o plot de dependência parcial. Vou focar aqui em apenas três daquelas seis variáveis que vimos serem mais importantes para o modelo de Random Forest: o ano, o coeficiente de variação na pontuação dos pilotos até a prova anterior (cvl_all_pilots) e o coeficiente de variação na pontuação dos 5 pilotos com mais pontos até a prova anterior (cvl_top_pilots). De que forma estas variáveis contribuem para definir a probabilidade de ocorrência de acidentes de acordo com o modelo preditivo gerado?\n\nCódigo# pegando os dados do partial dependence plot\nmodel_profile(\n  explainer = explainer, \n  variables = c('year', 'cvl_all_pilots', 'cvl_top_pilots'),\n  type = 'partial'\n) %&gt;%\n  # extraindo os dados para criar a figura no ggplot\n  pluck('agr_profiles') %&gt;% \n  # ajustando a ordem dos graficos\n  mutate(\n    `_vname_` = case_when(`_vname_` == 'cvl_top_pilots' ~ 'CV pontuação dos cinco pilotos no topo do ranking',\n                          `_vname_` == 'cvl_all_pilots' ~ 'CV pontuação de todos os pilotos',\n                          TRUE ~ 'Ano')\n  ) %&gt;%\n  # criando a figura\n  ggplot(mapping = aes(x = `_x_`, y = `_yhat_`)) +\n  facet_wrap(~ `_vname_`, scales = 'free', strip.position = 'bottom') +\n  geom_line() +\n  labs(\n    title    = 'Efeitos parciais de três variáveis sobre a probabilidade de ocorrência de acidentes',\n    subtitle = 'O efeito parcial de uma variável sobre a resposta é aquele que presenta o impact de uma variável sobre a resposta fixando-se o impacto de todas as demais variáveis',\n    y        = 'Probabilidade de ocorrência de um acidente'\n  ) +\n  theme(axis.title.x = element_blank())\n\n\n\n\nMuitos dos padrões capturados pelo modelo para estas três variáveis são similares aqueles observados na análise exploratória. O modelo foi capaz de identificar o padrão temporal da frequência de ocorrência de acidentes, mostrando que houve uma queda de cerca de 20% na probabilidade de ocorrência destes eventos a partir da década de 90. Não dá para saber o quanto isto pode estar relacionado às intervenções de segurança implementadas no período, mas é importante ver que algo que aconteceu a partir daí contribuiu bastante para a queda na frequência de acidentes. Outro padrão interessante é aquele relacionado à intensidade e assimetria da disputa pelo campeonato, que também havíamos visto na análise exploratória. De fato, parece que quando a disputa pelo campeonato está muito acirrada e existem alguns pilotos que estão ali só de coadjuvante, é quando existe uma maior probabilidade de ocorrência de acidentes. De toda forma, a contribuição média destes fatores parece não ser tão expressiva assim, uma vez que seu impacto sobre as probabilidades é de cerca de 5% a 6%."
  },
  {
    "objectID": "posts/2021-11-30_scrapper-gwent/index.html",
    "href": "posts/2021-11-30_scrapper-gwent/index.html",
    "title": "Raspando a biblioteca de decks de Gwent",
    "section": "",
    "text": "The Witcher é uma franquia lançada como uma série de livros de fantasia que contam as aventuras do bruxo (i.e., Witcher) Geralt de Rivia. Essas estórias foram popularizadas através da série da Netflix de mesmo nome e, também, através do jogo The Witcher 3: The Wild Hunt1. Esse jogo é bastante complexo e tem uma experiência bem imersiva, trazendo inclusive diversas tradições do universo à ela. Uma delas é o Gwent, um jogo de cartas entre dois jogadores, onde ganha aquele que mais pontuar em pelo menos 2 de 3 turnos. Parece ser um mini-jogo bobo dentro do título, mas ele próprio invoca muito da fantasia da série na disputa.\nGwent é um jogo que lembra muito o Magic, onde você deve construir um deck de no mínimo 25 cartas pertencentes à uma facção de sua escolha, respeitando algumas restrições (e.g., custo total das cartas no deck, quantidade de unidades e etc). No momento em que escrevo este post, existem cerca de 200 cartas pertencentes à cada uma de 6 facções distintas, além de outras 200 à 400 cartas neutras (i.e., que não pertencem à nenhuma facção) que podem ser usadas para montar um deck Existe uma diferença inerente ao modo de jogar com cada facção (e.g., foco em dano direto, foco bloqueio e roubo de cartas,…) e, dentro de uma dada facção, também existe uma pequena diversidade de formas de favorecer uma estratégia de jogo (e.g., cartas que juntas reforçam muito umas as outras, cartas que ajudam a ativar a habilidade de outras cartas mais frequentemente). Neste sentido, montar um deck forte e consistente passa a ser quase uma arte, mas que poderia ser aprimorado com um pouquinho de acesso aos metadados das cartas.\nExiste bastante conteúdo na internet que é produzido pela própria comunidade que joga o Gwent. Em particular, a própria comunidade contribui compartilhando a composição de cartas nos seus decks, as estratégias de jogo e votando nestas a partir do próprio site oficial do jogo. E é aqui que entra o meu interesse: se pudermos obter estas informações e estruturá-las, ficaria muito mais fácil entender os padrões dentro e entre os decks e usar isso em favor de aprimorar a jogabilidade. Além disso, acredito que estes dados podem dar um bom modelo de estudo para responder à algumas perguntas e praticar algumas outras técnicas. Falarei sobre essas idéias ao final do post, mas por agora vou mostrar como obter essas informações."
  },
  {
    "objectID": "posts/2021-11-30_scrapper-gwent/index.html#footnotes",
    "href": "posts/2021-11-30_scrapper-gwent/index.html#footnotes",
    "title": "Raspando a biblioteca de decks de Gwent",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nUm dos jogos mais tops que já joguei.↩︎\nEsse passo não é obrigatório, mas decidi colocá-lo aqui só para não bombardear o servidor com um monte de requests de uma vez quando formos escalar o seu uso.↩︎\ne.g., https://www.playgwent.com/pt-BR/decks/2, https://www.playgwent.com/pt-BR/decks/3,…↩︎\nIsso pode estar relacionado ao comportamento aparentemente linear que pode ser visto no painel B.↩︎"
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html",
    "href": "posts/2021-12-11_notas-boardgames/index.html",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "",
    "text": "Se você quiser saber quão legal pode ser um jogo de tabuleiro específico, uma opção seria recorrer aos sites especializados no assunto para ter uma ideia. Dois destes portais são a Ludopedia e o BoardGameGeek: o primeiro é um site brasileiro e o segundo é internacional. Apesar de compartilharem muitas similaridades em torno do conteúdo, uma diferença fundamental entre eles está na quantidade de títulos disponíveis, onde o BoardGameGeek (BGG, daqui em diante) tem quase 50 vezes mais títulos registrados do que a Ludopedia. Outro ponto que à princípio pode nos enganar está no próprio ranking dos jogos que estes sites possuem: se você reparar bem, apesar de muitos títulos se repetirem entre eles, as notas não são exatamente próximas. Desta forma, quão similar será que os rankings e as notas dadas aos títulos são entre eles?\nExistem muitas formas de pensar nessa pergunta, mas uma que aguçou a minha curiosidade foi a possibilidade do público de um dos portais tender a ser mais ‘bonzinho’ do que o do outro no momento de avaliar os jogos. Isto se manifestaria como um viés quando comparássemos a média das notas dos rankings entre os dois portais - isto é, a diferença na média das notas no portal x é sempre maior do que àquela no portal y. Neste contexto, seria interessante entender não só se existe viés existe mas, caso positivo, qual a direção e o tamanho do mesmo. Não que eu espere que isso vá interferir de alguma forma em uma decisão de compra ou de jogar um título específico…mas às vezes é legal saber se as informações que estamos recebendo possuem algum tipo de viés quando comparado com outras fontes disponíveis.\nOs dados que nos permitem ganhar àquela compreensão estão prontamente disponíveis nos respectivos portais e, ao longo dos últimos posts, já construímos uma compreensão de como obtê-los. Você pode buscar o código para raspar a página do ranking da Ludopedia aqui e o do ranking do BGG aqui e, se quiser, também ganhar uma compreensão de como eles funcionam aqui e aqui. Como eu já havia adiantado naqueles posts anteriores, meu intuito é utilizar as informações extraídas a partir dos scrappers não só pela prática, mas também para responder à alguma pergunta, entender algum padrão interessante ou testar algum pacote.\nFalando nisso, um objetivo secundário deste post é, de fato, testar o pacote infer1. De acordo com o próprio site, o infer tem por objetivo implementar a inferência estatística utilizando uma gramática estatística expressiva que seja coerente com o framework existente no tidyverse. Esse pacote traz algumas funções para atingir este objetivo e, através de sua documentação, podemos ver que a análise acaba ficando bastante verbosa - mas bem aderente às idéias e conceitos da estatística inferencial. O R já possui muitas funcionalidades nativas orientadas à mesma finalidade, mas acho interessante testar essa outra opção que parece conversar tão bem com o universo tidy.\nVamos começar preparando os dados e entendendo de que forma vamos avaliar a similaridades nas notas dos rankings. A partir daí, avançaremos para ganhar um entendimento geral do padrão estudado e concluiremos mostrando três formas através das quais podemos analisar este mesmo dado - sendo àquela do pacote infer uma delas."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#stats",
    "href": "posts/2021-12-11_notas-boardgames/index.html#stats",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "stats",
    "text": "stats\nA pergunta que estamos abordando é se existe alguma diferença entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG. A hipótese estatística aqui seria a de que a diferença média entre as duas notas seria igual a zero; posto de outra forma, a diferença entre cada par de notas associada à cada uma das posições do ranking de i à 2810 tenderia à zero. Como estamos considerando a diferença de notas entre um par de observações, o teste estatístico que precisamos utilizar é um teste-t pareado. Este teste é implementado no R através da função stats::t.test.\nQuando formos especificar o teste abaixo, é importante estar atento à duas coisas. Uma delas é que a hipótese que estamos testando diz que a diferença média deve ser diferente 0, ou seja, um número maior ou menor que 0. Neste contexto, precisamos especificar que a nossa hipótese alternativa é bicaudal - argumento alternative = 'two.sided'. O segundo ponto é que a variância nas notas entre os dois portais é diferente, conforme pode ser visto em uma das figura acima onde apresentamos a distribuição dos valores de nota para cada portal3. Poderíamos fazer um teste estatístico para confirmar o fato de que as variâncias não são homogêneas (e.g., o teste de Barttlet), mas podemos dispensar essa formalidade aqui pois o padrão já é muito claro naquela figura que mencionei - ainda assim, esse seria um passo importante a se tomar em uma análise mais formal. Para considerar que as variâncias não são homogêneas, basta utilizar var.equal = FALSE.\nVamos implementar o teste-t pareado abaixo.\n\nCódigo# teste-t pareado com variancias diferentes conforme disponivel no base R\nt_test_stats &lt;- t.test(x = ranks$ludopedia, y = ranks$bgg, \n                       alternative = 'two.sided', paired = TRUE, var.equal = FALSE)\nt_test_stats\n\n\n    Paired t-test\n\ndata:  ranks$ludopedia and ranks$bgg\nt = 24.418, df = 2809, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.3413125 0.4009153\nsample estimates:\nmean difference \n      0.3711139 \n\n\nO resultado desta análise sugere que devemos rejeitar a hipótese nula, sugerindo que a diferença média entre as notas da Ludopedia e do BGG para uma mesma posição do ranking não tende à zero. Na realidade, podemos ver que a estimativa da diferença média estimada entre as notas é de 0.371 pontos, com um intervalo de confiança de 95% de 0.341 à 0.401 pontos. Isto seria o tanto, em média, que as notas do ranking da Ludopedia superam àquelas do BGG. Vamos agora examinar o que o pacote infer nos oferece."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#infer",
    "href": "posts/2021-12-11_notas-boardgames/index.html#infer",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "infer",
    "text": "infer\nO infer não possui uma implementação explícita do teste-t pareado, mas isso não é necessariamente um problema. Como a própria documentação do pacote mostra, o que o teste-t pareado faz por baixo dos panos é calcular a diferença entre os valores de x e y, e calcular a estatística em cima desta diferença. Ou seja, este teste seria um caso especial de um teste-t univariado. Assim, para utilizarmos o teste-t pareado no infer, precisaremos adicionar uma coluna no tibble original com a diferença par a par entre as notas já calculada.\n\nCódigolibrary(infer) # para fazer o teste de hipótese\n\n# colocando a diferenca no tibble\nranks &lt;- ranks %&gt;% \n  # calculando a coluna de diferenca\n  mutate(\n    diferenca = ludopedia - bgg\n  )\n\n\nA partir daqui, usaremos os verbos do infer para especificar (specify) a variável resposta e calcular (calculate) a estatística de teste que vamos usar. Como estamos utilizando um teste-t, o ideal seria focarmos no valor de t como a nossa estatísticade de teste. No entanto, nosso maior interesse está na estimativa da diferença média entre as notas e o seu intervalo de confiança. Por conta disso, vamos especificar que a nossa estatística de teste será o valor da média. O problema ao fazermos isso é que qualquer cálculo relacionado à nossa estatística de teste deverá ser feito criando uma distribuição de probabilidade a partir da própria amostra estudada. Para isso, o pacote infer faz uso de técnicas de permutação e reamostragem. Antes de passar para esse ponto, no entanto, vamos medir o valor observado da nossa estatística de teste - a diferença média entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG.\n\nCódigo# rodando um teste-t pareado original\nobs_statistic &lt;- ranks %&gt;% \n  # especificando a variável analisada para o infer\n  specify(\n    response = diferenca\n  ) %&gt;% \n  # calculando estatistica de teste\n  calculate(stat = 'mean') %&gt;% \n  # extraindo a coluna com a estatistica\n  pull(stat)\nobs_statistic\n\n[1] 0.3711139\n\n\nAs técnicas de reamostragem são uma ferramenta muito útil em diversas situações. Alguns exemplos de casos de aplicação são quando:\n\nNão sabemos a que família de distribuição estatística a variável resposta e/ou seus resíduos pertence;\n\nPrecisamos fazer uma inferência ou estimar intervalos de confiança;\n\nTemos uma hipótese específica sobre o processo gerador dos dados e queremos testá-lo explicitamente;\n\nPrecisamos realizar comparações entre duas ou mais amostras e temos um número diferente de observações entre elas;\n\nPrecisamos realizar comparações entre duas ou mais amostras e temos um número muito grande de observações;\n\n…\n\nComo a nossa estatística de teste é a diferença média entre as notas, utilizaremos a reamostragem para criar uma estimativa de seu intervalo de confiança. Isto vai nos permitir dizer se existe ou não uma diferença nas notas entre os portais no caso onde a estimativa deste intervalo não contenha o valor 0. Para implementar a reamostragem no infer, utilizaremos o verbo generate para gerar 1.000 amostras aleatórias a partir dos dados originais utilizando a técnica de bootstrap. Através deste técnica, o algoritmo gera uma amostra com substituição a partir de um conjunto de dados e com o mesmo número de observações que ele - i.e., a mesma observação pode aparecer mais de uma vez em uma mesma amostra, e cada amostra tem o mesmo número de observações que os dados que o geraram. Uma vez que tenhamos as 1.000 amostras do bootstrap, vamos calcular (calculate) o valor da média da diferença entre as notas para cada uma delas, e vamos determinar o intervalo que contém 95% das estimativas. Este será o intervalo de confiança na estatística de teste, e que será obtido através do verbo get_confidence_interval.\n\nCódigo## criando reamostragem para estimar o intervalo de confiança\nboot_ci_tbl &lt;- ranks %&gt;% \n  # especificando a variável analisada para o infer\n  specify(\n    response = diferenca\n  ) %&gt;% \n  # gerando bootstrap\n  generate(reps = 1000, type = 'bootstrap') %&gt;% \n  # calculando estatistica do bootstrap\n  calculate(stat = 'mean')\n\n## criando o histograma de distribuição de frequência da médias\nggplot(data = boot_ci_tbl, mapping = aes(x = stat)) +\n  geom_histogram(color = 'black', fill = 'grey80') +\n  scale_x_continuous(n.breaks = 8) +\n  labs(\n    title = 'Distribuição de frequência da diferença média entre notas nos dados reamostrados',\n    x     = 'Diferença média entre as notas (Ludopedia - BGG)',\n    y     = 'Frequência'\n  )\n\n\n\nCódigo## calculando o intervalo de confiança\nboot_ci &lt;- boot_ci_tbl %&gt;% \n  # pegando o intervalo de confianca\n  get_confidence_interval(type = 'percentile')\nboot_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.341    0.398\n\n\nComo podemos ver, a estatística de teste e o intervalo de confiança extraído do bootstrap são muito parecidos com aqueles que tínhamos observado com a função stats::t.test. Muito bom!\nMas e se eu tivesse outra hipótese nula?\nUma coisa que achei legal no infer é que ele também deixa você criar uma distribuição de valores que sigam uma hipótese nula específica. Isso pode ser útil, por exemplo, caso tivéssemos declarado que a hipótese nula que queremos testar é que a diferença entre as notas é um outro valor que não 0. No caso abaixo, podemos utilizar o verbo hypothesize para declarar qual é a hipótese nula que queremos testar e o valor esperado para a estatística de teste de acordo com a nossa hipótese nula. Neste exemplo, testarei se a diferença média entre as notas é diferente de 0,2 pontos, usando o verbo generate para gerar 500 amostras aleatórias usando a técnica de boostrap a partir de algum conjunto de dados - não ficou claro para mim de que distribuição o pacote está tirando essa estimativa…assumo que seja de uma distribuição normal, com a média que definimos na chamada da função e desvio padrão igual a 1…mas o pacote falhou em documentar isso direito.\n\nCódigo## calculando a distribuicao da estatistica de teste com o bootstrap\nnull_statistic &lt;- ranks %&gt;% \n  # especificando a variável analisada para o infer\n  specify(\n    response = diferenca\n  ) %&gt;% \n  # especificando a hipotese nula que queremos testar\n  # não existe diferença entre as notas medias para uma mesma posicao entre rankins\n  hypothesize(null = 'point', mu = 0.2) %&gt;%\n  # gerando bootstrap\n  generate(reps = 500, type = 'bootstrap') %&gt;% \n  # calculando estatistica do bootstrap\n  calculate(stat = 'mean')\nnull_statistic\n\nResponse: diferenca (numeric)\nNull Hypothesis: point\n# A tibble: 500 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.174\n 2         2 0.204\n 3         3 0.216\n 4         4 0.201\n 5         5 0.189\n 6         6 0.202\n 7         7 0.197\n 8         8 0.203\n 9         9 0.175\n10        10 0.222\n# ℹ 490 more rows\n\n\nUma vez que tenhamos montado a distribuição de valores que descreve a hipótese nula, podemos usar um outro conjunto de verbos para visualizar os resultados. O verbo visualize vai plotar a distribuição dos valores relacionados à hipótese nula, enquanto que os verbos shade_confidence_interval e shade_p_value servem para plotar na mesma figura o intervalo de confiança e a estatística de teste original - caso a tenhamos calculado anteriormente.\n\nCódigonull_statistic %&gt;% \n  # criando a visualização\n  visualize() +\n  # sombreando o intervalo de confianca\n  shade_confidence_interval(endpoints = boot_ci, color = 'white', fill = 'tomato') +\n  # colocar uma linha para a estimativa\n  shade_p_value(obs_stat = obs_statistic, direction = 'two-sided', color = 'tomato', size = 1) +\n  # ediitando a figura\n  scale_x_continuous(breaks = seq(from = -0.05, to = 0.4, by = 0.05)) +\n  labs(\n    title    = 'Distribuição dos valores nulos e observados',\n    subtitle = 'O histograma representa a distribuição dos valores da diferença entre médias de acordo com a hipótese nula. A linha\nvertical vermelha indica a estimativa da diferença média entre as notas, enquanto a barra vertical o intervalo de\nconfiança 95%. Como não há sobreposição entre a distribuição dos dados de acordo com a hipótese nula e o intervalo de\nconfiança, devemos rejeitar a hipótese nula.',\n    caption  = 'Utilizamos o método de reamostragem por bootstrap para a estimativa do intervalo de confiança, onde o calculamos através\ndo percentil de distribuição das médias reamostradas.',\n    x        = 'Diferença entre médias (Ludopedia - BGG)',\n    y        = 'Frequência'\n  )\n\n\n\n\nTá aí uma forma fácil de implementar um bootstrap e testar diferenças entre médias. Vamos agora à mais uma forma que eu pensei para tentar abordar a pergunta proposta."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#subamostragem",
    "href": "posts/2021-12-11_notas-boardgames/index.html#subamostragem",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "Subamostragem",
    "text": "Subamostragem\nUm ponto importante e comum à todos esses métodos estatísticos é que tanto maior for o tamanho da amostra, mais estreitos serão as estimativas de intervalo de confiança e maior a probabilidade de rejeitarmos a hipótese nula quando não deveríamos ter feito isso (i.e., um erro do Tipo II ou um falso positivo). Nossa base de dados não é tão grande assim (2.810 observações), mas acredito já ser grande o suficiente para que estes tipos de problema comecem a se manifestar - especialmente para um teste estatístico como o teste-t, que foi concebido sob a ideia de amostras contendo até umas 30 observações. É claro que isto não invalida o que fizemos até aqui, mas é bastante provável que isto esteja tornando as estimativas feitas muito otimistas.\nPara resolver este problema, podemos buscar uma inspiração nas técnicas de reamostragem descritas anteriormente, e implementar uma subamostragem da base de dados seguida de diversas reamostragens. Isto é, vamos amostrar aleatoriamente uma fração de tamanho muito pequeno n dos dados sem substituição, e repetir este procedimento m vezes. Daí então vamos calcular a estatística de teste para cada uma das m amostras e combiná-las de alguma forma - pode ser através de uma média, de um percentil ou até mesmo utilizando uma técnica de meta-análise.\nVamos começar construindo a solução através da função que vai criar uma subamostra.\n\nCódigo# criando funcao para fazer a reamostragem\ncreate_sample &lt;- function(dataset, tamanho) {\n  dataset %&gt;% \n    # retira uma amostra aleatoria de um determinado tamanho\n    slice_sample(n = tamanho)\n}\n\n\nPodemos então utilizar a função purrr::rerun para repetir a execução da função create_sample 500 vezes, sendo que amostraremos apenas 30 das 2.810 observações disponíveis em cada uma delas. Esta função retornará uma lista de listas e, portanto, utilizaremos a função bind_rows para juntá-las em um tibble e aninhar todas as colunas utilizando um nest, deixando de fora apenas a coluna com a identidade da amostra.\n\nCódigo## criando dataframe reamostrado\nreamostragem &lt;- rerun(.n = 500,\n                      # rodando a funcao para criar amostrar 500 vezes, cada uma \n                      # das quais amostrando 50 observacoes aleatoriamente\n                      create_sample(dataset = ranks, tamanho = 30)) %&gt;% \n  # juntando cada um dos dataframes e adicioando um sample id\n  bind_rows(.id = 'sample_id') %&gt;% \n  # aninhando o dataframe com os dados que vamos usar para ajustar o test-t dentro \n  # de cada sample_id\n  nest(data = -sample_id)\nreamostragem\n\n# A tibble: 500 × 2\n   sample_id data             \n   &lt;chr&gt;     &lt;list&gt;           \n 1 1         &lt;tibble [30 × 4]&gt;\n 2 2         &lt;tibble [30 × 4]&gt;\n 3 3         &lt;tibble [30 × 4]&gt;\n 4 4         &lt;tibble [30 × 4]&gt;\n 5 5         &lt;tibble [30 × 4]&gt;\n 6 6         &lt;tibble [30 × 4]&gt;\n 7 7         &lt;tibble [30 × 4]&gt;\n 8 8         &lt;tibble [30 × 4]&gt;\n 9 9         &lt;tibble [30 × 4]&gt;\n10 10        &lt;tibble [30 × 4]&gt;\n# ℹ 490 more rows\n\n\nCom estas amostras, podemos agora ajustar um teste-t pareado similar aquele implementado usando a função stats::t.test à cada amostra, e extrair os resultados de cada uma delas. Para isso, precisaremos também fazer uso das funções purrr::map, broom::tidy e tidyr::unnest. Os resultados abaixo representam os resultados de cada um dos testes-t pareados aplicados à cada uma das m subamostras criadas.\n\nCódigoresultados_reamostragem &lt;- reamostragem %&gt;% \n  mutate(\n    # aplicando um teste-t pareado às amostras de cada sample_id\n    test_t = map(.x = data, \n                 .f = ~t.test(x = .x$ludopedia, y = .x$bgg, \n                              alternative = 'two.sided', paired = TRUE, var.equal = FALSE)\n    ),\n    # extraindo as informacoes tidy do ajuste do test-t pareado\n    tidy_t = map(.x = test_t, .f = broom::tidy)\n  ) %&gt;% \n  # desaninhando os resultados do test-t\n  unnest(tidy_t) %&gt;% \n  # jogando fora alguma colunas que nao precisaremos mais\n  select(-data, -test_t)\nresultados_reamostragem\n\n# A tibble: 500 × 9\n   sample_id estimate statistic   p.value parameter conf.low conf.high method   \n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n 1 1            0.236      1.24 0.223            29  -0.152      0.623 Paired t…\n 2 2            0.205      1.06 0.297            29  -0.190      0.601 Paired t…\n 3 3            0.175      1.34 0.189            29  -0.0910     0.440 Paired t…\n 4 4            0.393      2.58 0.0152           29   0.0814     0.704 Paired t…\n 5 5            0.578      5.27 0.0000121        29   0.354      0.802 Paired t…\n 6 6            0.394      2.61 0.0142           29   0.0853     0.703 Paired t…\n 7 7            0.455      3.88 0.000547         29   0.215      0.695 Paired t…\n 8 8            0.243      1.58 0.124            29  -0.0711     0.558 Paired t…\n 9 9            0.339      1.97 0.0590           29  -0.0138     0.692 Paired t…\n10 10           0.566      5.11 0.0000187        29   0.339      0.792 Paired t…\n# ℹ 490 more rows\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nFinalmente, vamos extrair a média da estimativa da diferença entre as notas (i.e., a média da coluna estimate) para ser a nossa estatística de teste, e calcular os quantis de 2.5% e 97.5% da distribuição destes valores para serem os limites inferior e superior, respectivamente, do intervalo de confiança de 95%. Com isso, podemos visualizar os resultados obtidos através da reamostragem abaixo.\n\nCódigo# pegando os quantis da distribuicao das estimativas das subamostras\nquantis_subamostra &lt;- quantile(x = resultados_reamostragem$estimate, probs = c(0.025, 0.975))\nmedia_subamostra &lt;- mean(x = resultados_reamostragem$estimate)\n\n## pegando os dados do que o density plot vai usar e retendo apenas as areas\n## que estao fora do quantil\ndf_dos_quantis &lt;- tibble(\n  # pegando os valores de x e y que serão usados para o density plot\n  x = density(x = resultados_reamostragem$estimate)$x,\n  y = density(x = resultados_reamostragem$estimate)$y\n) %&gt;% \n  mutate(\n    # sinalizando os valores de x que estão fora do quantil de 95%\n    lower_quantile = x &lt; quantis_subamostra[1],\n    upper_quantile = x &gt; quantis_subamostra[2]\n  ) %&gt;% \n  # filtrando apenas as observacoes que estao fora do quantil\n  filter(lower_quantile | upper_quantile)\n\n# criando a figura\nresultados_reamostragem %&gt;% \n  ggplot(mapping = aes(x = estimate)) +\n  geom_density(fill = 'grey80', color = 'black') +\n  geom_ribbon(data = filter(df_dos_quantis, lower_quantile),\n              mapping = aes(x = x, ymin = 0, ymax = y), fill = 'tomato') +\n  geom_ribbon(data = filter(df_dos_quantis, upper_quantile),\n              mapping = aes(x = x, ymin = 0, ymax = y), fill = 'tomato') +\n  geom_vline(xintercept = 0, color = 'black', alpha = 0.6) +\n  geom_vline(xintercept = media_subamostra, color = 'black', linetype = 2) +\n  labs(\n    title    = 'As notas do ranking da Ludopedia são maiores que as do BoardGameGeek',\n    subtitle = str_glue('A estimativa da diferença através da subamostragem é de que as notas da Ludopedia superam as do BoardGameGeek\\nem {round(media_subamostra, digits = 2)} pontos (Intervalo de Confiança de 95%: {round(quantis_subamostra[1], digits = 2)} à {round(quantis_subamostra[2], digits = 2)} pontos)'),\n    caption  = 'A área cinza representa o intervalo de confiança de 95% da estimativa da diferença nas notas entre os\\ndois portais, obtidas através de 500 estimativas independentes tomadas a partir de 30 amostras aleatórias dos dados.',\n    x        = 'Diferença entre as notas do ranking da Ludopedia e do BoardGameGeek',\n    y        = 'Densidade'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nJá é possível reparar que a diferença média estimada entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG é bastante similar àquela dos dois testes anteriores - as notas na Ludopedia são 0.37 pontos maiores, em média, do que no BGG. No entanto, o intervalo de confiança ficou bem maior: 0.06 à 0.65 pontos de diferença em favor da Ludopedia. Como podemos ver, o grande número de amostras nos dados tinha algum impacto sobre estas últimas estimativas, e ela me parece muito mais realista agora do que àquelas obtidas anteriormente. Falo isso principalmente pensando na figura que mostra a variação na diferença entre as notas de acordo com a posição do ranking: podemos ver que a diferença se mantém num patamar elevado até certo ponto e de repente despenca. Assim, a diferença que estimamos estava longe de ser tão pequena quanto àquela calculada anteriormente.\nMas, calma aí…tinha esse padrão que eu acabei de falar…será que essa estimativa que tomamos agora faz mesmo sentido, então?"
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#a-milha-extra",
    "href": "posts/2021-12-11_notas-boardgames/index.html#a-milha-extra",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "A milha extra",
    "text": "A milha extra\nFiquei um bocado incomodado com os resultados que encontrei e os padrões observados naquela figura mostrando a variação na diferença entre as notas de acordo com a posição dos rankings. Fica muito claro ali que as notas dos títulos tendem a ser maiores na Ludopedia até certo ponto, a partir do qual o padrão contrário passa a valer. Assim, me parece mais razoável assumir que existam duas populações distintas dentro da nossa amostra, e que seria importante considerar isso na análise.\nPara tentar achar o ponto onde parece existir uma mudança de tendência nas notas de acordo com a posição do ranking, utilizei o algoritmo de regressão MARS (Multivariate adaptative regression splines). Eu acho esse algoritmo bastante legal: ele segmenta a feature em vários bins e vai vendo a partir de qual bin a relação entre a feature e o target muda; quando isto ocorre, ele cria uma função hinge, dando um coeficiente beta para valores da feature menores ou iguais ao limite daquele bin e um outro coeficiente beta para valores maiores que aquele limite. Tomei vantagem deste aspecto do algoritmo e ajustei o mesmo aos dados especificando, propositalmente, que gostaria de apenas dois termos no modelo - um coeficiente para a nota até a posição do ranking em que ele se mantém em um nível, e outro para o coeficiente a partir do qual o comportamento da relação muda. Esse modelo está super underfitado, mas isso foi mesmo proposital: eu só quero o valor que aparece nos nomes dos coeficientes, e nada mais.\n\nCódigo# carregando o tidymodels \nlibrary(tidymodels)\nlibrary(earth)\n\n# ajustando um MARS aos dados, forcando apenas 2 termos\nmodelo &lt;- mars(mode = 'regression', num_terms = 2) %&gt;% \n  set_engine(engine = 'earth') %&gt;% \n  fit(diferenca ~ ranking, data = ranks)\n\n# sumario do modelo\nsummary(modelo$fit)\n\nCall: earth(formula=diferenca~ranking, data=data, keepxy=TRUE, nprune=~2)\n\n                coefficients\n(Intercept)      0.591006338\nh(ranking-2254) -0.003990402\n\nSelected 2 of 7 terms, and 1 of 1 predictors (nprune=2)\nTermination condition: RSq changed by less than 0.001 at 7 terms\nImportance: ranking\nNumber of terms at each degree of interaction: 1 1 (additive model)\nGCV 0.3724764    RSS 1044.425    GRSq 0.4263666    RSq 0.4271832\n\n\nO output acima traz dois coeficientes, o (Intercept) e o h(ranking-2254), que representam o valor dos betas quando a posição do ranking é menor ou igual a 2.254 e maior que esta posição, respectivamente. Ou seja, se pegarmos os dígitos associados à string do segundo coeficiente, podemos definir exatamente onde que o algoritmo encontrou o ponto a partir do qual a relação entre a diferença nas notas e a posição do ranking mudou.\n\nCódigoponto_de_corte &lt;- modelo$fit$coefficients %&gt;% \n  # pegando o nome das linhas\n  rownames %&gt;% \n  # pegando o segundo elemento - nome do coeficiente da funcao hinge\n  pluck(2) %&gt;% \n  # extraindo todos os numeros do string\n  str_extract(pattern = '[0-9]+') %&gt;% \n  # parseando a string para numeric\n  parse_number()\nponto_de_corte\n\n[1] 2254\n\n\nAgora que temos o ponto de corte para definir as duas populações com que vamos trabalhar, vou repetir o procedimento feito no item anterior em que subamostramos os dados. No entanto, vamos fazer apenas uma pequena modificação na função que cria as subamostras para que ela aceite um argumento que vai nos ajudar a fazer a subamostragem para cada uma das duas populações separadamente. Para fazer isso, vamos passar o teste lógico para saber se a posição do ranking é menor ou igual ao ponto de corte para group_by, que já dará conta de estratificar a subamostragem. Com isso, podemos então proceder normalmente, e ajustar um teste-t pareado para cada subamostra de cada uma das duas populações4.\n\nCódigo# criando funcao para fazer a reamostragem\ncreate_stratified_sample &lt;- function(dataset, tamanho,...) {\n  dataset %&gt;% \n    # agrupa de acordo com o teste logico que for passado em ...\n    group_by(...) %&gt;% \n    # retira uma amostra aleatoria de um determinado tamanho de cada grupo \n    slice_sample(n = tamanho) %&gt;% \n    # desagrupa o dataframe\n    ungroup\n}\n\n## criando dataframe reamostrado\nreamostragem_estratificada &lt;- rerun(.n = 500,\n                                    # rodando a funcao para criar amostrar 500 vezes, cada\n                                    # uma das quais amostrando 50 observacoes aleatoriamente\n                                    create_stratified_sample(dataset = ranks, tamanho = 30, \n                                                             ranking &lt;= ponto_de_corte)) %&gt;% \n  # juntando cada um dos dataframes e adicioando um sample id\n  bind_rows(.id = 'sample_id') %&gt;% \n  # renomeando a coluna de estratificacao\n  rename(estratificacao = `ranking &lt;= ponto_de_corte`) %&gt;% \n  # aninhando o dataframe com os dados que vamos usar para ajustar o test-t\n  # dentro de cada sample_id\n  nest(data = -c(sample_id, estratificacao))\n\n# ajustando o teste-t pareado à cada amostra pertencente à cada grupo\nresultados_reamostragem_estratificada &lt;- reamostragem_estratificada %&gt;% \n  mutate(\n    # aplicando um teste-t pareado às amostras de cada sample_id\n    test_t = map(.x = data, \n                 .f = ~t.test(x = .x$ludopedia, y = .x$bgg, \n                              alternative = 'two.sided', paired = TRUE, var.equal = FALSE)\n    ),\n    # extraindo as informacoes tidy do ajuste do test-t pareado\n    tidy_t = map(.x = test_t, .f = broom::tidy)\n  ) %&gt;% \n  # desaninhando os resultados do test-t\n  unnest(tidy_t) %&gt;% \n  # jogando fora alguma colunas que nao precisaremos mais\n  select(-data, -test_t)\nresultados_reamostragem_estratificada\n\n# A tibble: 1,000 × 10\n   sample_id estratificacao estimate statistic  p.value parameter conf.low\n   &lt;chr&gt;     &lt;lgl&gt;             &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 1         FALSE            -0.561     -3.05 4.90e- 3        29   -0.938\n 2 1         TRUE              0.769      9.56 1.83e-10        29    0.605\n 3 2         FALSE            -0.457     -3.04 5.04e- 3        29   -0.765\n 4 2         TRUE              0.506      5.07 2.09e- 5        29    0.302\n 5 3         FALSE            -0.305     -2.58 1.51e- 2        29   -0.547\n 6 3         TRUE              0.774      6.20 9.31e- 7        29    0.519\n 7 4         FALSE            -0.389     -2.44 2.10e- 2        29   -0.714\n 8 4         TRUE              0.878      8.38 3.07e- 9        29    0.664\n 9 5         FALSE            -0.512     -3.40 1.99e- 3        29   -0.820\n10 5         TRUE              0.658      7.84 1.21e- 8        29    0.487\n# ℹ 990 more rows\n# ℹ 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nUma vez que já temos os resultados das estimativas das diferenças para cada subamostra de cada uma das duas populações, podemos agora extrair o valor da tendência central da distribuição, bem como o intervalo de confiança associado à cada grupo. Eu optei aqui por usar uma média e os quantis da distribuição para estas duas últimas informações mas, de novo, daria para usar uma meta-análise também. O resultado disso pode ser visto na tabela abaixo, em que fica muito clara que a diferença entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG é muito dependente da porção da lista que estamos analisando: as avaliações dos jogos são mais enviesadas para notas maiores no portal da Ludopedia até a posição 2.254, e depois daí o viés inverte de forma muito forte.\n\nCódigoresultados_reamostragem_estratificada %&gt;% \n  # agrupando pelo grupo de posicao do ranking\n  group_by(estratificacao) %&gt;% \n  # calculando a estimativa central e intervalo de confianca por grupo\n  summarise(\n    'Diferença estimada' = mean(x = estimate),\n    'IC inferior'        = quantile(x = estimate, probs = 0.025),\n    'IC superior'        = quantile(x = estimate, probs = 0.975)\n  ) %&gt;% \n  # ajustando o texto da coluna de estratificacao\n  mutate(\n    estratificacao = ifelse(test = estratificacao, \n                            yes = str_glue('Até a {ponto_de_corte}º posição'), \n                            no = str_glue('Acima da {ponto_de_corte}º posição'))\n  ) %&gt;% \n  # arredondando tudo o que for numerico para duas casas decimais\n  mutate(across(where(is.numeric), \\(x) round(x, digits = 2))) %&gt;% \n  # renomeando a coluna do grupo de posicao\n  rename(Grupo = estratificacao) %&gt;% \n  # organizando a tabela em ordem decrescente\n  arrange(desc(`Diferença estimada`)) %&gt;% \n  # printando a tabela\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nTambém podemos visualizar estes mesmos resultados através da figura abaixo - focando aqui não só no intervalo de confiança em si, mas também nos valores que ficaram abaixo e cima do intervalo de confiança.\n\nCódigo# criando a figura\nresultados_reamostragem_estratificada %&gt;% \n  # estilizando o texto que aparecerá no eixo y\n  mutate(\n    estratificacao = ifelse(test = estratificacao, \n    yes = str_glue('Até a {ponto_de_corte}º posição'), \n    no = str_glue('Acima da {ponto_de_corte}º posição'))\n  ) %&gt;% \n  # plotando a figura\n  ggplot(mapping = aes(x = estimate, y = estratificacao, fill = factor(stat(quantile)))) +\n  stat_density_ridges(geom = 'density_ridges_gradient', scale = 0.95, \n                      calc_ecdf = TRUE, quantiles = c(0.025, 0.975), \n                      show.legend = FALSE) +\n  scale_fill_manual(values = c('tomato', 'grey80', 'tomato')) +\n  geom_vline(xintercept = 0, color = 'black', alpha = 0.5) +\n  labs(\n    title    = 'A diferença na nota entre os dois portais depende da posição do ranking',\n    subtitle = str_glue('O ranking da Ludopedia tem notas maiores que aquele do BoardGameGeek para os títulos que ocupem\\naté a {ponto_de_corte}º posição, a partir de onde eles passam a ser melhores avaliados neste último'),\n    caption  = 'A área cinza representa o intervalo de confiança de 95% da estimativa da diferença nas notas entre os\\ndois portais, obtidas através de 500 estimativas independentes tomadas a partir de 30 amostras aleatórias dos dados.',\n    x        = 'Diferença entre as notas do ranking da Ludopedia e do BoardGameGeek'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nUma coisa que podemos perceber com estes resultados é que eles parecem retratar muito melhor a tendência apresentada por àquela figura que mostra relação entre a diferença entre as notas e a posição do ranking. Acho que, com isso, já posso me dar por satisfeito com as análises que fizemos, e parar por aqui."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#footnotes",
    "href": "posts/2021-12-11_notas-boardgames/index.html#footnotes",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nhttps://infer.netlify.app/index.html↩︎\nExiste uma excelente explicação sobre a média bayesiana, sua utilização para rankear itens e implementação em código em: https://www.algolia.com/doc/guides/solutions/ecommerce/relevance-optimization/tutorials/bayesian-average/↩︎\nA variância calculada é de 0.642 na Ludopedia vs 0.2404 no BGG↩︎\nOu seriam sub-populações, nesse caso?↩︎"
  },
  {
    "objectID": "posts/2021-12-23_convertendo-coordenadas/index.html",
    "href": "posts/2021-12-23_convertendo-coordenadas/index.html",
    "title": "Convertendo coordenadas através da calculadora geográfica do INPE",
    "section": "",
    "text": "Há algumas semanas uma das pessoas com quem trabalho trouxe a necessidade de fazer uma conversão de coordenadas, a fim de que pudéssemos seguir com algumas análises que estávamos fazendo. Essa tarefa parecia ser coisa simples, pois deveríamos apenas passar as coordenadas da projeção1 de grau-minuto-segundo no datum2 SAD69 para a projeção de grau decimal no datum SIRGAS2000. Eu sabia que existia uma forma de fazer essas conversões a partir dos metadados disponíveis em um arquivo shapefile, tanto através do pacote sf no R quanto na lib geopandas no Python. Assim, não havia com o que se preocupar…certo?\nDe uma forma surpreendente, não consegui encontrar um jeito confiável de fazer a conversão das coordenadas nem no R e nem no Python. A primeira tentativa que fiz foi no R, e acabei esbarrando com a falta de suporte ao datum SIRGAS2000: apesar do código EPSG3 para o SIRGAS2000 existir, as funções do sf não parecem ter suporte para ela - falhando na conversão logo de cara. Com isso, fiz minha segunda tentativa usando o Python, mas fiquei meio desconfiado do output: na maior parte dos casos, parecia que o geopandas fazia a mudança de projeção e datum do arquivo shapefile sem que, no entanto, os valores das coordenadas em si fossem alteradas. Assim, acabamos esbarrando nesse bloqueio para avançar.\nComo não eram muitos pontos que deveriam ter as coordenadas convertidas - mais ou menos uns 50 -, surgiu a ideia de usar uma aplicação como QGIS para realizar as conversões. Apesar da ideia ser boa para o momento, ela traria muitos problemas no curto ou médio prazo: (1) precisávamos colocar as informações de latitude e longitude dentro de um dataframe geográfico (i.e., um geodataframe ou um sf) e setar o seu datum e projeção, (2) a partir daí precisaríamos salvar o arquivo para o disco para abrir no QGIS, (3) onde precisaríamos executar manualmente muitos passos para converter as coordenadas e exportar um novo shapefile que, (4) finalmente, poderíamos abrir no R/Python para usar. Além disso, em algum momento receberíamos mais um batch de dados, e precisaríamos repetir o procedimento todo de novo. Logo, resolvemos usar essa solução para sair do lugar naquele momento, mas precisávamos de outra estratégia para tornar essa etapa do pipeline de dados mais robusta e reprodutível.\nUma solução que propus para isso foi o uso da calculadora geográfica do INPE. Eu já havia usado ela para desenvolver um trabalho que no passado, e sabia que ali teríamos um resultado bastante confiável. Naquela época, eu havia feito a conversão das coordenadas toda de forma manual4, mas achava que seria tranquilo usar as técnicas de web scrapping que aprendi para automar o processo. No fim das contas, não foi! Todavia, acredito que o exercício foi útil para usar o Selenium para interagir com uma página dinâmica. Nesse contexto, acabei criando essa automação usando o Python, mas reproduzi os mesmos passos com o R e, neste post, aproveito o reticulate para contar sobre a solução usando tanto o Python quanto o R."
  },
  {
    "objectID": "posts/2021-12-23_convertendo-coordenadas/index.html#footnotes",
    "href": "posts/2021-12-23_convertendo-coordenadas/index.html#footnotes",
    "title": "Convertendo coordenadas através da calculadora geográfica do INPE",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nÉ a forma pela qual a superfície de uma esfera é achatada em um plano de forma a criar um mapa, envolvendo uma série de transformações para que a latitude e longitude de cada ponto seja equivalente entre os dois.↩︎\nÉ o sistema de referência utilizado para medir precisamente as distâncias no planeta ou qualquer outra entidade planetária.↩︎\nEste é um código numérico único para representar cada combinação de datum, sistemas de referência espacial, ellipsoides, transformações de coordenadas e unidades de medida.↩︎\nIsso foi há uns 5 anos atrás.↩︎"
  },
  {
    "objectID": "posts/2022-01-08_associacoes-gwent/index.html",
    "href": "posts/2022-01-08_associacoes-gwent/index.html",
    "title": "Quais as associações entre as cartas de Gwent nos decks existentes?",
    "section": "",
    "text": "Motivação\nGwent é um jogo de cartas do universo de The Witcher no qual dois jogadores se enfrentam em busca da maior pontuação em pelo menos 2 de 3 rodadas, cada uma com no máximo uns 10 turnos para cada jogador. Esta pontuação é dada pelo poder de cada carta e, também, através da forma que elas interagem entre si. A tabela abaixo traz um exemplo disto para 4 cartas pertencentes aos decks da facção Scoia’tael - uma das 6 facções existentes no jogo.\n\n\n De volta ao topoReusohttps://creativecommons.org/licenses/by/4.0/deed.ptCitaçãoBibTeX@online{marino2022,\n  author = {Marino, Nicholas},\n  title = {Quais as associações entre as cartas de Gwent nos decks\n    existentes?},\n  date = {2022-01-08},\n  url = {https://nacmarino.netlify.app//posts/2022-01-08_associacoes-gwent},\n  langid = {pt}\n}\nPor favor, cite este trabalho como:\nMarino, Nicholas. 2022. “Quais as associações entre as cartas de\nGwent nos decks existentes?” January 8, 2022. https://nacmarino.netlify.app//posts/2022-01-08_associacoes-gwent."
  }
]