[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "De volta ao topo"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Bio\nAprendi a viver e a fazer ciência ao longo da minha trajetória profissional, através de experimentos e estudos observacionais, publicando artigos científicos, contribuindo em grupos de pesquisa e coordenando projetos. Além disso, também atuei como professor colaborador, ministrando disciplinas orientadas à análise de dados e programação. Foi durante esta trajetória percebi que não precisava permanecer na academia para continuar fazendo ciência, e passei a atuar como cientista (de dados) na iniciativa privada. Tenho uma vontade enorme de aprender e gosto de atuar em projetos onde as perguntas não estão bem definidas e as soluções fogem do convencional. Além de estudar e programar, estão entre minhas paixões a minha família, a corrida e os jogos de tabuleiro.\n\n\nFormação\n\nSuperior\nUNIVERSIDADE FEDERAL DO RIO DE JANEIRO | Rio de Janeiro, RJ\n\nPós-Doutorado Ecologia | Programa de Pós-Graduação em Ecologia | Março 2016 - Junho 2019\n\nEfeito das Mudanças Climáticas sobre as Interações Ecológicas\nBolsista PNPD/CAPES\n\n\nPh.D. Ecologia | Programa de Pós-Graduação em Ecologia | Março 2012 - Fevereiro 2016\n\nO efeito de predadores na estrutura e funcionamento de cadeias traficas e sua interação com as mudanças climáticas\nBolsista CAPES\nPrêmio de Melhor Tese de Doutorado defendida no Programa em 2016\n\n\nM.Sc. Ecologia | Programa de Pós-Graduação em Ecologia | Agosto 2009 - Outubro 2011\n\nEstruturação da comunidade de macroinvertebrados aquáticos em bromélias-tanque\nBolsista CAPES\n\n\nB.Sc. Ciências Biológicas | Especialização em Ecologia | Agosto 2005 - Julho 2009\n\nBromélias como ilhas: Influência das características do habitat sobre a riqueza de macroinvertebrados aquáticos\nBolsista FAPERJ\n\n\n\n\n\nComplementar\n\nDATACAMP\n\nData Scientist: Customer Channel/Marketing (Basic, Intermediate & Advanced) | 2021\nData Scientist: Risk (Basic) | 2021\nInteractive Data Visualization with R | 2020\nUnsupervised Machine Learning with R | 2020\nSupervised Machine Learning with R | 2020\nMachine Learning Specialist with R | 2020\nMachine Learning Fundamentals with R | 2020\nPython Programmer | 2019\nData Scientist with Python | 2019\nData Analyst with Python | 2019\nShiny Fundamentals with R | 2019\n\n\nDATA SCIENCE ACADEMY\n\nFormação Inteligência Artificial | Em andamento\nPython Fundamentos para a Análise de Dados | 2020\nMicrosoft Power BI para Data Science | 2020\nBig Data Fundamentos | 2020\nIntrodução à Inteligência Artificial | 2020\nWeb Scrapping e Análise de Dados | 2020\nSoft Skills: Desenvolvendo Suas Habilidades Comportamentais | 2020\n\n\nCURSO-R\n\nDashboards | 2021\nWeb Scrapping | 2021\nDeploy | 2021\nRelatórios e Visualização de Dados | 2021\nDeep Learning | 2020\n\n\nCOURSERA\n\nData Science | Johns Hopkins University | Em andamento\n\nReproducible Research | 2021\nExploratory Data Analysis | 2021\nGetting and Cleaning Data | 2021\nR Programming | 2021\nThe Data Scientist’s Toolbox | 2021\n\nNLP Specialization | Em andamento\n\nNatural Language Processing with Sequence Models | Em andamento\nNatural Language Processing with Probabilistic Models | 2022\nNatural Language Processing with Classification and Vector Spaces | 2022\n\nDeep Learning Specialization | 2022\n\nSequence Models | 2022\nConvolutional Neural Networks | 2022\nStructuring Machine Learning Projects | 2022\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization | 2022\nNeural Networks and Deep Learning | 2022\n\nIntroduction to Mathematical Thinking | 2021\nLearn to Program: Crafting Quality Code | 2021\nMathematics for Machine Learning\n\nPrincipal Component Analysis | 2022\nMultivariate Calculus | 2021\nLinear Algebra | 2021\n\n\n\nCLOUD\n\nAWS Cloud Practitioner | 2022\n\n\n\n\n\nExperiência\n\nAccenture | Cientista de Dados | Julho 2019 - Presente\n\nData Science Principal | Abril 2021 - Atual\nData Science Manager | Janeiro 2021 - Março 2021\nData Science Consultant | Julho 2019 - Dezembro 2020\n\n\nPrograma de Pós-Graduação em Ecologia/UNICAMP | Professor Colaborador | Q1 2019\n\nIntrodução ao Manejo e Visualizacao de Dados\n\n\nPrograma de Pós-Graduação em Ecologia/UFRJ | Professor Colaborador | Março 2016 - Junho 2019\n\nManejo, Visulizacao e Compartilhamento de Dados | 2018\n\nRevisão Sistemática e Meta-Análise | 2016 - 2019\nDelineamento Experimental e Estatística | 2016 - 2019\nIntrodução à Linguagem R | 2016 - 2019\n\n\nLaboratório de Limnologia/UFRJ | Pesquisador | Maio 2005 - Junho 2019\n\nPrograma de Monitoramento Integrado dos Igarapés da FLONA Saracá-Taquera | 2009 - 2020\nPrograma de Monitoramento das Lagoas do Norte Fluminense - ECOLagoas | 2005 - 2009\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "backlog.html",
    "href": "backlog.html",
    "title": "Backlog",
    "section": "",
    "text": "Web Scraping\n\nAnúncios do ZapImóveis;\n\nOpiniões dos funcionários do Glassdoor;\nColaboradores da empresa no LinkedIn;\nPerfil do LinkedIn;\nPostagens do LinkedIn.\n\n\n\nShinyApp\n\nRecomendação de jogos de tabuleiro;\n\n\n\nAnálises\n\nState of Data 2023;\n\n\n\nEnsaios\n\nReview sobre os cursos curtos em Gen AI do DeepLearning.ai;\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Raspando a Página do Ranking da Ludopedia\n\n\nEu já havia raspado a página do ranking do portal do BoardGameGeek, e agora eu vou repetir a tarefa focando no ranking do portal da Ludopedia. Meu objetivo com isso é criar a base para que, mais tarde, possamos fazer análises comparando os jogos entre os dois portais.\n\n\n\n\nweb scraping\n\n\n \n\n\n\n\n24 de out. de 2021\n\n\n17 minutos\n\n\n\n\n\n\n  \n\n\n\n\nEntendendo os Padrões de Duração das Provas da Fórmula 1\n\n\nA Fórmula 1 é um dos esportes de velocidade mais famosos do mundo, com provas ocorrendo desde o início da década de 50 até os dias de hoje. Muita coisa mudou nestes 70 anos, especialmente os carros: cada vez mais bonitos, mais seguros e mais rápidos. Mas será que isso também se traduziu em provas cada vez mais curtas também? Neste post eu examino de que forma a duração das provas da Fórmula 1 têm evoluído ao longo das temporadas.\n\n\n\n\nanalise\n\n\n \n\n\n\n\n8 de out. de 2021\n\n\n47 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a página do ranking do BoardGameGeek\n\n\nNeste post eu faço a raspagem da tabela do ranking dos jogos de tabuleiro do BoardGameGeek. Essa tarefa foi necessária para que eu conseguisse interagir da melhor forma possível com a API XML que o site oferece.\n\n\n\n\nweb scrapping\n\n\n \n\n\n\n\n17 de set. de 2021\n\n\n18 minutos\n\n\n\n\n\n\n  \n\n\n\n\nPrevendo o preço de apartamentos em Niterói/RJ\n\n\nA previsão de preços de imóveis é uma tarefa muito comum em ciência de dados, existingo até Hello World para esta prática - o Ames Housing, com informações sobre o preço e outros metadados de imóveis na cidade de Ames em Iowa. Neste post, sigo esta idéia e utilizo um conjunto de dados reais sobre os apartamentos disponíveis para a venda no município de Niterói/RJ. Buscarei entender e prever a variação no preço destes imóveis de acordo com as informações contidas nos anúncios com a ajuda de um modelo de Machine Learning.\n\n\n\n\nmachine learning\n\n\n \n\n\n\n\n27 de ago. de 2021\n\n\n52 minutos\n\n\n\n\n\n\nNenhum item correspondente\n\n De volta ao topo"
  },
  {
    "objectID": "posts/2021-08-27_niteroi-housing-prices/index.html",
    "href": "posts/2021-08-27_niteroi-housing-prices/index.html",
    "title": "Prevendo o preço de apartamentos em Niterói/RJ",
    "section": "",
    "text": "Nota\n\n\n\nEste post foi o meu trabalho de conclusão do curso Relatórios e visualização de dados que fiz em julho de 2021 na Curso-R Curso-R, que foi escolhido como um dos três melhores trabalhos da turma."
  },
  {
    "objectID": "posts/2021-08-27_niteroi-housing-prices/index.html#footnotes",
    "href": "posts/2021-08-27_niteroi-housing-prices/index.html#footnotes",
    "title": "Prevendo o preço de apartamentos em Niterói/RJ",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nhttps://pt.wikipedia.org/wiki/Niter%C3%B3i↩︎\nShapefile obtido a partir de https://geo.niteroi.rj.gov.br/civitasgeoportal/↩︎\nNão mostro aqui mas, de fato, a relação entre muitas delas não é linear.↩︎\nDei uma lida no texto de descrição dos anúncios e parece que todos os casos que narrei aqui são erros de imputação de informação mesmo. Assim, acredito que esta solução não seja um problema.↩︎\nSe você for uma pessoa curiosa, eu acabei imputando um NA a 583 linhas da base↩︎\nNão existe uma função no fastshap para criar essa figura, então tentei emular uma figura similar que existente dentro do pacote shap.↩︎"
  },
  {
    "objectID": "posts/2021-09-17_scrapper-boardgamegeek/index.html",
    "href": "posts/2021-09-17_scrapper-boardgamegeek/index.html",
    "title": "Raspando a página do ranking do BoardGameGeek",
    "section": "",
    "text": "Sempre joguei os jogos de tabuleiro mais tradicionais, como Banco Imobiliário, Scotland Yard e War. Esses são jogos muito populares, apesar de cada partida ser muito repetitiva e eles demandarem uma quantidade razoável de jogadores para que tenham graça - e, no meio de uma pandemia, se já acabava sendo chato jogar um deles, a coisa passou a ser impossível. Mas será que não existem alternativas (mais divertidas, inclusive) para continuar com a distração num momento tão difícil como esse? Como eu bem descobri, a resposta estava nos próprios jogos de tabuleiro - mais precisamente, na reinvenção que eles sofreram nas últimas décadas.\nExistem inúmeros jogos de tabuleiro disponíveis atualmente e um número crescente de pessoas que os curtem. Dada esta diversidade de novos títulos, inúmeros portais têm focado em criar e manter essa cultura, trazendo reportagens, fóruns, marketplaces, reviews, rankings e fichas técnicas de cada um deles. Dois exemplos destes sites são o BoardGameGeek e a Ludopedia: ambos possuem praticamente o mesmo conteúdo, mas o primeiro é um portal americano e o segundo é brasileiro. Outro ponto interessante é que o consumo de informações desses portais não precisa ocorrer pelo browser, uma vez que ambos fornecem uma API. A Ludopedia oferece uma API REST bastante intuitiva1, enquanto o BoardGameGeek usa uma API XML que eu acabei achando meio complicada de usar. Mas o que isto tudo tem haver com dados?\nBom, logo que descobri esse hobby, acabei ficando muito perdido sobre quais são os títulos mais legais para se jogar. São tantas as possibilidades e informações disponíveis sobre cada jogo, que eu me peguei navegando entre inúmeras páginas naqueles portais para tentar encontrar aquilo que eu estava buscando. Assim, acabei tendo a ideia de compilar essas informações e colocar tudo dentro de uma linguagem de programação, a fim de deixar a análise de dados me ajudar a encontrar os jogos que mais combinavam com aquilo que eu estava buscando. Para isso, tive a ideia de pegar as informações dos jogos do BoardGameGeek (BGG daqui em diante) através de sua API, tabular tudo o que estava buscando e partir para o abraço. Mas nada é tão simples quanto parece.\nA parede que encontrei é bem chatinha: o request da API XML do BoardGameGeek funciona muito melhor quando usamos o código numérico de identificação do jogo. Quando passamos o nome do jogo para o request, ele precisa estar grafado igual à como está na base do BGG, caso contrário ele pode falhar em trazer o que você está buscando ou trazer todos os títulos que tenham um match parcial com aquele que você buscou (daí para a frente é só caos). Outra ressalva aqui é que essa API não oferece nenhum tipo de método através do qual podemos pegar um de-para dos IDs numéricos para os nomes dos jogos, e o código numérico deles também não é sequencial. Logo, não dá para fazer uma busca gulosa e loopar os IDs de 1 até n. A solução mais simples para o problema é montar a nossa própria base de-para, catando o nome dos títulos e o seu ID numérico de algum lugar do site do BGG - e esse lugar é a página que contém o ranking dos jogos de tabuleiro no site.\nNeste post eu vou mostrar como raspar a página do ranking do BGG, usando como base o fluxo do Web Scrapping que a galera da Curso-R criou (Lente (2018)), e muito bem ilustrada na figura abaixo.\n\n\nCódigo\nknitr::include_graphics(path = 'images/web_scrapping_cycle_curso_r.png')\n\n\n\n\n\nFluxo do Web Scrapping de acordo com o Lente (2018). Figura copiada de https://blog.curso-r.com/posts/2018-02-18-fluxo-scraping/."
  },
  {
    "objectID": "posts/2021-09-17_scrapper-boardgamegeek/index.html#footnotes",
    "href": "posts/2021-09-17_scrapper-boardgamegeek/index.html#footnotes",
    "title": "Raspando a página do ranking do BoardGameGeek",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nEssa API ainda está em desenvolvimento, e devo escrever sobre o consumo de informações através dela em outro post↩︎"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html",
    "href": "posts/2021-10-18_formula-1/index.html",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "",
    "text": "A Fórmula 1 é um dos esportes de velocidade mais famosos do mundo, com provas ocorrendo desde o início da década de 50 até os dias de hoje. Com um histórico tanto grande desses, com provas tão intrincadas, tantas montadoras, pilotos e acontecimentos, é de se imaginar também que uma enorme quantidade de dados tenham sido gerados em cada prova. Seguindo esta ideia, existe um site1 que hospeda este registro histórico, e que foi o tema de um #TidyTuesday2 recentemente. Achei os dados disponíveis bastante interessantes e, como (toda) pessoa interessada em dados, logo comecei a pensar sobre os tipos de perguntas que poderiam ser respondidos olhando para eles. Mas antes de seguir essa estória, vamos baixar os dados do repositório oficial do TidyTuesday (ou usá-los localmente se você já o tiver baixado).\n\n# carregando os pacotes necessários\nlibrary(tidyverse) # core\nlibrary(fs) # manipular paths\nlibrary(lubridate) # trablhar com datas\nlibrary(ggforce) # extender o ggplot2\nlibrary(broom) # wrangling dos resultados da regressão\nlibrary(metafor) # para a meta-análise\nlibrary(reactable) # tabelas reativas\nlibrary(sparkline) # embedar widgets\nlibrary(patchwork) # compor figuras\nlibrary(tidytuesdayR) # ler os arquivos do tidytuesday\n\n# carregando todos os dados a partir do github do tidytuesday\n# se você quiser baixar os dados direto da fonte é só descomentar \n# como existe um limite de requests que podem ser feitos ao site, resolvi\n# deixar aqui só para referência mesmo\n# tt_dataset &lt;- tt_load(x = 2021, week = 37)\n\n# carregando a copia local dos dados\n## extraindo os paths das copias locais\npaths_copias_locais &lt;- dir_ls(path = 'data/')\n\n## criando vetor de nomes dos arquivos\nnomes_arquivos &lt;- paths_copias_locais %&gt;% \n  path_file() %&gt;% \n  path_ext_remove()\n\n## carregando os arquivos em uma lista\ntt_dataset &lt;- map(.x = paths_copias_locais, .f = read_rds)\n\n## renomeando os elementos da lista\nnames(tt_dataset) &lt;- nomes_arquivos\n\nUma das primeiras coisas que me ocorreram é que ao longo do tempo a tecnologia do setor automobilístico avançou bastante, levando ao desenvolvimento de motores cada vez mais performáticos e potentes. Não só os motores em si devem ter melhorado, mas todas as outras partes dos carros sofreram alterações no intuito de melhorar a sua aerodinâmica e fornecer mais vantagens aos pilotos que os conduzem - seja em termos de manobrabilidade, velocidade, aceleração ou segurança. Neste contexto, eu esperaria que as provas da Fórmula 1 ficassem cada vez mais curtas com o passar dos anos, certo? Então, a primeira coisa que fiz foi montar um histórico com a evolução da duração das provas ao longo dos anos.\nComo primeiro passo para construir este histórico precisei combinar duas tabelas que estão disponíveis junto aos dados - tt_dataset$results e tt_dataset$status. Fiz isso pois a primeira tabela traz o tempo de conclusão de prova de cada piloto, enquanto a segunda tabela é um de-para que nos permite mapear quais pilotos concluíram ou não cada uma delas. Usei esta informação para filtrar a primeira tabela e extrair o tempo de prova (em minutos) de cada piloto que as concluiu. Com base nessa informação, então, calculei o tempo de duração de cada prova como a média do tempo entre todos os pilotos, bem como o desvio padrão deste valor e a quantidade de observações nas quais se baseiam estas estimativas.\n\n\nCódigo\n## adicionando o dicionario com o de-para do statusId\nresultados &lt;- left_join(x = tt_dataset$results,\n                        y = tt_dataset$status,\n                        by = 'statusId')\n\n# criando base com o minimo, media e maximo dos tempos de cada prova\ntempos_de_prova &lt;- resultados %&gt;% \n  # pegando apenas os pilotos que concluiram a prova\n  filter(status == 'Finished') %&gt;% \n  # removendo qualquer valor na coluna milliseconds que não contenha pelo menos um número\n  filter(str_detect(string = milliseconds, pattern = '[0-9]')) %&gt;% \n  # parseando a coluna de milliseconds para numerico\n  mutate(\n    milliseconds = parse_number(milliseconds),\n    # calculando a quantidade de tempo em horas\n    minutos      = (milliseconds / 1000) / 60\n  ) %&gt;% \n  # agrupando pela prova\n  group_by(raceId) %&gt;% \n  # pegando o valor minimo, medio e maximo dos tempo de prova\n  summarise(\n    media  = mean(minutos, na.rm = TRUE),\n    erro   = sd(minutos, na.rm = TRUE),\n    obs    = n()\n  )\n\n## criando a tabela para a visualização\ntempos_de_prova %&gt;% \n  # passando para caracter só para facilitar o plot da tabela\n  mutate(raceId = as.character(raceId)) %&gt;% \n  reactable(striped = TRUE, highlight = TRUE, compact = TRUE, \n            columns = list(\n              media = colDef(name = 'Duração média (min)', format = colFormat(digits = 2)),\n              erro  = colDef(name = 'Desvio padrão', format = colFormat(digits = 2)),\n              obs = colDef(name = 'Observações')\n            ),\n            defaultColDef = colDef(align = 'center',\n                                   footer = function(values) {\n                                     if (!is.numeric(values)) return()\n                                     sparkline(values, type = \"box\", width = 100, height = 30)\n                                   })\n  )\n\n\n\n\n\n\n\nComo podemos ver na figura abaixo, existe uma tendência forte de queda no tempo de duração das provas até a década de 70 e, então, uma desaceleração desta tendência. Além disso, parece haver uma certa variância nesta série temporal ao longo da última década, inclusive com um aparente aumento nos tempos de prova. Isso acabou me surpreendendo, uma vez que a minha expectativa era de que as provas estariam ficando mais curtas. Mas o que será que poderia estar ocorrendo?\n\n\nCódigo\ntempos_de_prova %&gt;% \n  # juntando com as informacoes da data de ocorrência de cada prova\n  left_join(y = select(tt_dataset$races, raceId, date), \n            by = 'raceId') %&gt;% \n  mutate(\n    # parseando a data para date\n    data   = as_date(x = date),\n    # extraindo o ano do objeto de data\n    year   = year(x = data),\n    # calculando a decada onde ocorreu cada prova\n    decada = (year %/% 10) * 10,\n    # passando a decada para caracter, pois quero que o mapeamento de cores\n    # seja feito usando uma escala discreta, e não contínua\n    decada = as.character(decada)\n  ) %&gt;% \n  # criando a figura do historico de tempos de prova\n  ggplot(mapping = aes(x = data, y = media)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = decada), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('1950-01-01'), to = as.Date('2021-12-01'), by = '5 years'),\n               labels = seq(from = 1950, to = 2020, by = 5)) +\n  scale_fill_viridis_d() +\n  labs(\n    title    = 'Série histórica da duração média das provas',\n    subtitle = 'A linha vermelha representa a tendência geral de duração das provas no histórico, enquanto os pontos representam\\na duração de cada uma das provas',\n    x        = 'Período',\n    y        = 'Duração Média (minutos)'\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\n\nEu esperaria que a duração das provas permanecessem relativamente estáveis em pelo menos duas condições. A primeira delas é no caso dos carros não estarem ficando mais rápidos ao longo dos anos. Não dá para avaliar isso tão bem com os dados que estão disponíveis, uma vez esta informação (i.e., fastestLapSpeed) só passou a ser registrado de forma consistente a partir de 2004 (painel A na figura abaixo). Esta série até mostra que a velocidade andou baixando um pouquinho na última década, mas precisávamos mesmo é ver como eram as velocidades lá para trás, quando a série temporal dos tempos de conclusão passou a ficar mais flat.\nA segunda coisa que me ocorreu seria o caso onde houvesse uma mudança sistemática na quantidade de voltas em cada prova, que refletisse aquele mesmo padrão da duração das provas. Embora o painel B da figura abaixo mostre que existe alguma semelhança entre as duas séries temporais, podemos ver que a variância na quantidade de voltas pareceu ficar bem mais tamponada ao longo do tempo. Além disso, a queda observada ao longo das primeiras décadas não é tão forte quanto àquela observada na outra série temporal. Finalmente, ainda que a mesma quantidade de voltas em cada prova sejam dadas hoje e no passado, não me parece razoável acreditar que a velocidade dos carros não variou em nada neste mesmo período. Claro, pode sempre ter alguma regra da FIA que defina alguns padrões que segurassem aqueles comportamentos, mas algo não parece fechar.\n\n\nCódigo\n## mapeando a velocidade maxima por prova\nvelocidades_por_prova &lt;- resultados %&gt;%\n  # parseando a velocidade para numerico\n  mutate(fastestLapSpeed = parse_number(fastestLapSpeed)) %&gt;% \n  # extraindo a velocidade maxima por prova\n  group_by(raceId) %&gt;% \n  filter(fastestLapSpeed == max(fastestLapSpeed, na.rm = TRUE)) %&gt;% \n  ungroup %&gt;% \n  # garantindo que temos valores unicos por prova\n  distinct(raceId, fastestLapSpeed)\n\n## calculando a quantidade de voltas em cada prova\nvoltas_por_prova &lt;- resultados %&gt;% \n  # considerando apenas os pilotos que concluiram cada prova\n  filter(status == 'Finished') %&gt;% \n  # selecionando as colunas de interesse\n  select(raceId, laps) %&gt;% \n  # pegando o valor maximo da quantidade de voltas por prova\n  group_by(raceId) %&gt;% \n  summarise(laps = max(laps))\n\n## mapeando cada circuito à uma prova\nprovas &lt;- left_join(x = tt_dataset$races,\n                    y = tt_dataset$circuits,\n                    by = 'circuitId') %&gt;% \n  # removendo URL da wikipedia\n  select(-contains('url'), -circuitRef, -circuitId, -round, -time) %&gt;%\n  # renomeando o nome do GP e do circuito\n  rename(gp = name.x, circuit = name.y, data = date)\n\n## juntando informacoes\nfeatures_por_prova &lt;- provas %&gt;% \n  ## juntando de voltas por prova\n  left_join(y = voltas_por_prova, by = 'raceId') %&gt;% \n  ## juntando velocidades por prova\n  left_join(y = velocidades_por_prova, by = 'raceId') %&gt;% \n  ## adicionando decada à tabela\n  mutate(decada = (year %/% 10) * 10)\n\n## criando figura do historico de velocidade por prova\nfig1 &lt;- features_por_prova %&gt;% \n  select(data, decada, fastestLapSpeed) %&gt;% \n  drop_na() %&gt;% \n  ggplot(mapping = aes(x = data, y = fastestLapSpeed)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = as.character(decada)), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('2004-01-01'), to = as.Date('2021-01-01'), by = '4 years'),\n               labels = seq(from = 2004, to = 2020, by = 4)) +\n  scale_fill_viridis_d() +\n  labs(\n    title = '(A) Série histórica da velocidade máxima nas provas',\n    x     = 'Período',\n    y     = 'Velocidade máxima (Km/h)'\n  ) +\n  theme(legend.position = 'none')\n\n## criando figura do historico de voltas por prova\nfig2 &lt;- features_por_prova %&gt;% \n  ggplot(mapping = aes(x = data, y = laps)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = as.character(decada)), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('1950-01-01'), to = as.Date('2021-12-01'), by = '10 years'),\n               labels = seq(from = 1950, to = 2020, by = 10)) +\n  scale_fill_viridis_d() +\n  labs(\n    title = '(B) Série histórica da quantidade de voltas por prova',\n    x     = 'Período',\n    y     = 'Quantidade de voltas'\n  ) +\n  theme(legend.position = 'none')\n\n## criando composição\nfig1 / fig2\n\n\n\n\n\nFoi daí que me ocorreu que essa não era uma série temporal convencional. Isto porquê cada corrida dentro de uma temporada vem de uma prova ocorrida em um dado circuito, e poderia ser o caso que o roster de circuitos tivesse mudado bastante entre as temporadas. Mais importante, dada tantas peculiaridades ligadas aos circuitos em si, não faria mais sentido tentar entender o quanto as provas dentro do mesmo circuito têm ou não ficado mais rápidas entre as temporadas? Isto é, será que nossa unidade básica de previsão para entender os padrões de duração das provas não seria o circuito, ao invés de cada prova em si? Afinal, observações vindas de um mesmo circuito não são independentes entre si.\nCom isto em mente, isolei cada circuito e fiz um levantamento do período no qual cada um deles esteve no roster das temporadas. A primeira coisa legal que vi com isso é que nenhum circuito esteve em todas as temporadas da Fórmula 1 (i.e., nenhum segmento é contínuo de ponta à ponta na figura abaixo). O segundo padrão interessante é que são poucos os circuitos que se mantiveram por um período longo de temporadas (i.e., os circuito estão ordenados de cima para baixo, daqueles com o maior volume de provas para o menor volume de provas). Por fim, outro padrão que me saltou aos olhos foi o fato de que alguns circuitos só acabaram estando em uma única temporada mesmo (i.e., os últimos circuitos na figura abaixo) ou, ainda, participado de forma muito intermitente do roster (i.e., segmentos descontínuos e/ou pontos isolados). Em essência, (1) temos uma série temporal sequência de valores com a duração das provas da Fórmula 1 ao longo das temporadas, (2) estes valores não pertencem sempre as mesmas entidades e, (3) quando estas entidades se repetem, pode ser que estejam bem distantes uma das outras no tempo.\n\n\nCódigo\nfeatures_por_prova %&gt;% \n  # pegando as ocorrências únicas de cada circuito em cada temporada\n  distinct(circuit, data) %&gt;% \n  # extraindo o ano a partir da data de ocorrência da corrida\n  mutate(ano = year(data)) %&gt;% \n  # organizando a base de acordo com os anos dentro de cada circuito\n  arrange(circuit, ano) %&gt;% \n  # agrupando pelo circuito\n  group_by(circuit) %&gt;%\n  mutate(\n    # criando uma dummy que será 1 caso a diferença entre o ano de ocorrência\n    # de corridas sucessivas dentro de um mesmo circuito seja 1 ou caso seja\n    # o primeiro registro de prova naquele circuito\n    recorrencia       = (ano - lag(ano)) != 1 | is.na(ano - lag(ano)),\n    # acumulando a dummy de recorrencia, de forma a criar grupos que sinalizem\n    # anos sucessivos onde houve uma prova naquele circuito\n    grupo_recorrencia = cumsum(recorrencia),\n    # calculando a quantidade total provas registradas em cada circuito\n    n_provas          = n()\n  ) %&gt;% \n  # adicionando a recorrencia ao group_by\n  group_by(grupo_recorrencia, .add = TRUE) %&gt;% \n  summarise(\n    # extraindo o ano de inicio de cada fase sucessiva de ocorrência de provas\n    # em cada circuito\n    inicio   = min(ano),\n    # extraindo o ano de fim de cada fase sucessiva de ocorrência de provas\n    # em cada circuito\n    fim      = max(ano),\n    # extraindo a quantidade total de provas em cada circuito\n    n_provas = max(n_provas),\n    # dropando os grupos\n    .groups = 'drop'\n  ) %&gt;% \n  mutate(\n    # reordenando os níveis do circuito de acordo com a quantidade total de provas\n    circuit = fct_reorder(.f = circuit, .x = n_provas, .fun = sum)\n  ) %&gt;% \n  # criando figura para ver o período e/ou ano de ocorrências das provas em cada circuito\n  ggplot() +\n  geom_segment(mapping = aes(x = inicio, xend = fim, y = circuit, yend = circuit)) +\n  geom_point(mapping = aes(x = inicio, y = circuit), size = 2,\n             shape = 21, color = 'black', fill = 'white') +\n  geom_point(mapping = aes(x = fim, y = circuit), size = 2,\n             shape = 21, color = 'black', fill = 'grey70') +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 10)) +\n  labs(\n    title    = 'Ocorrência das provas em cada circuito ao longo dos anos',\n    subtitle = 'Os segmentos representam o intervalo de anos sucessivos nos quais cada circuito esteve no roster',\n    x        = 'Período'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\n\nParece que não tem muito o que fazer para abordarmos analiticamente a evolução da duração das provas da Fórmula 1. Por um lado, a sequência temporal está bem estacionária há uns 50 anos - fato que parece estar associado à constância na quantidade de voltas em cada prova e, mais recentemente, à mudança bem pequena na velocidade dos carros. Por outro lado, parece que parte do que estamos buscando entender pode estar sendo mascarado pelo fato das provas ocorrerem em diferentes circuitos em cada temporada. Se pudéssemos analisar a série temporal de cada circuito, isto nos daria mais insights sobre a real evolução da duração das provas, uma vez que controlamos o efeito do circuito; todavia, como vimos, temos um sashimi de séries temporais e, portanto, não daria para ajustar um modelo estatístico mais tradicional. Como então atacar esse problema?"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#footnotes",
    "href": "posts/2021-10-18_formula-1/index.html#footnotes",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nhttps://ergast.com/mrd/↩︎\nhttps://github.com/rfordatascience/tidytuesday↩︎\nAlém de ser a função utilizada para realizar os cálculos dos tamanhos do efeito quando estamos usando este pacote para fazer uma meta-análise.↩︎\na função calcula o inverso deste argumento internamente, mas é possível passar o inverso da variância já calculada através do argumento W↩︎\nAlgumas pessoas também chamam de modelo de painel.↩︎\nE isto seria algo que poderia nos motivar a buscar entender o porquê disso, também através de uma meta-análise, utilizando moderadores - que é a forma como chamamos as variáveis preditoras na área.↩︎\nEm essência, vamos estimar um intercepto e um slope fixo para todos os circuitos, além de um intercepto aleatório para cada circuito↩︎"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-i",
    "href": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-i",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Abordagem Meta-Analítica I",
    "text": "Abordagem Meta-Analítica I\nO sucesso de uma meta-análise está logo na sua primeira etapa: a definição de, pelo menos, 2 de 4 informações sobre o que queremos fazer:\n\nPopulação: o objeto básico do nosso estudo. Nesta seção, a nossa população será a sequência de provas ocorridas em um circuito;\n\nIntervenção: o tratamento, intervenção ou variável independente que temos interesse em relacionar com a população. Nesta seção, isso será o tempo - mais especificamente, uma sequência de provas ocorrendo de forma ininterrupta ao longo dos anos;\n\nComparação: aquilo que queremos contrastar. Não faremos uso dessa informação nesta seção, mas na próxima;\n\nOutcome: aquilo que quantifica o impacto da intervenção sobre a população - isto é, o que servirá, direta ou indiretamente, para medir o efeito da intervenção na população. Nesta seção, isto será a taxa de variação da duração das provas ao longo dos anos para um circuito.\n\nE como trabalhamos isto na prática? O outcome que vamos buscar nada mais é, nesse caso, do que o slope de uma regressão entre a duração da prova e o tempo (i.e., os anos) para cada um dos circuitos. Se o slope dessa relação for negativo, então é sinal de que as provas estão ficando curtas ao longo das temporadas; caso contrário, as provas estão ficando mais longas. Um detalhe que não podemos perder de vista é o fato de que muitas vezes um circuito sai do roster e depois retorna. Assim, precisaremos ajustar uma regressão para cada uma das sequências ininterruptas de anos nos quais aquele circuito esteve no roster. Como não queremos errar muito na mão e ajustar uma regressão com poucos pontos, vamos colocar uma restrição para só considerarmos dentro da nossa população as sequências de provas que tenham ocorrido pelo menos durante 5 anos sucessivos em cada circuito. O pedaço de código abaixo dá conta de identificar as provas que atendem à essa restrição em cada circuito, e já prepara o dataframe que vamos usar para ajustar as regressões.\n\n## pegando as provas que ocorrem em sequencia\nprovas_alvo &lt;- df %&gt;% \n  # pegando só as informações do id da prova, circuito e ano\n  select(raceId, circuit, year) %&gt;% \n  # organizando a base de acordo com os anos dentro de cada circuito\n  arrange(circuit, year) %&gt;% \n  # agrupando a base pelo circuito\n  group_by(circuit) %&gt;% \n  mutate(\n    # criando uma dummy que será 1 caso a diferença entre o ano de ocorrência\n    # de corridas sucessivas dentro de um mesmo circuito seja 1 ou caso seja\n    # o primeiro registro de prova naquele circuito\n    recorrencia = (year - lag(year)) != 1 | is.na(year - lag(year)),\n    # acumulando a dummy de recorrencia, de forma a criar grupos que sinalizem\n    # anos sucessivos onde houve uma prova naquele circuito\n    grupo_recorrencia = cumsum(recorrencia)\n  ) %&gt;% \n  ungroup %&gt;% \n  # contando quantas vezes cada grupo dentro de cada circuito aparece - i.e., calculando\n  # o tamanho de cada uma das sequências ininterruptas de provas dentro de cada circuito\n  add_count(circuit, grupo_recorrencia, name = 'ocorrencias_continuas') %&gt;% \n  # removendo toda as sequências compostas por menos de 4 anos consecutivos\n  filter(ocorrencias_continuas &gt;= 5) %&gt;% \n  # pegando só o id da prova e o grupo de recorrencia de cada uma\n  select(raceId, grupo_recorrencia)\n\n## criando dataframe com as provas que vamos usar\ndf_regs &lt;- provas_alvo %&gt;% \n  # filtrando as provas que de fato podemos usar\n  left_join(y = df, by = 'raceId')\nglimpse(x = df_regs)\n\nRows: 849\nColumns: 17\n$ raceId            &lt;dbl&gt; 651, 639, 628, 616, 601, 587, 570, 554, 538, 522, 50…\n$ grupo_recorrencia &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ media             &lt;dbl&gt; 102.77994, 91.07974, 89.98567, 89.59983, 89.63631, 5…\n$ erro              &lt;dbl&gt; 0.8427061623, 0.6348897167, 0.4935152496, 0.64426457…\n$ obs               &lt;int&gt; 3, 7, 8, 6, 6, 7, 7, 5, 3, 5, 7, 6, 2, 4, 4, 5, 1, 2…\n$ year              &lt;dbl&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978…\n$ gp                &lt;chr&gt; \"Austrian Grand Prix\", \"Austrian Grand Prix\", \"Austr…\n$ data              &lt;date&gt; 1970-08-16, 1971-08-15, 1972-08-13, 1973-08-19, 197…\n$ circuit           &lt;chr&gt; \"A1-Ring\", \"A1-Ring\", \"A1-Ring\", \"A1-Ring\", \"A1-Ring…\n$ location          &lt;chr&gt; \"Spielberg\", \"Spielberg\", \"Spielberg\", \"Spielberg\", …\n$ country           &lt;chr&gt; \"Austria\", \"Austria\", \"Austria\", \"Austria\", \"Austria…\n$ lat               &lt;dbl&gt; 47.2197, 47.2197, 47.2197, 47.2197, 47.2197, 47.2197…\n$ lng               &lt;dbl&gt; 14.7647, 14.7647, 14.7647, 14.7647, 14.7647, 14.7647…\n$ alt               &lt;dbl&gt; 678, 678, 678, 678, 678, 678, 678, 678, 678, 678, 67…\n$ laps              &lt;dbl&gt; 60, 54, 54, 54, 54, 29, 54, 54, 54, 54, 54, 53, 53, …\n$ fastestLapSpeed   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ decada            &lt;dbl&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970…\n\n\nComo podemos ver, das 1047 provas que tínhamos originalmente, somente 849 atendem ao pré-requisito de terem ocorrido dentro daquela sequência contínua de anos em cada circuito. A figura abaixo tenta mostrar o que queremos fazer até aqui: ajustar uma regressão para cada sequência de anos dentro de cada circuito, extrair os slopes destas regressões e os erros associado à cada um deles.\n\n\nCódigo\n## visualizando regressões\ndf_regs %&gt;% \n  ggplot(mapping = aes(x = year, y = media, color = circuit, group = grupo_recorrencia)) +\n  facet_wrap(~ circuit, scales = 'free') +\n  geom_point(color = 'black', fill = 'grey70', shape = 21) +\n  geom_smooth(color = 'black', linetype = 1, method = 'lm', se = TRUE) +\n  labs(\n    title = 'Regressões aplicadas à cada período dentro de cada circuito',\n    x     = 'Ano',\n    y     = 'Duração Média (minutos)'\n  )\n\n\n\n\n\nCom isto em mente, é hora de ajustar das regressões! Vamos fazer isso de forma tidy, aninhando o dataframe e deixando de fora apenas o circuito e o identificador da sequência de anos dentro de cada um deles. Na sequência, vamos usar a função purrr::map para ajustar uma regressão para prever a duração das provas de acordo com uma sequência de anos. Um ponto importante é que diferenças nos valores do intervalo de anos para cada regressão influenciam diretamente a estimativa do slope de cada uma delas. Portanto, vamos padronizar a variável independente ano dentro de cada recorte, de forma que os slopes estejam livres do confounding do valor dos anos utilizados para ajustá-los. Neste contexto, os slopes vão representar a mudança na duração das provas para cada unidade de desvio padrão do tempo (i.e., anos).\n\ndf_regs &lt;- df_regs %&gt;% \n  # aninhando o dataframe e deixando de fora apenas o circuito e o identificador da \n  # sequencia de anos dentro de cada circuito\n  nest(data = -c(circuit, grupo_recorrencia)) %&gt;% \n  mutate(\n    # padronizando o ano dentro de cada recorte\n    data      = map(.x = data, .f = mutate, year_scaled = (year - mean(year)) / sd(year)),\n    # ajustando uma regressão para cada recorte\n    modelo    = map(.x = data, .f = ~ lm(media ~ year_scaled, data = .x)),\n    # extraindo os coeficientes da regressão\n    tidyed    = map(.x = modelo, .f = tidy),\n    # extraindo o ano maximo dentro de cada recorte\n    ano_max   = map_dbl(.x = data, .f = ~ pull(.x, 'year') %&gt;% max),\n    # extraindo o ano minimo dentro de cada recorte\n    ano_min   = map_dbl(.x = data, .f = ~ pull(.x, 'year') %&gt;% min),\n    # extraindo a quantidade de anos que cada intervalo compreende\n    intervalo = ano_max - ano_min\n  )\n\nRegressões ajustadas, vamos olhar o que conseguimos extrair dos dados. A figura abaixo mostra que os slopes (i.e., estimate) não estão muito relacionados às estimativas de erro, o ano de fim da estimativa e nem o intervalo de anos. Por outro lado e, como era de se esperar, as estimativas de erro parecem ser menores tanto maior forem a quantidade de pontos que usamos para ajustar as regressões. Tirando isso, nada de muito surpreendente nos dados.\n\n\nCódigo\ndf_regs %&gt;% \n  # desempacotando a coluna com as estimativas de cada regressão\n  unnest(tidyed) %&gt;% \n  # pegando só os slopes\n  filter(term == 'year_scaled') %&gt;% \n  # plotando a figura\n  ggplot() +\n  geom_autopoint(shape = 21, color = 'black', fill = 'grey70', alpha = 0.5) +\n  geom_autodensity(color = 'black', fill = 'grey70') +\n  facet_matrix(vars(estimate, std.error, ano_max, intervalo), layer.diag = 2)\n\n\n\n\n\nVamos fazer uma breve pausa para entender o próximo passo da meta-análise. O outcome que estávamos buscando é o slope das regressões da duração das provas vs o tempo para cada sequência ininterrupta de anos nos quais elas ocorreram em cada circuito. Dentro do contexto da meta-análise, utilizaremos estes slopes como uma medida do tamanho do efeito (i.e., effect size): a informação sobre a magnitude e o sinal de uma intervenção sobre a população estudada. É esta a medida que será combinada através do modelo meta-analítico. Aqui estamos usando o slope das regressões como métrica de tamanho do efeito, mas qualquer métrica quantitativa pode ser usada para tal em uma meta-análise: o valor de uma média, uma métrica relacionada a um modelo (e.g., coeficiente de determinação, coeficiente de correlação,…) a um tipo de problema de negócio (e.g., acurácia, AUC,…), informações extraídas de uma tabela de contingência, comparações entre médias (veremos um exemplo deste na próxima seção). O importante é que o tamanho do efeito seja caracterizado por uma métrica quantitativa, comum a todos os estudos na população que estamos estudando. A função escalc do pacote metafor fornece uma visão bastante detalhada sobre as diferentes métricas de tamanho do efeito e os seus casos de uso3.\nUma outra coisa que precisamos é uma estimativa da incerteza ao redor do tamanho do efeito. Isto é importante pois nem todos os estudos possuem a mesma precisão ao estimar as relações que estamos querendo investigar e, se queremos combiná-los, devemos levar em consideração que estimativas mais precisas devem ter um peso maior na nossa análise do que àquelas com maior erro. Esta medida de incerteza normalmente é dada como o inverso da variância do tamanho do efeito, e é utilizada no modelo meta-analítico para ponderar cada estudo. Logo, nesse contexto, um modelo meta-analítico pode ser pensado como um tipo de regressão ponderada. Um curiosidade importante: você só pode chamar uma meta-análise como tal caso esta medida de incerteza seja utilizada para ajustar o modelo; caso contrário (i.e., todos os tamanhos do efeito têm o mesmo peso), a análise feita pode ser chamada apenas de síntese.\nCom esta visão em mente, vamos extrar os slopes de cada um dos modelos ajustados anteriormente, bem como a estimativa do erro associado à cada um deles. Para chegarmos à variância do slope, basta então elevar o valor desse momento ao quadrado.\n\n## desempacotando os resultados das regressoes\ndf_regs &lt;- df_regs %&gt;% \n  # removendo a list column de data e a coluna com o objeto dos modelos ajustados\n  select(-data, - modelo) %&gt;% \n  # desaninhando a list column com os coeficientes das regressões\n  unnest(cols = tidyed) %&gt;% \n  # pegando apenas o slope das regressões\n  filter(term == 'year_scaled') %&gt;% \n  # dropando a coluna com a string do slope\n  select(-term) %&gt;% \n  # calculando a variância do slope, elevando o erro da estimativa ao quadrado\n  mutate(\n    variance = std.error ^ 2\n  )\nrmarkdown::paged_table(x = df_regs)\n\n\n\n  \n\n\n\nTemos tudo pronto, agora é só ajustar o modelo meta-analítica. Para isso, vou utilizar a função rma.mv do pacote metafor para ajustar um modelo meta-analítico de efeitos aleatórios. Este modelo vai combinar os tamanhos do efeito (i.e., slopes) obtidos a partir de cada regressão em um efeito global, ponderando cada observação através de sua variância4. Outro ponto importante é que como alguns circuitos contribuem com mais de uma medida, é necessário considerar esta fonte de não-independência na análise. Fazemos isso especificando o argumento random, e passando uma fórmula que especificará que os tamanhos do efeito estão agrupados através dos níveis da variável circuit. Posto de outra forma, este modelo funciona de forma muito parecida com um modelo de efeitos aleatórios5 ponderado, onde estimamos um intercepto fixo para todas as observações e um intercepto aleatório para cada circuito. Para mais informações, o help dessa função está muito bem documentada.\n\nmodelo_ma_slopes &lt;- rma.mv(yi = estimate, V = variance, random = ~ 1 | circuit, data = df_regs)\nmodelo_ma_slopes\n\n\nMultivariate Meta-Analysis Model (k = 61; method: REML)\n\nVariance Components:\n\n             estim    sqrt  nlvls  fixed   factor \nsigma^2    12.1997  3.4928     42     no  circuit \n\nTest for Heterogeneity:\nQ(df = 60) = 617.5129, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub     \n -1.8680  0.5979  -3.1245  0.0018  -3.0398  -0.6962  ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO output da análise traz algumas informações muito importantes:\n\nO valor em sigma^2 quantifica a variância entre estudos, neste caso mostrando que existe uma variabilidade substancial no slope das regressões entre circuitos;\n\nIsto também é representado através da estatística Q, que é utilizada para testar a hipótese nula de que todas as observações representam uma amostra aleatória homogênea da população de estudo (i.e., os slopes são homogêneos entre os circuitos). O teste de hipótese baseado nesta estatística segue uma distribuição do Qui-Quadrado com K-1 graus de liberdade (onde K é o número de observações) e, nesse caso, sugere que devemos rejeitar a hipótese nula, em favor da hipótese de que os slopes são de fato diferentes entre os circuitos6.\nFinalmente, temos o resultado do modelo, que nos mostra: o valor da estimativa global estimada pelo modelo (estimate), o erro associado à esta estimativa (se), os intervalos de confiança inferior e superior da estimativa (ci.lb e ci.ub, respectivamente), bem como um teste estatístico da hipótese nula de que o efeito global estimado não difere de 0 (i.e., o slope na realidade é flat). Neste caso, podemos ver que rejeitamos esta hipótese nula, o que também fica claro olhássemos só a estimativa do intervalo de confiança.\n\nMas beleza…o que esse último resultado quer nos dizer? Ele mostra que, entre todas as sequências ininterruptas de provas entre os circuitos, a evidência sugere que para cada mudança de um desvio padrão entre os anos (i.e., para cada três anos), as provas ficam cerca de 1.868 minutos mais curtas, com um intervalo de confiança de 3.04 à 0.696 minutos. Isto é, quando levamos em consideração diferenças entre os circuitos, existe evidência para dizer que as provas estão ficando mais curtas sim - mas é tão pouco que chega à dar dó: de 40 segundos à 3 minutos.\nPara fechar essa seção, também podemos ter acesso aos valores dos interceptos aleatórios estimados pelo modelo através da função ranef (i.e., a diferença circuito-específico da estimativa global do modelo). O 0 na figura abaixo representa a estimativa global do modelo, e podemos ver que alguns circuitos têm um desvio consistente para cima (i.e., circuitos nos quais o slope tende a ser mais positivo do que o estimado - círculos azuis) e outros para baixo (i.e., circuitos nos quais o slope tende a ser mais negativo do que o estimado - círculos vermelhos), mas a maioria deles não difere muito do slope global estimado (i.e., círculos vazios).\n\n\nCódigo\nranef(object = modelo_ma_slopes) %&gt;% \n  pluck('circuit') %&gt;% \n  rownames_to_column(var = 'circuit') %&gt;% \n  mutate(\n    circuit  = fct_reorder(.f = circuit, .x = intrcpt, .fun = mean),\n    efeito   = case_when(intrcpt &gt; 0 & pi.lb &gt; 0 ~ 'pos',\n                         intrcpt &lt; 0 & pi.ub &lt; 0 ~ 'neg',\n                         TRUE ~ 'none')\n  ) %&gt;% \n  ggplot(mapping = aes(y = circuit, x = intrcpt)) +\n  geom_vline(xintercept = 0, color = 'grey50') +\n  geom_errorbar(mapping = aes(xmin = pi.lb, xmax = pi.ub), \n                width = 0, size = 0.5, color = 'grey50') +\n  geom_point(mapping = aes(fill = efeito), shape = 21, size = 2.5, color = 'black') +\n  scale_fill_manual(values = c('indianred3', 'white', 'dodgerblue3')) +\n  labs(\n    title    = 'Diferença no tamanho do efeito associado à cada circuito',\n    subtitle = 'Este figura demonstra o quanto cada circuito se afasta da estimativa do tamanho de\nefeito global. Neste contexto, a linha horizontal centrada em zero representaria aquele\nefeito, e a diferença no eixo horizontal é o quanto que uma instância pertencer à cada\ncircuito o modifica.',\n    x        = 'Diferença no tamanho do efeito'\n  ) +\n  theme(\n    legend.position = 'none',\n    axis.title.y    = element_blank()\n  )\n\n\n\n\n\nCom estes resultados, já temos a nossa resposta: sim, as provas estão ficando mais curtas ao longo das temporadas…mas bem pouquinho. Mas…o quanto será que este efeito varia de acordo com a janela de tempo que estamos usando para a comparação?"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-ii",
    "href": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-ii",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Abordagem Meta-Analítica II",
    "text": "Abordagem Meta-Analítica II\nVamos mudar um pouco a abordagem agora, e tentar entender se e o quanto a duração de cada prova tem mudado ano a ano. Isto é, vamos responder à pergunta do quão diferente é o tempo de prova ‘hoje’ quando comparada à cada um dos anos anteriores: se as provas estão ficando mais curtas, então esperamos que a diferença entre o agora e o passado fique cada vez maior tanto mais distante for esse passado. Para isso, vamos mudar um pouco a forma como definimos àquelas informações sobre o passo inicial de uma meta-análise. Nesta seção temos:\n\nPopulação: definida como as provas ocorridas em um circuito. Apesar de parecer ser a mesma população da seção anterior, agora consideraremos todas as provas, não só àquelas que ocorrem em uma sequência ininterrupta de anos;\n\nIntervenção: continuaremos olhando o tempo, mas aqui ele será representado como cada um dos anos nos quais uma prova ocorreu em cada circuito (e, novamente, independentemente destas provas terem ocorrido em uma sequência ininterrupta de anos ou não);\n\nComparação: em alguns casos, queremos fazer uma comparação entre dois ou mais níveis de uma intervenção - e.g. tratamento vs controle. No nosso caso, vamos comparar a duração da prova mais recente em cada circuito com cada um dos tempos nos anos anteriores para aquele mesmo circuito.\nOutcome: como nossa ideia é comparar a duração das provas entre dois anos, vamos focar na diferença entre as médias destes tempos entre os dois anos para cada circutio.\n\nUm ponto importante aqui é que como nosso foco é fazer uma comparação, precisaremos sintetizar esta diferença de alguma forma. Neste contexto, vamos utilizar uma métrica de tamanho de efeito bastante popular em meta-análise: o log response ratio (novamente, o arquivo de ajuda da função metafor::escalc traz muita informação sobre essa métrica e suas aplicações). De forma muito breve, esta métrica é calculada através do logaritimo da razão entre as duas médias e, no nosso caso, será implementado para cada circuito x como:\n\\[\nLRR = log(\\frac{\\bar{x_i}}{\\bar{x_j}})\n\\] onde \\[\\bar{x_i}~=~média~da~duração~da~prova~no~circuito~x~no~ano~i~(i = max(i)),\\] \\[\\bar{x_j}~=~média~da~duração~da~prova~no~mesmo~circuito~x~no~ano~j~(j~&lt;~i)\\]\nDesta forma, se as provas tiverem ficado mais curtas quando comparamos o presente ao passado, então os valores desta métrica serão negativos (e o contrário quando as provas estiverem ficando mais longas). Além disso, essa métrica é de fácil interpretação, o que a torna bastante atraente também (e.g., um LRR = -0.10 representa uma redução de cerca de 10% na duração da prova do ano i quando comparado ao ano j). Como em toda meta-análise, também precisamos calcular a incerteza ao redor da estimativa do tamanho do efeito do LRR (i.e., a variância do LRR). Ele é implementado no código abaixo, onde também calculamos o LRR e extraímos algumas outras informações dos dados.\n\ndf_diffs &lt;- df %&gt;% \n  # selecionando apenas as colunas que vamos usar\n  select(raceId:circuit) %&gt;% \n  # removendo qualquer valor faltante na duração média e erro padrão associado\n  drop_na(media, erro) %&gt;% \n  # organizando a base em ordem decrescente de anos dentro de cada circuito\n  arrange(circuit, -year) %&gt;% \n  # agrupando a base por circuito\n  group_by(circuit) %&gt;% \n  mutate(\n    # calculando o log response ratio para cada observação - log(atual / ti)\n    lrr           = log(first(media) / media),\n    # calculando a estimativa geral de variância de cada observação\n    variancia     = (erro ^ 2) / (obs * (media ^ 2)), \n    # calculando a variância do log response ratio\n    lrr_var       = first(variancia) + variancia,\n    # calculando o intervalo em anos da comparação\n    intervalo     = first(year) - year,\n    # categorizando a variável em torno de intervalos de 5 anos\n    bin_intervalo = as.factor((intervalo %/% 5) * 5)\n  ) %&gt;% \n  # desagrupando o dataframe\n  ungroup %&gt;% \n  # removendo a primeira observação (_i.e._, atual vs atual)\n  filter(intervalo &gt; 0)\nrmarkdown::paged_table(x = df_diffs)\n\n\n\n  \n\n\n\nComo pode ser notado acima, a nossa estratégia foi calcular o tamanho do efeito comparando sempre a duração da prova mais recente disponível em cada circuito vs cada uma das provas ocorridas nos anos anteriores naquele mesmo circuito. Neste sentido, se a prova mais recente disponível para o circuito x tiver ocorrido no ano de 2015 e houverem outras duas provas em 2000 e 1990, então teremos uma instância com o LRR para a comparação 2015 vs 2000 e uma outra para a comparação 2015 vs 1990. Portanto, cada circuito contribui com N - 1 comparações (onde N é o número de provas para aquele circuito), e precisaremos considerar esta fonte de variação e não-independência dos dados modelo.\nOutra informação que extraímos no código foi a diferença de tempo entre o ano mais recente e cada um dos outros anos na comparação, que utilizaremos como moderador (i.e., variável preditora) no modelo. Só para lembrar, a nossa ideia principal nessa seção é averiguar se tanto mais para o passado realizarmos àquela comparação, tanto maior será a diferença observada no LRR. Desta forma, quando observamos uma comparação associada a um intervalo de 5 anos, estamos considerando todas as comparações que envolvam uma prova e àquelas ocorridas até 5 anos antes, e assim sucessivamente. Devido à natureza não-linear da relação entre o intervalo de tempo e o LRR, resolvi por discretizar o intervalo de anos em buckets de 5 anos para facilitar a análise. No entanto, deixei uma linha comentada em que é possível usar esta variável em seu formato contínuo.\nO modelo funciona de forma bastante similar à versão da seção anterior. A única diferença é que agora especificamos o argumento mods, passando o right-hand side da fórmula. Este modelo é conhecido com um modelo meta-analítico de efeitos mistos, pois temos uma variável fixa representada pelo moderador e uma variável aleatória representada pela identidade do circuito7.\n\nmodelo_ma_diffs &lt;- rma.mv(yi = lrr, V = lrr_var, \n                          # mods = ~ poly(x = intervalo, degree = 2, raw = TRUE), \n                          mods = ~ bin_intervalo,\n                          random = ~ 1 | circuit, data = df_diffs)\nmodelo_ma_diffs\n\n\nMultivariate Meta-Analysis Model (k = 946; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2    0.0316  0.1777     62     no  circuit \n\nTest for Residual Heterogeneity:\nQE(df = 931) = 8921303.0919, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:15):\nQM(df = 14) = 2991114.1249, p-val &lt; .0001\n\nModel Results:\n\n                 estimate      se        zval    pval    ci.lb    ci.ub      \nintrcpt           -0.0656  0.0226     -2.9089  0.0036  -0.1099  -0.0214   ** \nbin_intervalo5     0.0487  0.0003    190.9061  &lt;.0001   0.0482   0.0492  *** \nbin_intervalo15    0.0672  0.0003    223.8302  &lt;.0001   0.0666   0.0678  *** \nbin_intervalo20    0.0762  0.0003    249.2053  &lt;.0001   0.0756   0.0768  *** \nbin_intervalo25    0.0576  0.0004    149.2714  &lt;.0001   0.0569   0.0584  *** \nbin_intervalo30    0.0387  0.0004     89.5131  &lt;.0001   0.0378   0.0395  *** \nbin_intervalo10    0.0648  0.0003    224.3725  &lt;.0001   0.0642   0.0653  *** \nbin_intervalo35    0.0340  0.0005     67.5145  &lt;.0001   0.0330   0.0349  *** \nbin_intervalo40   -0.0550  0.0005   -103.1802  &lt;.0001  -0.0561  -0.0540  *** \nbin_intervalo45   -0.0243  0.0006    -41.4670  &lt;.0001  -0.0254  -0.0231  *** \nbin_intervalo50   -0.2137  0.0005   -445.0941  &lt;.0001  -0.2146  -0.2128  *** \nbin_intervalo55   -0.3905  0.0007   -577.1681  &lt;.0001  -0.3918  -0.3892  *** \nbin_intervalo60   -0.4647  0.0005   -883.3099  &lt;.0001  -0.4657  -0.4636  *** \nbin_intervalo65   -0.6387  0.0005  -1164.8388  &lt;.0001  -0.6398  -0.6376  *** \nbin_intervalo70   -0.5334  0.0016   -339.1343  &lt;.0001  -0.5365  -0.5303  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTemos algumas novidades quanto ao output do modelo:\n\nA estatística Q agora é particionada entre QE e QM: o primeiro é a mesma coisa que a definição da estatística Q dada na seção anterior, enquanto o segundo testa a significância estatística do modelo conforme representado pelos efeitos fixos - i.e., a significância do moderador. Neste caso, podemos ver que os tamanhos dos efeitos não representam uma amostra aleatória homogênea da população de estudo (i.e., existem diferenças entre circuitos em termos do LRR; p valor do QE é menor que 0.05) e que o moderador contribui para explicar a diferença no LRR entre os circuitos (i.e., p valor do QM é menor que 0.05);\n\nO model results traz a estimativa não só para o intercepto, mas também para todos os outros níveis do moderador que utilizamos. Todavia, o intercepto aqui representa o primeiro nível do moderador - i.e., provas ocorridas até 5 anos antes da prova mais recente na base para cada circuito. Portanto, este valor de estimate representa o LRR global para este subconjunto de provas, enquanto todos os outros estimates representam a diferença entre os níveis seguintes e este primeiro. Em outras palavras, eles nos informam se existe e qual o tamanho da diferença entre o LRR global para provas ocorridas até 5 anos antes vs provas ocorridas entre 5 à 9 anos antes, vs 10 à 14 anos antes e etc. Explicando assim, fica difícil de entender, então vamos partir para a prática.\n\n\n\nCódigo\n## construindo matriz identidade para traçar o contraste\nmatriz_contraste &lt;- diag(x = 1, nrow = length(unique(df_diffs$bin_intervalo)))\n\n## colocando 1 na primeira coluna para que todo calculo de efeito seja\n## baseado no valor do intercepto + nivel\nmatriz_contraste[, 1] &lt;- 1\n\n## calculando o efeito de cada nível\ncontrastes &lt;- anova(modelo_ma_diffs, L = matriz_contraste) \ncontrastes\n\n\n\nHypotheses:                                  \n1:                    intrcpt = 0 \n2:   intrcpt + bin_intervalo5 = 0 \n3:  intrcpt + bin_intervalo15 = 0 \n4:  intrcpt + bin_intervalo20 = 0 \n5:  intrcpt + bin_intervalo25 = 0 \n6:  intrcpt + bin_intervalo30 = 0 \n7:  intrcpt + bin_intervalo10 = 0 \n8:  intrcpt + bin_intervalo35 = 0 \n9:  intrcpt + bin_intervalo40 = 0 \n10: intrcpt + bin_intervalo45 = 0 \n11: intrcpt + bin_intervalo50 = 0 \n12: intrcpt + bin_intervalo55 = 0 \n13: intrcpt + bin_intervalo60 = 0 \n14: intrcpt + bin_intervalo65 = 0 \n15: intrcpt + bin_intervalo70 = 0 \n\nResults:\n    estimate     se     zval   pval \n1:   -0.0656 0.0226  -2.9089 0.0036 \n2:   -0.0170 0.0226  -0.7517 0.4523 \n3:    0.0016 0.0226   0.0704 0.9439 \n4:    0.0105 0.0226   0.4664 0.6410 \n5:   -0.0080 0.0226  -0.3550 0.7226 \n6:   -0.0270 0.0226  -1.1942 0.2324 \n7:   -0.0009 0.0226  -0.0386 0.9692 \n8:   -0.0317 0.0226  -1.4041 0.1603 \n9:   -0.1207 0.0226  -5.3464 &lt;.0001 \n10:  -0.0899 0.0226  -3.9835 &lt;.0001 \n11:  -0.2793 0.0226 -12.3768 &lt;.0001 \n12:  -0.4562 0.0226 -20.2064 &lt;.0001 \n13:  -0.5303 0.0226 -23.4955 &lt;.0001 \n14:  -0.7043 0.0226 -31.2040 &lt;.0001 \n15:  -0.5990 0.0226 -26.4830 &lt;.0001 \n\nOmnibus Test of Hypotheses:\nQM(df = 15) = 2991119.6531, p-val &lt; .0001\n\n\nA matriz de contraste acima traz alguns resultados muito interessantes e importantes para fecharmos essa estória. Ela mostra que se compararmos as provas mais recentes em cada circuito com àquelas ocorridas:\n\naté 4 anos antes naquele mesmo circuito (intrcpt), existe uma tendência das provas serem ~6% mais curtas;\n\nentre 35 à 70 anos atrás (intrcpt + bin_intervalo35 à intrcpt + bin_intervalo70, respectivamente), também podemos observar que as provas também estão mais curtas. Todavia, essa tendência varia bastante ao longo daquele intervalo: comparadas a ele, as provas estão entre ~3% (e.g., vs 30 anos atrás) à ~70% mais curtas (e.g., vs 65 anos através); e,\n\nentre 5 e 34 anos atrás (intrcpt + bin_intervalo5 à intrcpt + bin_intervalo30, respectivamente), não existe evidência de que as provas estejam ficando mais curtas. Podemos tirar essa conclusão principalmente pelo fato do intervalo de confiança da estimativa (i.e., estimate \\(\\pm\\) 1.96 \\(\\times\\) se) cruzar o valor de 0.\n\nO que estes resultados mostram é que é muito claro que as provas estão mais curtas hoje do que há 4 décadas atrás. Todavia, embora as provas estejam ligeiramente mais curtas quando olhamos o seu passado muito recente (até 4 anos), elas ainda sim têm durações semelhantes àquelas de 1 à 3 décadas atrás. Se, assim como eu, você prefere entender estes resultados visualmente, basta olhar o gráfico abaixo. Um tanto curioso este padrão, não?\n\n\nCódigo\ncontrastes %&gt;% \n  unclass %&gt;% \n  keep(names(.) %in% c('hyp', 'Xb', 'se', 'pval')) %&gt;% \n  map(.f = as.data.frame) %&gt;% \n  bind_cols() %&gt;% \n  as_tibble() %&gt;% \n  set_names(nm = c('intervalo', 'estimate', 'se', 'pval')) %&gt;% \n  mutate(\n    intervalo   = str_extract(string = intervalo, pattern = '(?&lt;=bin_intervalo)[0-9]{1,2}'),\n    intervalo   = as.numeric(intervalo),\n    significant = pval &lt;= 0.05\n  ) %&gt;% \n  replace_na(replace = list(intervalo = 0)) %&gt;% \n  ggplot(mapping = aes(x = intervalo, y = estimate)) +\n  geom_hline(yintercept = 0, color = 'grey40', linetype = 2) +\n  geom_errorbar(mapping = aes(ymin = estimate - 1.96 * se, ymax = estimate + 1.96 * se),\n                width = 1) +\n  geom_point(mapping = aes(fill = significant), shape = 21, size = 2, color = 'black') +\n  scale_x_continuous(breaks = seq(from = 0, to = 70, by = 5)) +\n  scale_y_continuous(breaks = seq(from = -1, to = 0.2, by = 0.2), limits = c(-0.9, 0.2)) +\n  scale_fill_manual(values = c('grey70', 'white')) +\n  labs(\n    title    = 'Diferença entre o tempo de prova mais recente e os anteriores',\n    subtitle = 'As provas ficaram cerca de 10% à 60% mais curtas quando comparadas àquelas mesmas provas quando ocorriam 50\nanos atrás. As provas ficam um pouco mais rápidas quando comparadas ao histórico mais recente (até 5 anos antes),\nmas a duração não muda muito quando comparada à 10 ou 35 anos atrás.',\n    x        = 'Anos de Diferença',\n    y        = 'Log Response Ratio'\n  ) +\n  theme(\n    legend.position = 'none',\n    panel.grid      = element_blank(),\n    axis.line       = element_line()\n  )"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "",
    "text": "Há algum tempo atrás eu explorei o caminho para raspar os dados do ranking do BoardGameGeek1, e consolidei o passo-a-passo nesse post e script. Meu principal interesse naquele momento era que eu precisava obter o código numérico identificador de cada título a fim de poder usar esse valor quando fosse interagir com a API XML do BGG. Como o único lugar em que encontrei essa informação foi no hyperlink para a página de cada título na tabela do ranking, resolvi criar aquele scrapper.\nUma outra fonte de informação sobre jogos de tabuleiro é o site brasileiro da Ludopedia. Este portal tem muita coisa em comum com o BGG, inclusive uma API e uma página de ranking. Todavia, diferente do equivalente gringo, a Ludopedia oferece (1) uma REST API e (2) um meio mais fácil de obter o código identificador de cada título a partir da própria API. De toda forma, no momento em que escrevo este post, ainda não é possível obter as informações da página do ranking diretamente pela API. Desta forma, aqui também existe a possibilidade de exercitar um pouco o web scrapping para a extração dessa informação.\nVou aproveitar esta oportunidade para continuar construindo uma trilha a partir da qual construiremos uma base de dados que nos permitirá responder muitas outras perguntas interessantes, e aplicar técnicas bastante legais de Machine Learning. Falo mais sobre essas idéias ao final desse post."
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#footnotes",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#footnotes",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nBGG daqui para a frente.↩︎\nEssa paginação não estará evidente na primeira vez que você visitar essa página. Entretanto, se você avançar para a próxima página e depois voltar, verá que ela aparecerá na url.↩︎\nEssa informação estava dentro de um atributo chamado Last Page em uma tag div, tornando a extração da informação bem fácil.↩︎\nHavíamos raspado apenas 5 páginas do ranking do BGG, mas cada página contém informações sobre 100 jogos. Portanto, dado que cada página do ranking da Ludopedia contém as informações de 50 jogos, tivemos que raspar 10 páginas.↩︎"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#identificar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#identificar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Identificar",
    "text": "Identificar\nA primeira coisa aqui é navegar até a página-alvo e entender como funciona a sua paginação e onde está o conteúdo que queremos raspar. A figura abaixo mostra um print da primeira página do ranking, onde podemos ver a url que precisaremos visitar bem como constatar que a paginação funciona incrementando a contagem da página (i.e., pagina=1, pagina=2,…)2.\nOutro ponto importante é que as informação que queremos parecem estar em uma tabela, como foi no caso do BGG. Além disso, cada página contém 50 jogos ordenados de forma sequencial de acordo com a sua posição no ranking.\n\n\nCódigo\ninclude_graphics(path = 'images/imagem_1.jpg')"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#navegar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#navegar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Navegar",
    "text": "Navegar\nO próximo passo é olhar o fluxo de informação da página a partir da aba Network, acessível através da ferramenta Inspecionar do navegador. Podemos ver que o conteúdo que queremos raspar não é produzido a partir de nenhuma API nem nada parecido, mas totalmente disponível a partir do código HTML mesmo. Além disso, podemos ver que o conteúdo não está organizado dentro de tags de tabela em HTML, mas sim dentro de várias tags div associadas à classe pad-top. Isto já torna o parser deste scrapper diferente daquele do BGG, onde foi bastante simples tabular as informações a partir do código HTML.\n\n\nCódigo\ninclude_graphics(path = 'images/imagem_2.jpg')"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#replicar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#replicar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Replicar",
    "text": "Replicar\nVamos então tentar fazer um request da primeira página do ranking e ver o que conseguimos. Isso é feito de forma bem simples, passando apenas a url base para acessar a página e deixando o valor correspondente à página como algo a ser determinado separadamente. Faremos isso usando a função GET do pacote httr.\n\n\nCódigo\n## url base do ranking\nbase_url &lt;- 'https://www.ludopedia.com.br/ranking?pagina='\n\n# fazendo o GET\nresultado &lt;- GET(url = str_glue(base_url, 1))\nresultado\n\n\nResponse [https://ludopedia.com.br/ranking?pagina=1]\n  Date: 2024-01-20 03:00\n  Status: 200\n  Content-Type: text/html; charset=UTF-8\n  Size: 162 kB\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml...\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"\n      xmlns:og=\"http://ogp.me/ns#\"\n      xmlns:fb=\"https://www.facebook.com/2008/fbml\" \n      lang=\"pt-BR\"\n&gt;\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximu...\n    \n...\n\n\nApesar da forma como o conteúdo está disponível nesta página ser diferente daquele do BGG, o request em si parace também ser bem simples!"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#parsear",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#parsear",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Parsear",
    "text": "Parsear\nComo vimos anteriormente, as informações que queremos não estão formatadas e organizadas dentro de tags de tabela em HTML. Portanto, precisaremos identificar e parsear cada uma das informações que queremos usando os respectivos xpath. Para começar, podemos ver que temos acesso ao hyperlink que leva à imagem da capa do jogo se extrairmos o atributo src a partir da classe img-capa dentro da tag img. Isto pode ser uma informação legal se, depois, e.g. quisermos plotar essa imagem como uma célula em uma tabela do reactable.\n\n\nCódigo\nresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando a imagem da capa\n  xml_find_all(xpath = '//img[@class=\"img-capa\"]') %&gt;% \n  # pegando o url\n  xml_attr(attr = 'src') %&gt;% \n  # pegando a primeira observação\n  head(1) %&gt;% \n  # plotando a imagem de uma capa\n  magick::image_read() %&gt;% \n  # aumentando a resolução da imagem\n  magick::image_scale(geometry = '300')\n\n\n\n\n\nOutra informação legal de buscar é o hyperlink para a página de cada jogo no domínio da Ludopedia. Esta informação está dentro da tag que contém o nome do título (i.e., classe media-heading dentro do header h4), e pode ser obtida extraindo o atributo href de dentro da tag a. Como já conheço a API REST da Ludopedia, sei que essa informação pode ser útil para e.g. raspar o campo de descrição completa do jogo, a fim de utilizar esse texto em alguma análise.\n\n\nCódigo\nresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando todos os links\n  xml_find_all(xpath = 'a') %&gt;% \n  # extraindo o atributo dos hiperlinks\n  xml_attr(attr = 'href') %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n\n[1] \"https://ludopedia.com.br/jogo/brass-birmingham\" \n[2] \"https://ludopedia.com.br/jogo/gaia-project\"     \n[3] \"https://ludopedia.com.br/jogo/terraforming-mars\"\n[4] \"https://ludopedia.com.br/jogo/terra-mystica\"    \n[5] \"https://ludopedia.com.br/jogo/gloomhaven\"       \n[6] \"https://ludopedia.com.br/jogo/brass-lancashire\" \n\n\nA posição do ranking também pode ser extraída a partir da classe media-heading dentro do header h4, olhando a classe rank dentro da tag span…\n\n\nCódigo\nresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o ranking\n  xml_find_all(xpath = 'span[@class=\"rank\"]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n\n[1] \"1º\" \"2º\" \"3º\" \"4º\" \"5º\" \"6º\"\n\n\n…enquanto o nome do jogo pode ser extraído a partir do atributo title dentro da tag a…\n\n\nCódigo\nresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o nome do jogo\n  xml_find_all(xpath = 'a[@title]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n\n[1] \"Brass: Birmingham\" \"Projeto Gaia\"      \"Terraforming Mars\"\n[4] \"Terra Mystica\"     \"Gloomhaven\"        \"Brass: Lancashire\"\n\n\n…o ano de lançamento de cada título vêm do atributo small…\n\n\nCódigo\nresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o ano de lançamento do jogo\n  xml_find_all(xpath = 'small') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n\n[1] \" (2018)\" \" (2017)\" \" (2016)\" \" (2012)\" \" (2017)\" \" (2017)\"\n\n\n…enquanto, finalmente, todas as informações relacionadas às notas podem ser extraídas a partir da classe rank-info dentro da tag div.\n\n\nCódigo\nresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando as notas do jogo\n  xml_find_all(xpath = '//div[@class=\"rank-info\"]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head() %&gt;% \n  # tirando um pouco o excesso de whitespace\n  str_squish()\n\n\n[1] \"Nota Rank: 9.04 | Média: 9.15 | Notas: 1116 | Sua Nota: -\"\n[2] \"Nota Rank: 9.03 | Média: 9.12 | Notas: 1248 | Sua Nota: -\"\n[3] \"Nota Rank: 9.00 | Média: 9.04 | Notas: 3131 | Sua Nota: -\"\n[4] \"Nota Rank: 8.99 | Média: 9.04 | Notas: 2556 | Sua Nota: -\"\n[5] \"Nota Rank: 8.98 | Média: 9.09 | Notas: 1066 | Sua Nota: -\"\n[6] \"Nota Rank: 8.97 | Média: 9.16 | Notas: 616 | Sua Nota: -\" \n\n\nCom isso, temos um sashimi de parsers para pegar todas as informações que queremos a partir da página do ranking. Vamos agora consolidar esse entendimento e validá-lo na segunda página."
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#validar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#validar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Validar",
    "text": "Validar\nPara auxiliar na tarefa de raspar e parsear a segunda página do ranking, vamos definir duas funções abaixo - uma para cada tarefa. A função pega_pagina recebe a url base do ranking e o número da página que queremos raspar, fazendo então o request da página e salvando o HTML resultante em disco, no diretório definido pelo argumento path. A outra função, parser_pagina, recebe como único argumento o path para o arquivo HTML que a função pega_pagina salvou, e faz o que o próprio nome da função já diz. Ela está bem verbosa, mas o objetivo é mesmo deixar claro o que estamos fazendo.\n\n\nCódigo\n# função para fazer o GET\npega_pagina &lt;- function(url_base, pagina, save_dir) {\n  ## junta a base url com o numero da pagina e salva no diretorio alvo\n  GET(url = str_glue(url_base, pagina), \n      write_disk(path = sprintf(fmt = '%s/pagina_%03d.html', save_dir, pagina), \n                 overwrite = TRUE)\n  )\n  \n  # esperanando antes de prosseguir\n  Sys.sleep(runif(n = 1, min = 1, max = 5))\n}\n\n# função para parsear uma pagina\nparser_pagina &lt;- function(path_to_html){\n  \n  ## lendo a pagina raspada\n  pagina_raspada &lt;- read_html(x = path_to_html)\n  \n  ## infos do heading\n  media_head &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//h4[@class=\"media-heading\"]')\n  \n  ## link para a imagem da capa\n  links_da_capa &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//img[@class=\"img-capa\"]') %&gt;% \n    xml_attr(attr = 'src')\n  \n  ## link para a pagina do jogo\n  link_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'a') %&gt;% \n    xml_attr(attr = 'href')\n  \n  ## posicao do ranking de cada titulo\n  posicao_ranking &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'span[@class=\"rank\"]') %&gt;% \n    xml_text()\n  \n  ## nome do jogo\n  titulo_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'a[@title]') %&gt;% \n    xml_text()\n  \n  ## ano de lancamento do jogo\n  ano_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'small') %&gt;% \n    xml_text()\n  \n  ## informacoes gerais das notas\n  notas_jogo &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//div[@class=\"rank-info\"]') %&gt;% \n    xml_text()\n  \n  ## colocando rsultados numa tibble\n  tibble(\n    ranking   = posicao_ranking, \n    titulo    = titulo_jogo, \n    ano       = ano_jogo, # \n    notas     = notas_jogo,\n    link_capa = links_da_capa,\n    link_jogo = link_jogo\n  )\n}\n\n\nCom as funções definidas, agora é hora de utilizá-las! Primeiro, vamos pegar a segunda página e salvá-la em disco…\n\n\nCódigo\n# criando uma pasta para colocar os arquivos caso ela nao exista\nif(!dir_exists(path = 'temp/')){\n  dir_create(path = 'temp/')\n}\n\n# pegando a segunda pagina do ranking\npega_pagina(url_base = base_url, pagina = 2, save_dir = 'temp/')\n\n# checando para ver se o html foi baixado\ndir_ls(path = 'temp/', regexp = '.html')\n\n\ntemp/pagina_002.html\n\n\n…agora vamos parsear a página a partir do arquivo salvo em disco.\n\n\nCódigo\nparser_pagina(path_to_html = dir_ls(path = 'temp/', regexp = '.html'))\n\n\n# A tibble: 50 × 6\n   ranking titulo                                ano   notas link_capa link_jogo\n   &lt;chr&gt;   &lt;chr&gt;                                 &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 51º     Ticket to Ride: Europa                \" (2… \"\\r\\… https://… https://…\n 2 52º     Ark Nova                              \" (2… \"\\r\\… https://… https://…\n 3 53º     As Viagens de Marco Polo              \" (2… \"\\r\\… https://… https://…\n 4 54º     Ticket to Ride: Europa - 15 Anos      \" (2… \"\\r\\… https://… https://…\n 5 55º     Robinson Crusoé: Aventuras na Ilha A… \" (2… \"\\r\\… https://… https://…\n 6 56º     On Mars                               \" (2… \"\\r\\… https://… https://…\n 7 57º     Tiranos da Umbreterna                 \" (2… \"\\r\\… https://… https://…\n 8 58º     Viticulture - Edição Essencial        \" (2… \"\\r\\… https://… https://…\n 9 59º     Mombasa                               \" (2… \"\\r\\… https://… https://…\n10 60º     Lords of Waterdeep                    \" (2… \"\\r\\… https://… https://…\n# ℹ 40 more rows\n\n\nParece que está tudo ok!"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#iterar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#iterar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Iterar",
    "text": "Iterar\nA ideia agora seria repetir o processo acima, da página 1 até a última página disponível no ranking. Lá no post sobre a raspagem do ranking do BGG vimos que poderíamos descobrir qual o número da última página a partir do próprio código HTML que era raspado3. Faremos algo bem parecido aqui, embora a informação que buscamos não esteja disponível de forma tão clara. Se inspecionarmos o código HTML da página, podemos ver que é possível extrair o número da última página através da url que está em um dos atributos da classe pagination de uma tag ul.\n\n\nCódigo\ninclude_graphics(path = 'images/imagem_3.jpg')\n\n\n\n\n\nPara facilitar nosso trabalho de extração dessa informação aqui, vamos criar e usar a função pega_max_paginas: ela vai olhar dentro daquela classe e extrair o href do atributo title da tag a; a partir daí vamos ter que usar um pouquinho de regex para extrair o número da página em si, uma vez que o resultado original é uma string, e o que desejamos são os números que estão após o padrão pagina=.\n\n\nCódigo\n# função para definir o número máximo de páginas para raspar\npega_max_paginas &lt;- function(url_base) {\n  GET(url = str_glue(url_base, 1)) %&gt;% \n    # pegando o conteudo do GET\n    content() %&gt;% \n    # pegando o xpath da paginacao\n    xml_find_all(xpath = '//ul[@class=\"pagination\"]//a[@title=\"Última Página\"]') %&gt;% \n    # pegando o link que contem o numero da pagina maxima\n    xml_attr('href') %&gt;% \n    # pegando o numero da pagina\n    str_extract(pattern = '(?&lt;=pagina=)([0-9]+)') %&gt;% \n    # parseando para numero\n    parse_number()\n}\n\n## definindo qual o numero maximo de paginas para pegar\nultima_pagina &lt;- pega_max_paginas(url_base = base_url)\nultima_pagina\n\n\n[1] 74\n\n\nComo vimos, temos 74 para raspar, o que pode demorar um pouquinho. No entanto, como a ideia aqui é ser apenas ilustrativo, vou raspar apenas as 10 primeiras páginas e deixarei uma linha comentada com o que deveria ser passado para a função walk caso quiséssemos tudo.\n\n\nCódigo\n## pegando as paginas\nwalk(\n  .x = 1:10,\n  # .x = 1:ultima_pagina, # descomentar essa linha se for para raspar tudo\n  .f = pega_pagina,\n  url_base = base_url, save_dir = 'temp/'\n)"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#faxinar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#faxinar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Faxinar",
    "text": "Faxinar\nCom o HTML das páginas, agora devemos organizar e tratar os dados. Para tal, vou extrair o path de todos os arquivos HTML baixados e passá-los para a função map_dfr. Esta função vai se encarregar de aplicar a função parser_pagina ao arquivo HTML associado à cada path e retornar um único tibble com todos os resultados parseados.\n\n\nCódigo\n## pegando o path para as paginas\npath_das_paginas &lt;- dir_ls(path = 'temp/', regexp = 'html')\n\n## colocando todas as tabelas em um dataframe so\ndf &lt;- map_dfr(.x = path_das_paginas, .f = parser_pagina)\nrmarkdown::paged_table(x = df)\n\n\n\n\n  \n\n\n\nJá temos os dados tabulados. Vamos aplicar alguns ajustes a eles: remover o excesso de espaço em branco nas strings, separar as informações sobre as notas em diversas colunas e passar o que for numérico para tal. O código abaixo dá conta disso e nos retorna os dados do ranking tratados.\n\n\nCódigo\ndf &lt;- df %&gt;% \n  mutate(\n    # parseando o ranking para numerico\n    ranking = parse_number(ranking),\n    # tratando o string titulo do jogo\n    titulo  = str_squish(string = titulo),\n    # parseando o ano para numerico\n    ano     = parse_number(ano),\n    # ajustando a string do campo de nota\n    notas   = str_squish(string = notas),\n  ) %&gt;% \n  # separando a coluna com as informacoes de nota atraves do padrao da barra\n  separate(col = notas, into = c('nota_rank', 'nota_media', 'notas', 'leftover'), sep = '\\\\|') %&gt;% \n  # tratando as informacoes da coluna separada\n  mutate(\n    # nota do ranking\n    nota_rank  = parse_number(nota_rank),\n    # nota dos usuarios\n    nota_media = parse_number(nota_media),\n    # quantidade de notas\n    notas      = parse_number(notas) \n  ) %&gt;% \n  # removendo colunas que nao serao mais necessarias\n  select(-leftover)\nrmarkdown::paged_table(x = df)\n\n\n\n\n  \n\n\n\nPara concluir, vamos criar uma figura para verificar a relação entre as notas do ranking da Ludopedia, a nota média dada pelos usuários e a quantidade de votos para cada jogo. Essa figura é bastante similar àquela que havíamos criado para o BGG e, inclusive, raspamos 10 páginas neste exemplo aqui justamente para colocar as duas figuras em pé de igualdade4. Apesar desta pequena diferença entre os dois portais, podemos ver padrões similares aqueles já vistos no ranking do BGG:\n\na nota média do jogo de acordo com os usuários parece ser maior do que àquelas do ranking final da Ludopedia;\n\nparece existir uma tendência aos jogos que recebem mais votos também terem maiores notas no ranking; e,\n\nParece que os jogos mais votados são aqueles com menores notas dadas pelos usuários.\n\n\n\nCódigo\ndf %&gt;% \n  # renomeando as colunas para ficar mais parecido com o plot que fizemos para o BGG\n  rename(nota_ludopedia = nota_rank, nota_usuarios = nota_media, votos = notas) %&gt;% \n  # criando a figura\n  ggplot() +\n  geom_autopoint(alpha = 0.7, shape = 21, fill = 'tomato') +\n  geom_autodensity(mapping = aes(x = .panel_x, y = .panel_y), \n                   fill = 'tomato', color = 'black', alpha = 0.7) +\n  facet_matrix(rows = vars(nota_ludopedia:votos), layer.diag = 2)\n\n\n\n\n\nNão sei qual era a sua expectativa, mas me surpreende o fato dos padrões serem tão parecidos entre os dois portais dado a diferença que acredito existir entre os públicos brasileiros e estrangeiros."
  }
]