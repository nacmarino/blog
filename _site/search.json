[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Currilum vitae",
    "section": "",
    "text": "nac.marino@gmail.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Acredito que tudo pode ser muito interessante se você for fundo o suficiente para descobrir, souber fazer as perguntas certas e buscar nos dados as suas respostas. É essa curiosidade e paixão por dados que me move à buscar uma melhor compreensão de um problema para propor soluções que possam melhorar a nossa qualidade de vida e levar à decisões melhor informadas.\nEncontrei na ciência um meio de realizar esse propósito, tanto quando estive na academia quanto atualmente no mundo corporativo. Nessa jornada, tenho tido a oportunidade de colocar em prática tudo o que eu aprendi até aqui, além de absorver toda nova experiência que a evolução de carreira nos traz. É claro que nem sempre conseguimos colocar em prática tudo aquilo que aprendemos, mas isso não quer dizer que não devamos tentar de alguma forma. Esse blog é uma das formas de atender ao meu propósito pessoal além de ajudar a consolidar o aprendizado contínuo.\nOs posts que você encontrará aqui são principalmente motivados por alguma curiosidade minha, necessidade de endereçar um problema que tenho ou simplesmente manifestar um ponto de vista sobre a atuação no mundo de dados. Todas essas obras são de minha propriedade intelectual, não tendo relação com demandas que surgem no trabalho, além de não representar necessariamente os pontos de vista de qualquer empregador ao qual esteja associado (muito embora, caso representem, acredito que seja um evento fortuito).\n\n\n De volta ao topo"
  },
  {
    "objectID": "backlog.html",
    "href": "backlog.html",
    "title": "Backlog",
    "section": "",
    "text": "Web Scraping\n\nAnúncios do ZapImóveis;\n\nOpiniões dos funcionários do Glassdoor;\nColaboradores da empresa no LinkedIn;\nPerfil do LinkedIn;\nPostagens do LinkedIn.\n\n\n\nShinyApp\n\nRecomendação de jogos de tabuleiro;\n\n\n\nAnálises\n\nState of Data 2023;\n\n\n\nEnsaios\n\nReview sobre os cursos curtos em Gen AI do DeepLearning.ai;\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Como eu começaria a estudar GenAI hoje?\n\n\nEssa é uma pergunta atualmente relevante, mas cuja resposta pode não ser tão óbvia - ela pode depender do seu propósito, quão fundo você quer ir, quanto tempo você tem e muitos outros aspectos. Nesse post eu trago uma perspectiva sobre essa questão, focando na recomendação de alguns conteúdos que podem ajudar nessa jornada.\n\n\n\n\nlearning\n\n\ndataviz\n\n\ngenai\n\n\npov\n\n\n \n\n\n\n\n24 de jun. de 2024\n\n\n32 minutos\n\n\n\n\n\n\n  \n\n\n\n\nTudo começa de algum lugar - até mesmo a sua solução analítica\n\n\nA definição de um ponto de partida, um baseline, é vital no desenvolvimento de qualquer tipo de solução analítica. E, frequentemente, eu me pego falando sobre esse tema com pessoas cientistas de dados de todos os níveis de senioridade. Então, resolvi escrever esse post para compartilhar com um fórum maior o que eu normalmente ficou tagarelando com eles.\n\n\n\n\nmachine learning\n\n\nestatistica\n\n\npov\n\n\nr\n\n\n \n\n\n\n\n12 de abr. de 2024\n\n\n53 minutos\n\n\n\n\n\n\n  \n\n\n\n\nLá e de volta outra vez\n\n\nDepois de um longo período de hiato, coloquei o blog de volta no ar e quero voltar à escrever. Esse post fala um pouco mais sobre esse retorno, o aprendizado envolvido no processo e meus novos objetivos.\n\n\n\n\nnotas\n\n\n \n\n\n\n\n2 de mar. de 2024\n\n\n6 minutos\n\n\n\n\n\n\n  \n\n\n\n\nQual a diferença entre júnior, pleno e sênior?\n\n\nEssa pergunta vive circulando nas rodas de conversa das pessoas dentro da área de dados e, apesar de existir muito material sobre o assunto, ainda não conseguimos olhar para nenhum tipo de dado para responder à essa pergunta. Neste post vou usar os resultados da pesquisa State of Data Brasil 2021 para tentar preencher esse gap, aproveitando para trazer alguns insights sobre o que o mercado de trabalho brasileiro está praticando.\n\n\n\n\nanalise\n\n\nmachine learning\n\n\nr\n\n\ndataviz\n\n\ncompeticao\n\n\n \n\n\n\n\n24 de jun. de 2022\n\n\n100 minutos\n\n\n\n\n\n\n  \n\n\n\n\nInteragindo com a API da Ludopedia\n\n\nNeste post eu mostro como interagir com a API da Ludopedia para adquirir os dados dos jogos de tabuleiro disponíveis nesse portal. Veremos que o processo é diferente daquele que usamos para explorar a API do BoardGameGeek, apesar das informações obtidas serem bastante similares. Este post servirá de referência para outras ideias que vamos desenvolver no futuro.\n\n\n\n\nweb scraping\n\n\nboardgames\n\n\nr\n\n\n \n\n\n\n\n25 de mar. de 2022\n\n\n23 minutos\n\n\n\n\n\n\n  \n\n\n\n\nComo encontrar as cartas de Gwent mais similares entre si?\n\n\nJá olhamos os decks de Gwent para analisar os padrões de co-ocorrência entre as cartas, que utilizou as estratégias já conhecidas pela comunidade. Neste post vamos tomar outra abordagem, empregando uma análise voltada aos padrões de similaridade do texto de descrição das cartas para identificar pares que poderiam gerar estratégias potencialmente viáveis.\n\n\n\n\nboardgames\n\n\ntopic models\n\n\nanalise\n\n\nr\n\n\n \n\n\n\n\n28 de fev. de 2022\n\n\n81 minutos\n\n\n\n\n\n\n  \n\n\n\n\nInteragindo com a API XML do BoardGameGeek\n\n\nNeste post eu mostro como obter e fazer os parser dos dados dos jogos de tabuleiro do BoardGameGeek, obtidos através de sua API XML. O processo apresentado aqui faz uso e complementa o que já foi apresentado no scrapper do ranking do BGG, e servirá de base para alguns posts que penso em escrever no futuro.\n\n\n\n\nweb scraping\n\n\nboardgames\n\n\nr\n\n\n \n\n\n\n\n23 de jan. de 2022\n\n\n36 minutos\n\n\n\n\n\n\n  \n\n\n\n\nQuais as associações entre as cartas de Gwent nos decks existentes?\n\n\nEu tenho jogado Gwent: the Witcher Card Game há algum tempo, e é impressionante a quantidade de combos e sinergias que podem haver entre as cartas de acordo com o deck que você monta. Neste post, eu tento identificar as combinações de cartas que aparecem com maior frequência através de uma análise das regras de associação entre elas.\n\n\n\n\nboardgames\n\n\narules\n\n\nanalise\n\n\nr\n\n\n \n\n\n\n\n8 de jan. de 2022\n\n\n78 minutos\n\n\n\n\n\n\n  \n\n\n\n\nConvertendo coordenadas através da calculadora geográfica do INPE\n\n\nNesse post eu mostro a solução que propus para resolver um problema: converter coordenadas de uma projeção e datum qualquer para SIRGAS2000. Como não encontrei um bom suporte para a conversão no R, tive que recorrer à calculadora geográfica do INPE, criando uma automação para interagir com ela e realizar esta tarefa.\n\n\n\n\nweb scraping\n\n\nselenium\n\n\npython\n\n\n \n\n\n\n\n23 de dez. de 2021\n\n\n18 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a biblioteca de decks de Gwent\n\n\nGwent é um jogo de cartas que nasceu dentro do universo de The Witcher e, dada a popularidade da franquia, chegou aos smartphones. A comunidade de jogadores é bastante ativa, e existe uma biblioteca de decks contribuídos que está disponível dentro do site oficial do jogo. Meu objetivo neste post será obter os dados desta biblioteca e de seus decks. Isto servirá para montar uma base de dados para fazermos outras análises posteriormente.\n\n\n\n\nweb scraping\n\n\nr\n\n\nboardgames\n\n\n \n\n\n\n\n30 de nov. de 2021\n\n\n32 minutos\n\n\n\n\n\n\n  \n\n\n\n\nPrevisão de acidentes com os dados da Fórmula 1\n\n\nHá algum tempo atrás escrevi um post para tentar entender se e de que forma os tempos de conclusão das provas de Fórmulas 1 vêm evoluindo ao longo das temporadas. Neste post eu mudo o foco, e tento entender e determinar a probabilidade de ocorrência de acidentes nas provas da Fórmula 1.\n\n\n\n\nweb scraping\n\n\nr\n\n\nmachine learning\n\n\nanalise\n\n\n \n\n\n\n\n15 de nov. de 2021\n\n\n59 minutos\n\n\n\n\n\n\n  \n\n\n\n\nQuão similares são as notas dos jogos de tabuleiro entre os portais especializados?\n\n\nMeu principal objetivo neste post é analisar as notas dadas aos jogos de tabuleiros nos rankings do portal da Ludopedia e do portal do BoardGameGeek para determinar quão similares são as notas dadas aos títulos nas mesmas posições entre os dois rankings. Isto é, será que a nota dada ao título na i-ésima posição no ranking da Ludopedia é parecida com a nota dada ao título na mesma posição no ranking do BoardGameGeek?\n\n\n\n\nestatistica\n\n\nboardgames\n\n\nanalise\n\n\nr\n\n\n \n\n\n\n\n12 de nov. de 2021\n\n\n44 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a Página do Ranking da Ludopedia\n\n\nEu já havia raspado a página do ranking do portal do BoardGameGeek, e agora eu vou repetir a tarefa focando no ranking do portal da Ludopedia. Meu objetivo com isso é criar a base para que, mais tarde, possamos fazer análises comparando os jogos entre os dois portais.\n\n\n\n\nweb scraping\n\n\nboardgames\n\n\nr\n\n\n \n\n\n\n\n24 de out. de 2021\n\n\n17 minutos\n\n\n\n\n\n\n  \n\n\n\n\nEntendendo os Padrões de Duração das Provas da Fórmula 1\n\n\nA Fórmula 1 é um dos esportes de velocidade mais famosos do mundo, com provas ocorrendo desde o início da década de 50 até os dias de hoje. Muita coisa mudou nestes 70 anos, especialmente os carros: cada vez mais bonitos, mais seguros e mais rápidos. Mas será que isso também se traduziu em provas cada vez mais curtas também? Neste post eu examino de que forma a duração das provas da Fórmula 1 têm evoluído ao longo das temporadas.\n\n\n\n\nestatistica\n\n\nanalise\n\n\nr\n\n\n \n\n\n\n\n8 de out. de 2021\n\n\n47 minutos\n\n\n\n\n\n\n  \n\n\n\n\nRaspando a página do ranking do BoardGameGeek\n\n\nNeste post eu faço a raspagem da tabela do ranking dos jogos de tabuleiro do BoardGameGeek. Essa tarefa foi necessária para que eu conseguisse interagir da melhor forma possível com a API XML que o site oferece.\n\n\n\n\nweb scraping\n\n\nr\n\n\nboardgames\n\n\n \n\n\n\n\n17 de set. de 2021\n\n\n18 minutos\n\n\n\n\n\n\n  \n\n\n\n\nPrevendo o preço de apartamentos em Niterói/RJ\n\n\nA previsão de preços de imóveis é uma tarefa muito comum em ciência de dados, existingo até Hello World para esta prática - o Ames Housing, com informações sobre o preço e outros metadados de imóveis na cidade de Ames em Iowa. Neste post, sigo esta idéia e utilizo um conjunto de dados reais sobre os apartamentos disponíveis para a venda no município de Niterói/RJ. Buscarei entender e prever a variação no preço destes imóveis de acordo com as informações contidas nos anúncios com a ajuda de um modelo de Machine Learning.\n\n\n\n\nmachine learning\n\n\nanalise\n\n\ncompeticao\n\n\nr\n\n\n \n\n\n\n\n27 de ago. de 2021\n\n\n52 minutos\n\n\n\n\n\n\nNenhum item correspondente\n\n De volta ao topo"
  },
  {
    "objectID": "posts/2021-08-27_niteroi-housing-prices/index.html",
    "href": "posts/2021-08-27_niteroi-housing-prices/index.html",
    "title": "Prevendo o preço de apartamentos em Niterói/RJ",
    "section": "",
    "text": "Nota\n\n\n\nEste post foi o meu trabalho de conclusão do curso Relatórios e visualização de dados que fiz em julho de 2021 na Curso-R Curso-R, que foi escolhido como um dos três melhores trabalhos da turma."
  },
  {
    "objectID": "posts/2021-08-27_niteroi-housing-prices/index.html#footnotes",
    "href": "posts/2021-08-27_niteroi-housing-prices/index.html#footnotes",
    "title": "Prevendo o preço de apartamentos em Niterói/RJ",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nhttps://pt.wikipedia.org/wiki/Niter%C3%B3i↩︎\nShapefile obtido a partir de https://geo.niteroi.rj.gov.br/civitasgeoportal/↩︎\nNão mostro aqui mas, de fato, a relação entre muitas delas não é linear.↩︎\nDei uma lida no texto de descrição dos anúncios e parece que todos os casos que narrei aqui são erros de imputação de informação mesmo. Assim, acredito que esta solução não seja um problema.↩︎\nSe você for uma pessoa curiosa, eu acabei imputando um NA a 583 linhas da base↩︎\nNão existe uma função no fastshap para criar essa figura, então tentei emular uma figura similar que existente dentro do pacote shap.↩︎"
  },
  {
    "objectID": "posts/2021-09-17_scrapper-boardgamegeek/index.html",
    "href": "posts/2021-09-17_scrapper-boardgamegeek/index.html",
    "title": "Raspando a página do ranking do BoardGameGeek",
    "section": "",
    "text": "Sempre joguei os jogos de tabuleiro mais tradicionais, como Banco Imobiliário, Scotland Yard e War. Esses são jogos muito populares, apesar de cada partida ser muito repetitiva e eles demandarem uma quantidade razoável de jogadores para que tenham graça - e, no meio de uma pandemia, se já acabava sendo chato jogar um deles, a coisa passou a ser impossível. Mas será que não existem alternativas (mais divertidas, inclusive) para continuar com a distração num momento tão difícil como esse? Como eu bem descobri, a resposta estava nos próprios jogos de tabuleiro - mais precisamente, na reinvenção que eles sofreram nas últimas décadas.\nExistem inúmeros jogos de tabuleiro disponíveis atualmente e um número crescente de pessoas que os curtem. Dada esta diversidade de novos títulos, inúmeros portais têm focado em criar e manter essa cultura, trazendo reportagens, fóruns, marketplaces, reviews, rankings e fichas técnicas de cada um deles. Dois exemplos destes sites são o BoardGameGeek e a Ludopedia: ambos possuem praticamente o mesmo conteúdo, mas o primeiro é um portal americano e o segundo é brasileiro. Outro ponto interessante é que o consumo de informações desses portais não precisa ocorrer pelo browser, uma vez que ambos fornecem uma API. A Ludopedia oferece uma API REST bastante intuitiva1, enquanto o BoardGameGeek usa uma API XML que eu acabei achando meio complicada de usar. Mas o que isto tudo tem haver com dados?\nBom, logo que descobri esse hobby, acabei ficando muito perdido sobre quais são os títulos mais legais para se jogar. São tantas as possibilidades e informações disponíveis sobre cada jogo, que eu me peguei navegando entre inúmeras páginas naqueles portais para tentar encontrar aquilo que eu estava buscando. Assim, acabei tendo a ideia de compilar essas informações e colocar tudo dentro de uma linguagem de programação, a fim de deixar a análise de dados me ajudar a encontrar os jogos que mais combinavam com aquilo que eu estava buscando. Para isso, tive a ideia de pegar as informações dos jogos do BoardGameGeek (BGG daqui em diante) através de sua API, tabular tudo o que estava buscando e partir para o abraço. Mas nada é tão simples quanto parece.\nA parede que encontrei é bem chatinha: o request da API XML do BoardGameGeek funciona muito melhor quando usamos o código numérico de identificação do jogo. Quando passamos o nome do jogo para o request, ele precisa estar grafado igual à como está na base do BGG, caso contrário ele pode falhar em trazer o que você está buscando ou trazer todos os títulos que tenham um match parcial com aquele que você buscou (daí para a frente é só caos). Outra ressalva aqui é que essa API não oferece nenhum tipo de método através do qual podemos pegar um de-para dos IDs numéricos para os nomes dos jogos, e o código numérico deles também não é sequencial. Logo, não dá para fazer uma busca gulosa e loopar os IDs de 1 até n. A solução mais simples para o problema é montar a nossa própria base de-para, catando o nome dos títulos e o seu ID numérico de algum lugar do site do BGG - e esse lugar é a página que contém o ranking dos jogos de tabuleiro no site.\nNeste post eu vou mostrar como raspar a página do ranking do BGG, usando como base o fluxo do Web Scrapping que a galera da Curso-R criou (Lente (2018)), e muito bem ilustrada na figura abaixo.\n\nCódigoknitr::include_graphics(path = 'images/web_scrapping_cycle_curso_r.png')\n\n\n\nFluxo do Web Scrapping de acordo com o Lente (2018). Figura copiada de https://blog.curso-r.com/posts/2018-02-18-fluxo-scraping/."
  },
  {
    "objectID": "posts/2021-09-17_scrapper-boardgamegeek/index.html#footnotes",
    "href": "posts/2021-09-17_scrapper-boardgamegeek/index.html#footnotes",
    "title": "Raspando a página do ranking do BoardGameGeek",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nEssa API ainda está em desenvolvimento, e devo escrever sobre o consumo de informações através dela em outro post↩︎"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html",
    "href": "posts/2021-10-18_formula-1/index.html",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "",
    "text": "A Fórmula 1 é um dos esportes de velocidade mais famosos do mundo, com provas ocorrendo desde o início da década de 50 até os dias de hoje. Com um histórico tanto grande desses, com provas tão intrincadas, tantas montadoras, pilotos e acontecimentos, é de se imaginar também que uma enorme quantidade de dados tenham sido gerados em cada prova. Seguindo esta ideia, existe um site1 que hospeda este registro histórico, e que foi o tema de um #TidyTuesday2 recentemente. Achei os dados disponíveis bastante interessantes e, como (toda) pessoa interessada em dados, logo comecei a pensar sobre os tipos de perguntas que poderiam ser respondidos olhando para eles. Mas antes de seguir essa estória, vamos baixar os dados do repositório oficial do TidyTuesday (ou usá-los localmente se você já o tiver baixado).\n\n# carregando os pacotes necessários\nlibrary(tidyverse) # core\nlibrary(fs) # manipular paths\nlibrary(lubridate) # trablhar com datas\nlibrary(ggforce) # extender o ggplot2\nlibrary(broom) # wrangling dos resultados da regressão\nlibrary(metafor) # para a meta-análise\nlibrary(reactable) # tabelas reativas\nlibrary(sparkline) # embedar widgets\nlibrary(patchwork) # compor figuras\nlibrary(tidytuesdayR) # ler os arquivos do tidytuesday\n\n# carregando todos os dados a partir do github do tidytuesday\n# se você quiser baixar os dados direto da fonte é só descomentar \n# como existe um limite de requests que podem ser feitos ao site, resolvi\n# deixar aqui só para referência mesmo\n# tt_dataset &lt;- tt_load(x = 2021, week = 37)\n\n# carregando a copia local dos dados\n## extraindo os paths das copias locais\npaths_copias_locais &lt;- dir_ls(path = 'data/')\n\n## criando vetor de nomes dos arquivos\nnomes_arquivos &lt;- paths_copias_locais %&gt;% \n  path_file() %&gt;% \n  path_ext_remove()\n\n## carregando os arquivos em uma lista\ntt_dataset &lt;- map(.x = paths_copias_locais, .f = read_rds)\n\n## renomeando os elementos da lista\nnames(tt_dataset) &lt;- nomes_arquivos\n\nUma das primeiras coisas que me ocorreram é que ao longo do tempo a tecnologia do setor automobilístico avançou bastante, levando ao desenvolvimento de motores cada vez mais performáticos e potentes. Não só os motores em si devem ter melhorado, mas todas as outras partes dos carros sofreram alterações no intuito de melhorar a sua aerodinâmica e fornecer mais vantagens aos pilotos que os conduzem - seja em termos de manobrabilidade, velocidade, aceleração ou segurança. Neste contexto, eu esperaria que as provas da Fórmula 1 ficassem cada vez mais curtas com o passar dos anos, certo? Então, a primeira coisa que fiz foi montar um histórico com a evolução da duração das provas ao longo dos anos.\nComo primeiro passo para construir este histórico precisei combinar duas tabelas que estão disponíveis junto aos dados - tt_dataset$results e tt_dataset$status. Fiz isso pois a primeira tabela traz o tempo de conclusão de prova de cada piloto, enquanto a segunda tabela é um de-para que nos permite mapear quais pilotos concluíram ou não cada uma delas. Usei esta informação para filtrar a primeira tabela e extrair o tempo de prova (em minutos) de cada piloto que as concluiu. Com base nessa informação, então, calculei o tempo de duração de cada prova como a média do tempo entre todos os pilotos, bem como o desvio padrão deste valor e a quantidade de observações nas quais se baseiam estas estimativas.\n\nCódigo## adicionando o dicionario com o de-para do statusId\nresultados &lt;- left_join(x = tt_dataset$results,\n                        y = tt_dataset$status,\n                        by = 'statusId')\n\n# criando base com o minimo, media e maximo dos tempos de cada prova\ntempos_de_prova &lt;- resultados %&gt;% \n  # pegando apenas os pilotos que concluiram a prova\n  filter(status == 'Finished') %&gt;% \n  # removendo qualquer valor na coluna milliseconds que não contenha pelo menos um número\n  filter(str_detect(string = milliseconds, pattern = '[0-9]')) %&gt;% \n  # parseando a coluna de milliseconds para numerico\n  mutate(\n    milliseconds = parse_number(milliseconds),\n    # calculando a quantidade de tempo em horas\n    minutos      = (milliseconds / 1000) / 60\n  ) %&gt;% \n  # agrupando pela prova\n  group_by(raceId) %&gt;% \n  # pegando o valor minimo, medio e maximo dos tempo de prova\n  summarise(\n    media  = mean(minutos, na.rm = TRUE),\n    erro   = sd(minutos, na.rm = TRUE),\n    obs    = n()\n  )\n\n## criando a tabela para a visualização\ntempos_de_prova %&gt;% \n  # passando para caracter só para facilitar o plot da tabela\n  mutate(raceId = as.character(raceId)) %&gt;% \n  reactable(striped = TRUE, highlight = TRUE, compact = TRUE, \n            columns = list(\n              media = colDef(name = 'Duração média (min)', format = colFormat(digits = 2)),\n              erro  = colDef(name = 'Desvio padrão', format = colFormat(digits = 2)),\n              obs = colDef(name = 'Observações')\n            ),\n            defaultColDef = colDef(align = 'center',\n                                   footer = function(values) {\n                                     if (!is.numeric(values)) return()\n                                     sparkline(values, type = \"box\", width = 100, height = 30)\n                                   })\n  )\n\n\n\n\n\n\nComo podemos ver na figura abaixo, existe uma tendência forte de queda no tempo de duração das provas até a década de 70 e, então, uma desaceleração desta tendência. Além disso, parece haver uma certa variância nesta série temporal ao longo da última década, inclusive com um aparente aumento nos tempos de prova. Isso acabou me surpreendendo, uma vez que a minha expectativa era de que as provas estariam ficando mais curtas. Mas o que será que poderia estar ocorrendo?\n\nCódigotempos_de_prova %&gt;% \n  # juntando com as informacoes da data de ocorrência de cada prova\n  left_join(y = select(tt_dataset$races, raceId, date), \n            by = 'raceId') %&gt;% \n  mutate(\n    # parseando a data para date\n    data   = as_date(x = date),\n    # extraindo o ano do objeto de data\n    year   = year(x = data),\n    # calculando a decada onde ocorreu cada prova\n    decada = (year %/% 10) * 10,\n    # passando a decada para caracter, pois quero que o mapeamento de cores\n    # seja feito usando uma escala discreta, e não contínua\n    decada = as.character(decada)\n  ) %&gt;% \n  # criando a figura do historico de tempos de prova\n  ggplot(mapping = aes(x = data, y = media)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = decada), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('1950-01-01'), to = as.Date('2021-12-01'), by = '5 years'),\n               labels = seq(from = 1950, to = 2020, by = 5)) +\n  scale_fill_viridis_d() +\n  labs(\n    title    = 'Série histórica da duração média das provas',\n    subtitle = 'A linha vermelha representa a tendência geral de duração das provas no histórico, enquanto os pontos representam\\na duração de cada uma das provas',\n    x        = 'Período',\n    y        = 'Duração Média (minutos)'\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\nEu esperaria que a duração das provas permanecessem relativamente estáveis em pelo menos duas condições. A primeira delas é no caso dos carros não estarem ficando mais rápidos ao longo dos anos. Não dá para avaliar isso tão bem com os dados que estão disponíveis, uma vez esta informação (i.e., fastestLapSpeed) só passou a ser registrado de forma consistente a partir de 2004 (painel A na figura abaixo). Esta série até mostra que a velocidade andou baixando um pouquinho na última década, mas precisávamos mesmo é ver como eram as velocidades lá para trás, quando a série temporal dos tempos de conclusão passou a ficar mais flat.\nA segunda coisa que me ocorreu seria o caso onde houvesse uma mudança sistemática na quantidade de voltas em cada prova, que refletisse aquele mesmo padrão da duração das provas. Embora o painel B da figura abaixo mostre que existe alguma semelhança entre as duas séries temporais, podemos ver que a variância na quantidade de voltas pareceu ficar bem mais tamponada ao longo do tempo. Além disso, a queda observada ao longo das primeiras décadas não é tão forte quanto àquela observada na outra série temporal. Finalmente, ainda que a mesma quantidade de voltas em cada prova sejam dadas hoje e no passado, não me parece razoável acreditar que a velocidade dos carros não variou em nada neste mesmo período. Claro, pode sempre ter alguma regra da FIA que defina alguns padrões que segurassem aqueles comportamentos, mas algo não parece fechar.\n\nCódigo## mapeando a velocidade maxima por prova\nvelocidades_por_prova &lt;- resultados %&gt;%\n  # parseando a velocidade para numerico\n  mutate(fastestLapSpeed = parse_number(fastestLapSpeed)) %&gt;% \n  # extraindo a velocidade maxima por prova\n  group_by(raceId) %&gt;% \n  filter(fastestLapSpeed == max(fastestLapSpeed, na.rm = TRUE)) %&gt;% \n  ungroup %&gt;% \n  # garantindo que temos valores unicos por prova\n  distinct(raceId, fastestLapSpeed)\n\n## calculando a quantidade de voltas em cada prova\nvoltas_por_prova &lt;- resultados %&gt;% \n  # considerando apenas os pilotos que concluiram cada prova\n  filter(status == 'Finished') %&gt;% \n  # selecionando as colunas de interesse\n  select(raceId, laps) %&gt;% \n  # pegando o valor maximo da quantidade de voltas por prova\n  group_by(raceId) %&gt;% \n  summarise(laps = max(laps))\n\n## mapeando cada circuito à uma prova\nprovas &lt;- left_join(x = tt_dataset$races,\n                    y = tt_dataset$circuits,\n                    by = 'circuitId') %&gt;% \n  # removendo URL da wikipedia\n  select(-contains('url'), -circuitRef, -circuitId, -round, -time) %&gt;%\n  # renomeando o nome do GP e do circuito\n  rename(gp = name.x, circuit = name.y, data = date)\n\n## juntando informacoes\nfeatures_por_prova &lt;- provas %&gt;% \n  ## juntando de voltas por prova\n  left_join(y = voltas_por_prova, by = 'raceId') %&gt;% \n  ## juntando velocidades por prova\n  left_join(y = velocidades_por_prova, by = 'raceId') %&gt;% \n  ## adicionando decada à tabela\n  mutate(decada = (year %/% 10) * 10)\n\n## criando figura do historico de velocidade por prova\nfig1 &lt;- features_por_prova %&gt;% \n  select(data, decada, fastestLapSpeed) %&gt;% \n  drop_na() %&gt;% \n  ggplot(mapping = aes(x = data, y = fastestLapSpeed)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = as.character(decada)), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('2004-01-01'), to = as.Date('2021-01-01'), by = '4 years'),\n               labels = seq(from = 2004, to = 2020, by = 4)) +\n  scale_fill_viridis_d() +\n  labs(\n    title = '(A) Série histórica da velocidade máxima nas provas',\n    x     = 'Período',\n    y     = 'Velocidade máxima (Km/h)'\n  ) +\n  theme(legend.position = 'none')\n\n## criando figura do historico de voltas por prova\nfig2 &lt;- features_por_prova %&gt;% \n  ggplot(mapping = aes(x = data, y = laps)) +\n  geom_line(alpha = 0.3, size = 0.7) +\n  geom_point(mapping = aes(fill = as.character(decada)), \n             shape = 21, size = 1.5, alpha = 0.3, color = 'black', show.legend = FALSE) +\n  geom_smooth(se = FALSE, color = 'firebrick3', method = 'loess') +\n  scale_x_date(breaks = seq.Date(from = as.Date('1950-01-01'), to = as.Date('2021-12-01'), by = '10 years'),\n               labels = seq(from = 1950, to = 2020, by = 10)) +\n  scale_fill_viridis_d() +\n  labs(\n    title = '(B) Série histórica da quantidade de voltas por prova',\n    x     = 'Período',\n    y     = 'Quantidade de voltas'\n  ) +\n  theme(legend.position = 'none')\n\n## criando composição\nfig1 / fig2\n\n\n\n\nFoi daí que me ocorreu que essa não era uma série temporal convencional. Isto porquê cada corrida dentro de uma temporada vem de uma prova ocorrida em um dado circuito, e poderia ser o caso que o roster de circuitos tivesse mudado bastante entre as temporadas. Mais importante, dada tantas peculiaridades ligadas aos circuitos em si, não faria mais sentido tentar entender o quanto as provas dentro do mesmo circuito têm ou não ficado mais rápidas entre as temporadas? Isto é, será que nossa unidade básica de previsão para entender os padrões de duração das provas não seria o circuito, ao invés de cada prova em si? Afinal, observações vindas de um mesmo circuito não são independentes entre si.\nCom isto em mente, isolei cada circuito e fiz um levantamento do período no qual cada um deles esteve no roster das temporadas. A primeira coisa legal que vi com isso é que nenhum circuito esteve em todas as temporadas da Fórmula 1 (i.e., nenhum segmento é contínuo de ponta à ponta na figura abaixo). O segundo padrão interessante é que são poucos os circuitos que se mantiveram por um período longo de temporadas (i.e., os circuito estão ordenados de cima para baixo, daqueles com o maior volume de provas para o menor volume de provas). Por fim, outro padrão que me saltou aos olhos foi o fato de que alguns circuitos só acabaram estando em uma única temporada mesmo (i.e., os últimos circuitos na figura abaixo) ou, ainda, participado de forma muito intermitente do roster (i.e., segmentos descontínuos e/ou pontos isolados). Em essência, (1) temos uma série temporal sequência de valores com a duração das provas da Fórmula 1 ao longo das temporadas, (2) estes valores não pertencem sempre as mesmas entidades e, (3) quando estas entidades se repetem, pode ser que estejam bem distantes uma das outras no tempo.\n\nCódigofeatures_por_prova %&gt;% \n  # pegando as ocorrências únicas de cada circuito em cada temporada\n  distinct(circuit, data) %&gt;% \n  # extraindo o ano a partir da data de ocorrência da corrida\n  mutate(ano = year(data)) %&gt;% \n  # organizando a base de acordo com os anos dentro de cada circuito\n  arrange(circuit, ano) %&gt;% \n  # agrupando pelo circuito\n  group_by(circuit) %&gt;%\n  mutate(\n    # criando uma dummy que será 1 caso a diferença entre o ano de ocorrência\n    # de corridas sucessivas dentro de um mesmo circuito seja 1 ou caso seja\n    # o primeiro registro de prova naquele circuito\n    recorrencia       = (ano - lag(ano)) != 1 | is.na(ano - lag(ano)),\n    # acumulando a dummy de recorrencia, de forma a criar grupos que sinalizem\n    # anos sucessivos onde houve uma prova naquele circuito\n    grupo_recorrencia = cumsum(recorrencia),\n    # calculando a quantidade total provas registradas em cada circuito\n    n_provas          = n()\n  ) %&gt;% \n  # adicionando a recorrencia ao group_by\n  group_by(grupo_recorrencia, .add = TRUE) %&gt;% \n  summarise(\n    # extraindo o ano de inicio de cada fase sucessiva de ocorrência de provas\n    # em cada circuito\n    inicio   = min(ano),\n    # extraindo o ano de fim de cada fase sucessiva de ocorrência de provas\n    # em cada circuito\n    fim      = max(ano),\n    # extraindo a quantidade total de provas em cada circuito\n    n_provas = max(n_provas),\n    # dropando os grupos\n    .groups = 'drop'\n  ) %&gt;% \n  mutate(\n    # reordenando os níveis do circuito de acordo com a quantidade total de provas\n    circuit = fct_reorder(.f = circuit, .x = n_provas, .fun = sum)\n  ) %&gt;% \n  # criando figura para ver o período e/ou ano de ocorrências das provas em cada circuito\n  ggplot() +\n  geom_segment(mapping = aes(x = inicio, xend = fim, y = circuit, yend = circuit)) +\n  geom_point(mapping = aes(x = inicio, y = circuit), size = 2,\n             shape = 21, color = 'black', fill = 'white') +\n  geom_point(mapping = aes(x = fim, y = circuit), size = 2,\n             shape = 21, color = 'black', fill = 'grey70') +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 10)) +\n  labs(\n    title    = 'Ocorrência das provas em cada circuito ao longo dos anos',\n    subtitle = 'Os segmentos representam o intervalo de anos sucessivos nos quais cada circuito esteve no roster',\n    x        = 'Período'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nParece que não tem muito o que fazer para abordarmos analiticamente a evolução da duração das provas da Fórmula 1. Por um lado, a sequência temporal está bem estacionária há uns 50 anos - fato que parece estar associado à constância na quantidade de voltas em cada prova e, mais recentemente, à mudança bem pequena na velocidade dos carros. Por outro lado, parece que parte do que estamos buscando entender pode estar sendo mascarado pelo fato das provas ocorrerem em diferentes circuitos em cada temporada. Se pudéssemos analisar a série temporal de cada circuito, isto nos daria mais insights sobre a real evolução da duração das provas, uma vez que controlamos o efeito do circuito; todavia, como vimos, temos um sashimi de séries temporais e, portanto, não daria para ajustar um modelo estatístico mais tradicional. Como então atacar esse problema?"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#footnotes",
    "href": "posts/2021-10-18_formula-1/index.html#footnotes",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nhttps://ergast.com/mrd/↩︎\nhttps://github.com/rfordatascience/tidytuesday↩︎\nAlém de ser a função utilizada para realizar os cálculos dos tamanhos do efeito quando estamos usando este pacote para fazer uma meta-análise.↩︎\na função calcula o inverso deste argumento internamente, mas é possível passar o inverso da variância já calculada através do argumento W↩︎\nAlgumas pessoas também chamam de modelo de painel.↩︎\nE isto seria algo que poderia nos motivar a buscar entender o porquê disso, também através de uma meta-análise, utilizando moderadores - que é a forma como chamamos as variáveis preditoras na área.↩︎\nEm essência, vamos estimar um intercepto e um slope fixo para todos os circuitos, além de um intercepto aleatório para cada circuito↩︎"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-i",
    "href": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-i",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Abordagem Meta-Analítica I",
    "text": "Abordagem Meta-Analítica I\nO sucesso de uma meta-análise está logo na sua primeira etapa: a definição de, pelo menos, 2 de 4 informações sobre o que queremos fazer:\n\n\nPopulação: o objeto básico do nosso estudo. Nesta seção, a nossa população será a sequência de provas ocorridas em um circuito;\n\n\nIntervenção: o tratamento, intervenção ou variável independente que temos interesse em relacionar com a população. Nesta seção, isso será o tempo - mais especificamente, uma sequência de provas ocorrendo de forma ininterrupta ao longo dos anos;\n\n\nComparação: aquilo que queremos contrastar. Não faremos uso dessa informação nesta seção, mas na próxima;\n\n\nOutcome: aquilo que quantifica o impacto da intervenção sobre a população - isto é, o que servirá, direta ou indiretamente, para medir o efeito da intervenção na população. Nesta seção, isto será a taxa de variação da duração das provas ao longo dos anos para um circuito.\n\nE como trabalhamos isto na prática? O outcome que vamos buscar nada mais é, nesse caso, do que o slope de uma regressão entre a duração da prova e o tempo (i.e., os anos) para cada um dos circuitos. Se o slope dessa relação for negativo, então é sinal de que as provas estão ficando curtas ao longo das temporadas; caso contrário, as provas estão ficando mais longas. Um detalhe que não podemos perder de vista é o fato de que muitas vezes um circuito sai do roster e depois retorna. Assim, precisaremos ajustar uma regressão para cada uma das sequências ininterruptas de anos nos quais aquele circuito esteve no roster. Como não queremos errar muito na mão e ajustar uma regressão com poucos pontos, vamos colocar uma restrição para só considerarmos dentro da nossa população as sequências de provas que tenham ocorrido pelo menos durante 5 anos sucessivos em cada circuito. O pedaço de código abaixo dá conta de identificar as provas que atendem à essa restrição em cada circuito, e já prepara o dataframe que vamos usar para ajustar as regressões.\n\n## pegando as provas que ocorrem em sequencia\nprovas_alvo &lt;- df %&gt;% \n  # pegando só as informações do id da prova, circuito e ano\n  select(raceId, circuit, year) %&gt;% \n  # organizando a base de acordo com os anos dentro de cada circuito\n  arrange(circuit, year) %&gt;% \n  # agrupando a base pelo circuito\n  group_by(circuit) %&gt;% \n  mutate(\n    # criando uma dummy que será 1 caso a diferença entre o ano de ocorrência\n    # de corridas sucessivas dentro de um mesmo circuito seja 1 ou caso seja\n    # o primeiro registro de prova naquele circuito\n    recorrencia = (year - lag(year)) != 1 | is.na(year - lag(year)),\n    # acumulando a dummy de recorrencia, de forma a criar grupos que sinalizem\n    # anos sucessivos onde houve uma prova naquele circuito\n    grupo_recorrencia = cumsum(recorrencia)\n  ) %&gt;% \n  ungroup %&gt;% \n  # contando quantas vezes cada grupo dentro de cada circuito aparece - i.e., calculando\n  # o tamanho de cada uma das sequências ininterruptas de provas dentro de cada circuito\n  add_count(circuit, grupo_recorrencia, name = 'ocorrencias_continuas') %&gt;% \n  # removendo toda as sequências compostas por menos de 4 anos consecutivos\n  filter(ocorrencias_continuas &gt;= 5) %&gt;% \n  # pegando só o id da prova e o grupo de recorrencia de cada uma\n  select(raceId, grupo_recorrencia)\n\n## criando dataframe com as provas que vamos usar\ndf_regs &lt;- provas_alvo %&gt;% \n  # filtrando as provas que de fato podemos usar\n  left_join(y = df, by = 'raceId')\nglimpse(x = df_regs)\n\nRows: 849\nColumns: 17\n$ raceId            &lt;dbl&gt; 651, 639, 628, 616, 601, 587, 570, 554, 538, 522, 50…\n$ grupo_recorrencia &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ media             &lt;dbl&gt; 102.77994, 91.07974, 89.98567, 89.59983, 89.63631, 5…\n$ erro              &lt;dbl&gt; 0.8427061623, 0.6348897167, 0.4935152496, 0.64426457…\n$ obs               &lt;int&gt; 3, 7, 8, 6, 6, 7, 7, 5, 3, 5, 7, 6, 2, 4, 4, 5, 1, 2…\n$ year              &lt;dbl&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978…\n$ gp                &lt;chr&gt; \"Austrian Grand Prix\", \"Austrian Grand Prix\", \"Austr…\n$ data              &lt;date&gt; 1970-08-16, 1971-08-15, 1972-08-13, 1973-08-19, 197…\n$ circuit           &lt;chr&gt; \"A1-Ring\", \"A1-Ring\", \"A1-Ring\", \"A1-Ring\", \"A1-Ring…\n$ location          &lt;chr&gt; \"Spielberg\", \"Spielberg\", \"Spielberg\", \"Spielberg\", …\n$ country           &lt;chr&gt; \"Austria\", \"Austria\", \"Austria\", \"Austria\", \"Austria…\n$ lat               &lt;dbl&gt; 47.2197, 47.2197, 47.2197, 47.2197, 47.2197, 47.2197…\n$ lng               &lt;dbl&gt; 14.7647, 14.7647, 14.7647, 14.7647, 14.7647, 14.7647…\n$ alt               &lt;dbl&gt; 678, 678, 678, 678, 678, 678, 678, 678, 678, 678, 67…\n$ laps              &lt;dbl&gt; 60, 54, 54, 54, 54, 29, 54, 54, 54, 54, 54, 53, 53, …\n$ fastestLapSpeed   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ decada            &lt;dbl&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970…\n\n\nComo podemos ver, das 1047 provas que tínhamos originalmente, somente 849 atendem ao pré-requisito de terem ocorrido dentro daquela sequência contínua de anos em cada circuito. A figura abaixo tenta mostrar o que queremos fazer até aqui: ajustar uma regressão para cada sequência de anos dentro de cada circuito, extrair os slopes destas regressões e os erros associado à cada um deles.\n\nCódigo## visualizando regressões\ndf_regs %&gt;% \n  ggplot(mapping = aes(x = year, y = media, color = circuit, group = grupo_recorrencia)) +\n  facet_wrap(~ circuit, scales = 'free') +\n  geom_point(color = 'black', fill = 'grey70', shape = 21) +\n  geom_smooth(color = 'black', linetype = 1, method = 'lm', se = TRUE) +\n  labs(\n    title = 'Regressões aplicadas à cada período dentro de cada circuito',\n    x     = 'Ano',\n    y     = 'Duração Média (minutos)'\n  )\n\n\n\n\nCom isto em mente, é hora de ajustar das regressões! Vamos fazer isso de forma tidy, aninhando o dataframe e deixando de fora apenas o circuito e o identificador da sequência de anos dentro de cada um deles. Na sequência, vamos usar a função purrr::map para ajustar uma regressão para prever a duração das provas de acordo com uma sequência de anos. Um ponto importante é que diferenças nos valores do intervalo de anos para cada regressão influenciam diretamente a estimativa do slope de cada uma delas. Portanto, vamos padronizar a variável independente ano dentro de cada recorte, de forma que os slopes estejam livres do confounding do valor dos anos utilizados para ajustá-los. Neste contexto, os slopes vão representar a mudança na duração das provas para cada unidade de desvio padrão do tempo (i.e., anos).\n\ndf_regs &lt;- df_regs %&gt;% \n  # aninhando o dataframe e deixando de fora apenas o circuito e o identificador da \n  # sequencia de anos dentro de cada circuito\n  nest(data = -c(circuit, grupo_recorrencia)) %&gt;% \n  mutate(\n    # padronizando o ano dentro de cada recorte\n    data      = map(.x = data, .f = mutate, year_scaled = (year - mean(year)) / sd(year)),\n    # ajustando uma regressão para cada recorte\n    modelo    = map(.x = data, .f = ~ lm(media ~ year_scaled, data = .x)),\n    # extraindo os coeficientes da regressão\n    tidyed    = map(.x = modelo, .f = tidy),\n    # extraindo o ano maximo dentro de cada recorte\n    ano_max   = map_dbl(.x = data, .f = ~ pull(.x, 'year') %&gt;% max),\n    # extraindo o ano minimo dentro de cada recorte\n    ano_min   = map_dbl(.x = data, .f = ~ pull(.x, 'year') %&gt;% min),\n    # extraindo a quantidade de anos que cada intervalo compreende\n    intervalo = ano_max - ano_min\n  )\n\nRegressões ajustadas, vamos olhar o que conseguimos extrair dos dados. A figura abaixo mostra que os slopes (i.e., estimate) não estão muito relacionados às estimativas de erro, o ano de fim da estimativa e nem o intervalo de anos. Por outro lado e, como era de se esperar, as estimativas de erro parecem ser menores tanto maior forem a quantidade de pontos que usamos para ajustar as regressões. Tirando isso, nada de muito surpreendente nos dados.\n\nCódigodf_regs %&gt;% \n  # desempacotando a coluna com as estimativas de cada regressão\n  unnest(tidyed) %&gt;% \n  # pegando só os slopes\n  filter(term == 'year_scaled') %&gt;% \n  # plotando a figura\n  ggplot() +\n  geom_autopoint(shape = 21, color = 'black', fill = 'grey70', alpha = 0.5) +\n  geom_autodensity(color = 'black', fill = 'grey70') +\n  facet_matrix(vars(estimate, std.error, ano_max, intervalo), layer.diag = 2)\n\n\n\n\nVamos fazer uma breve pausa para entender o próximo passo da meta-análise. O outcome que estávamos buscando é o slope das regressões da duração das provas vs o tempo para cada sequência ininterrupta de anos nos quais elas ocorreram em cada circuito. Dentro do contexto da meta-análise, utilizaremos estes slopes como uma medida do tamanho do efeito (i.e., effect size): a informação sobre a magnitude e o sinal de uma intervenção sobre a população estudada. É esta a medida que será combinada através do modelo meta-analítico. Aqui estamos usando o slope das regressões como métrica de tamanho do efeito, mas qualquer métrica quantitativa pode ser usada para tal em uma meta-análise: o valor de uma média, uma métrica relacionada a um modelo (e.g., coeficiente de determinação, coeficiente de correlação,…) a um tipo de problema de negócio (e.g., acurácia, AUC,…), informações extraídas de uma tabela de contingência, comparações entre médias (veremos um exemplo deste na próxima seção). O importante é que o tamanho do efeito seja caracterizado por uma métrica quantitativa, comum a todos os estudos na população que estamos estudando. A função escalc do pacote metafor fornece uma visão bastante detalhada sobre as diferentes métricas de tamanho do efeito e os seus casos de uso3.\nUma outra coisa que precisamos é uma estimativa da incerteza ao redor do tamanho do efeito. Isto é importante pois nem todos os estudos possuem a mesma precisão ao estimar as relações que estamos querendo investigar e, se queremos combiná-los, devemos levar em consideração que estimativas mais precisas devem ter um peso maior na nossa análise do que àquelas com maior erro. Esta medida de incerteza normalmente é dada como o inverso da variância do tamanho do efeito, e é utilizada no modelo meta-analítico para ponderar cada estudo. Logo, nesse contexto, um modelo meta-analítico pode ser pensado como um tipo de regressão ponderada. Um curiosidade importante: você só pode chamar uma meta-análise como tal caso esta medida de incerteza seja utilizada para ajustar o modelo; caso contrário (i.e., todos os tamanhos do efeito têm o mesmo peso), a análise feita pode ser chamada apenas de síntese.\nCom esta visão em mente, vamos extrar os slopes de cada um dos modelos ajustados anteriormente, bem como a estimativa do erro associado à cada um deles. Para chegarmos à variância do slope, basta então elevar o valor desse momento ao quadrado.\n\n## desempacotando os resultados das regressoes\ndf_regs &lt;- df_regs %&gt;% \n  # removendo a list column de data e a coluna com o objeto dos modelos ajustados\n  select(-data, - modelo) %&gt;% \n  # desaninhando a list column com os coeficientes das regressões\n  unnest(cols = tidyed) %&gt;% \n  # pegando apenas o slope das regressões\n  filter(term == 'year_scaled') %&gt;% \n  # dropando a coluna com a string do slope\n  select(-term) %&gt;% \n  # calculando a variância do slope, elevando o erro da estimativa ao quadrado\n  mutate(\n    variance = std.error ^ 2\n  )\nrmarkdown::paged_table(x = df_regs)\n\n\n\n  \n\n\n\nTemos tudo pronto, agora é só ajustar o modelo meta-analítica. Para isso, vou utilizar a função rma.mv do pacote metafor para ajustar um modelo meta-analítico de efeitos aleatórios. Este modelo vai combinar os tamanhos do efeito (i.e., slopes) obtidos a partir de cada regressão em um efeito global, ponderando cada observação através de sua variância4. Outro ponto importante é que como alguns circuitos contribuem com mais de uma medida, é necessário considerar esta fonte de não-independência na análise. Fazemos isso especificando o argumento random, e passando uma fórmula que especificará que os tamanhos do efeito estão agrupados através dos níveis da variável circuit. Posto de outra forma, este modelo funciona de forma muito parecida com um modelo de efeitos aleatórios5 ponderado, onde estimamos um intercepto fixo para todas as observações e um intercepto aleatório para cada circuito. Para mais informações, o help dessa função está muito bem documentada.\n\nmodelo_ma_slopes &lt;- rma.mv(yi = estimate, V = variance, random = ~ 1 | circuit, data = df_regs)\nmodelo_ma_slopes\n\n\nMultivariate Meta-Analysis Model (k = 61; method: REML)\n\nVariance Components:\n\n             estim    sqrt  nlvls  fixed   factor \nsigma^2    12.1997  3.4928     42     no  circuit \n\nTest for Heterogeneity:\nQ(df = 60) = 617.5129, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub     \n -1.8680  0.5979  -3.1245  0.0018  -3.0398  -0.6962  ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO output da análise traz algumas informações muito importantes:\n\nO valor em sigma^2 quantifica a variância entre estudos, neste caso mostrando que existe uma variabilidade substancial no slope das regressões entre circuitos;\n\nIsto também é representado através da estatística Q, que é utilizada para testar a hipótese nula de que todas as observações representam uma amostra aleatória homogênea da população de estudo (i.e., os slopes são homogêneos entre os circuitos). O teste de hipótese baseado nesta estatística segue uma distribuição do Qui-Quadrado com K-1 graus de liberdade (onde K é o número de observações) e, nesse caso, sugere que devemos rejeitar a hipótese nula, em favor da hipótese de que os slopes são de fato diferentes entre os circuitos6.\nFinalmente, temos o resultado do modelo, que nos mostra: o valor da estimativa global estimada pelo modelo (estimate), o erro associado à esta estimativa (se), os intervalos de confiança inferior e superior da estimativa (ci.lb e ci.ub, respectivamente), bem como um teste estatístico da hipótese nula de que o efeito global estimado não difere de 0 (i.e., o slope na realidade é flat). Neste caso, podemos ver que rejeitamos esta hipótese nula, o que também fica claro olhássemos só a estimativa do intervalo de confiança.\n\nMas beleza…o que esse último resultado quer nos dizer? Ele mostra que, entre todas as sequências ininterruptas de provas entre os circuitos, a evidência sugere que para cada mudança de um desvio padrão entre os anos (i.e., para cada três anos), as provas ficam cerca de 1.868 minutos mais curtas, com um intervalo de confiança de 3.04 à 0.696 minutos. Isto é, quando levamos em consideração diferenças entre os circuitos, existe evidência para dizer que as provas estão ficando mais curtas sim - mas é tão pouco que chega à dar dó: de 40 segundos à 3 minutos.\nPara fechar essa seção, também podemos ter acesso aos valores dos interceptos aleatórios estimados pelo modelo através da função ranef (i.e., a diferença circuito-específico da estimativa global do modelo). O 0 na figura abaixo representa a estimativa global do modelo, e podemos ver que alguns circuitos têm um desvio consistente para cima (i.e., circuitos nos quais o slope tende a ser mais positivo do que o estimado - círculos azuis) e outros para baixo (i.e., circuitos nos quais o slope tende a ser mais negativo do que o estimado - círculos vermelhos), mas a maioria deles não difere muito do slope global estimado (i.e., círculos vazios).\n\nCódigoranef(object = modelo_ma_slopes) %&gt;% \n  pluck('circuit') %&gt;% \n  rownames_to_column(var = 'circuit') %&gt;% \n  mutate(\n    circuit  = fct_reorder(.f = circuit, .x = intrcpt, .fun = mean),\n    efeito   = case_when(intrcpt &gt; 0 & pi.lb &gt; 0 ~ 'pos',\n                         intrcpt &lt; 0 & pi.ub &lt; 0 ~ 'neg',\n                         TRUE ~ 'none')\n  ) %&gt;% \n  ggplot(mapping = aes(y = circuit, x = intrcpt)) +\n  geom_vline(xintercept = 0, color = 'grey50') +\n  geom_errorbar(mapping = aes(xmin = pi.lb, xmax = pi.ub), \n                width = 0, size = 0.5, color = 'grey50') +\n  geom_point(mapping = aes(fill = efeito), shape = 21, size = 2.5, color = 'black') +\n  scale_fill_manual(values = c('indianred3', 'white', 'dodgerblue3')) +\n  labs(\n    title    = 'Diferença no tamanho do efeito associado à cada circuito',\n    subtitle = 'Este figura demonstra o quanto cada circuito se afasta da estimativa do tamanho de\nefeito global. Neste contexto, a linha horizontal centrada em zero representaria aquele\nefeito, e a diferença no eixo horizontal é o quanto que uma instância pertencer à cada\ncircuito o modifica.',\n    x        = 'Diferença no tamanho do efeito'\n  ) +\n  theme(\n    legend.position = 'none',\n    axis.title.y    = element_blank()\n  )\n\n\n\n\nCom estes resultados, já temos a nossa resposta: sim, as provas estão ficando mais curtas ao longo das temporadas…mas bem pouquinho. Mas…o quanto será que este efeito varia de acordo com a janela de tempo que estamos usando para a comparação?"
  },
  {
    "objectID": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-ii",
    "href": "posts/2021-10-18_formula-1/index.html#abordagem-meta-analítica-ii",
    "title": "Entendendo os Padrões de Duração das Provas da Fórmula 1",
    "section": "Abordagem Meta-Analítica II",
    "text": "Abordagem Meta-Analítica II\nVamos mudar um pouco a abordagem agora, e tentar entender se e o quanto a duração de cada prova tem mudado ano a ano. Isto é, vamos responder à pergunta do quão diferente é o tempo de prova ‘hoje’ quando comparada à cada um dos anos anteriores: se as provas estão ficando mais curtas, então esperamos que a diferença entre o agora e o passado fique cada vez maior tanto mais distante for esse passado. Para isso, vamos mudar um pouco a forma como definimos àquelas informações sobre o passo inicial de uma meta-análise. Nesta seção temos:\n\n\nPopulação: definida como as provas ocorridas em um circuito. Apesar de parecer ser a mesma população da seção anterior, agora consideraremos todas as provas, não só àquelas que ocorrem em uma sequência ininterrupta de anos;\n\n\nIntervenção: continuaremos olhando o tempo, mas aqui ele será representado como cada um dos anos nos quais uma prova ocorreu em cada circuito (e, novamente, independentemente destas provas terem ocorrido em uma sequência ininterrupta de anos ou não);\n\n\nComparação: em alguns casos, queremos fazer uma comparação entre dois ou mais níveis de uma intervenção - e.g. tratamento vs controle. No nosso caso, vamos comparar a duração da prova mais recente em cada circuito com cada um dos tempos nos anos anteriores para aquele mesmo circuito.\n\nOutcome: como nossa ideia é comparar a duração das provas entre dois anos, vamos focar na diferença entre as médias destes tempos entre os dois anos para cada circutio.\n\nUm ponto importante aqui é que como nosso foco é fazer uma comparação, precisaremos sintetizar esta diferença de alguma forma. Neste contexto, vamos utilizar uma métrica de tamanho de efeito bastante popular em meta-análise: o log response ratio (novamente, o arquivo de ajuda da função metafor::escalc traz muita informação sobre essa métrica e suas aplicações). De forma muito breve, esta métrica é calculada através do logaritimo da razão entre as duas médias e, no nosso caso, será implementado para cada circuito x como:\n\\[\nLRR = log(\\frac{\\bar{x_i}}{\\bar{x_j}})\n\\] onde \\[\\bar{x_i}~=~média~da~duração~da~prova~no~circuito~x~no~ano~i~(i = max(i)),\\] \\[\\bar{x_j}~=~média~da~duração~da~prova~no~mesmo~circuito~x~no~ano~j~(j~&lt;~i)\\]\nDesta forma, se as provas tiverem ficado mais curtas quando comparamos o presente ao passado, então os valores desta métrica serão negativos (e o contrário quando as provas estiverem ficando mais longas). Além disso, essa métrica é de fácil interpretação, o que a torna bastante atraente também (e.g., um LRR = -0.10 representa uma redução de cerca de 10% na duração da prova do ano i quando comparado ao ano j). Como em toda meta-análise, também precisamos calcular a incerteza ao redor da estimativa do tamanho do efeito do LRR (i.e., a variância do LRR). Ele é implementado no código abaixo, onde também calculamos o LRR e extraímos algumas outras informações dos dados.\n\ndf_diffs &lt;- df %&gt;% \n  # selecionando apenas as colunas que vamos usar\n  select(raceId:circuit) %&gt;% \n  # removendo qualquer valor faltante na duração média e erro padrão associado\n  drop_na(media, erro) %&gt;% \n  # organizando a base em ordem decrescente de anos dentro de cada circuito\n  arrange(circuit, -year) %&gt;% \n  # agrupando a base por circuito\n  group_by(circuit) %&gt;% \n  mutate(\n    # calculando o log response ratio para cada observação - log(atual / ti)\n    lrr           = log(first(media) / media),\n    # calculando a estimativa geral de variância de cada observação\n    variancia     = (erro ^ 2) / (obs * (media ^ 2)), \n    # calculando a variância do log response ratio\n    lrr_var       = first(variancia) + variancia,\n    # calculando o intervalo em anos da comparação\n    intervalo     = first(year) - year,\n    # categorizando a variável em torno de intervalos de 5 anos\n    bin_intervalo = as.factor((intervalo %/% 5) * 5)\n  ) %&gt;% \n  # desagrupando o dataframe\n  ungroup %&gt;% \n  # removendo a primeira observação (_i.e._, atual vs atual)\n  filter(intervalo &gt; 0)\nrmarkdown::paged_table(x = df_diffs)\n\n\n\n  \n\n\n\nComo pode ser notado acima, a nossa estratégia foi calcular o tamanho do efeito comparando sempre a duração da prova mais recente disponível em cada circuito vs cada uma das provas ocorridas nos anos anteriores naquele mesmo circuito. Neste sentido, se a prova mais recente disponível para o circuito x tiver ocorrido no ano de 2015 e houverem outras duas provas em 2000 e 1990, então teremos uma instância com o LRR para a comparação 2015 vs 2000 e uma outra para a comparação 2015 vs 1990. Portanto, cada circuito contribui com N - 1 comparações (onde N é o número de provas para aquele circuito), e precisaremos considerar esta fonte de variação e não-independência dos dados modelo.\nOutra informação que extraímos no código foi a diferença de tempo entre o ano mais recente e cada um dos outros anos na comparação, que utilizaremos como moderador (i.e., variável preditora) no modelo. Só para lembrar, a nossa ideia principal nessa seção é averiguar se tanto mais para o passado realizarmos àquela comparação, tanto maior será a diferença observada no LRR. Desta forma, quando observamos uma comparação associada a um intervalo de 5 anos, estamos considerando todas as comparações que envolvam uma prova e àquelas ocorridas até 5 anos antes, e assim sucessivamente. Devido à natureza não-linear da relação entre o intervalo de tempo e o LRR, resolvi por discretizar o intervalo de anos em buckets de 5 anos para facilitar a análise. No entanto, deixei uma linha comentada em que é possível usar esta variável em seu formato contínuo.\nO modelo funciona de forma bastante similar à versão da seção anterior. A única diferença é que agora especificamos o argumento mods, passando o right-hand side da fórmula. Este modelo é conhecido com um modelo meta-analítico de efeitos mistos, pois temos uma variável fixa representada pelo moderador e uma variável aleatória representada pela identidade do circuito7.\n\nmodelo_ma_diffs &lt;- rma.mv(yi = lrr, V = lrr_var, \n                          # mods = ~ poly(x = intervalo, degree = 2, raw = TRUE), \n                          mods = ~ bin_intervalo,\n                          random = ~ 1 | circuit, data = df_diffs)\nmodelo_ma_diffs\n\n\nMultivariate Meta-Analysis Model (k = 946; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2    0.0316  0.1777     62     no  circuit \n\nTest for Residual Heterogeneity:\nQE(df = 931) = 8921303.0919, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:15):\nQM(df = 14) = 2991114.1249, p-val &lt; .0001\n\nModel Results:\n\n                 estimate      se        zval    pval    ci.lb    ci.ub      \nintrcpt           -0.0656  0.0226     -2.9089  0.0036  -0.1099  -0.0214   ** \nbin_intervalo5     0.0487  0.0003    190.9061  &lt;.0001   0.0482   0.0492  *** \nbin_intervalo15    0.0672  0.0003    223.8302  &lt;.0001   0.0666   0.0678  *** \nbin_intervalo20    0.0762  0.0003    249.2053  &lt;.0001   0.0756   0.0768  *** \nbin_intervalo25    0.0576  0.0004    149.2714  &lt;.0001   0.0569   0.0584  *** \nbin_intervalo30    0.0387  0.0004     89.5131  &lt;.0001   0.0378   0.0395  *** \nbin_intervalo10    0.0648  0.0003    224.3725  &lt;.0001   0.0642   0.0653  *** \nbin_intervalo35    0.0340  0.0005     67.5145  &lt;.0001   0.0330   0.0349  *** \nbin_intervalo40   -0.0550  0.0005   -103.1802  &lt;.0001  -0.0561  -0.0540  *** \nbin_intervalo45   -0.0243  0.0006    -41.4670  &lt;.0001  -0.0254  -0.0231  *** \nbin_intervalo50   -0.2137  0.0005   -445.0941  &lt;.0001  -0.2146  -0.2128  *** \nbin_intervalo55   -0.3905  0.0007   -577.1681  &lt;.0001  -0.3918  -0.3892  *** \nbin_intervalo60   -0.4647  0.0005   -883.3099  &lt;.0001  -0.4657  -0.4636  *** \nbin_intervalo65   -0.6387  0.0005  -1164.8388  &lt;.0001  -0.6398  -0.6376  *** \nbin_intervalo70   -0.5334  0.0016   -339.1343  &lt;.0001  -0.5365  -0.5303  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTemos algumas novidades quanto ao output do modelo:\n\nA estatística Q agora é particionada entre QE e QM: o primeiro é a mesma coisa que a definição da estatística Q dada na seção anterior, enquanto o segundo testa a significância estatística do modelo conforme representado pelos efeitos fixos - i.e., a significância do moderador. Neste caso, podemos ver que os tamanhos dos efeitos não representam uma amostra aleatória homogênea da população de estudo (i.e., existem diferenças entre circuitos em termos do LRR; p valor do QE é menor que 0.05) e que o moderador contribui para explicar a diferença no LRR entre os circuitos (i.e., p valor do QM é menor que 0.05);\n\nO model results traz a estimativa não só para o intercepto, mas também para todos os outros níveis do moderador que utilizamos. Todavia, o intercepto aqui representa o primeiro nível do moderador - i.e., provas ocorridas até 5 anos antes da prova mais recente na base para cada circuito. Portanto, este valor de estimate representa o LRR global para este subconjunto de provas, enquanto todos os outros estimates representam a diferença entre os níveis seguintes e este primeiro. Em outras palavras, eles nos informam se existe e qual o tamanho da diferença entre o LRR global para provas ocorridas até 5 anos antes vs provas ocorridas entre 5 à 9 anos antes, vs 10 à 14 anos antes e etc. Explicando assim, fica difícil de entender, então vamos partir para a prática.\n\n\nCódigo## construindo matriz identidade para traçar o contraste\nmatriz_contraste &lt;- diag(x = 1, nrow = length(unique(df_diffs$bin_intervalo)))\n\n## colocando 1 na primeira coluna para que todo calculo de efeito seja\n## baseado no valor do intercepto + nivel\nmatriz_contraste[, 1] &lt;- 1\n\n## calculando o efeito de cada nível\ncontrastes &lt;- anova(modelo_ma_diffs, L = matriz_contraste) \ncontrastes\n\n\nHypotheses:                                  \n1:                    intrcpt = 0 \n2:   intrcpt + bin_intervalo5 = 0 \n3:  intrcpt + bin_intervalo15 = 0 \n4:  intrcpt + bin_intervalo20 = 0 \n5:  intrcpt + bin_intervalo25 = 0 \n6:  intrcpt + bin_intervalo30 = 0 \n7:  intrcpt + bin_intervalo10 = 0 \n8:  intrcpt + bin_intervalo35 = 0 \n9:  intrcpt + bin_intervalo40 = 0 \n10: intrcpt + bin_intervalo45 = 0 \n11: intrcpt + bin_intervalo50 = 0 \n12: intrcpt + bin_intervalo55 = 0 \n13: intrcpt + bin_intervalo60 = 0 \n14: intrcpt + bin_intervalo65 = 0 \n15: intrcpt + bin_intervalo70 = 0 \n\nResults:\n    estimate     se     zval   pval \n1:   -0.0656 0.0226  -2.9089 0.0036 \n2:   -0.0170 0.0226  -0.7517 0.4523 \n3:    0.0016 0.0226   0.0704 0.9439 \n4:    0.0105 0.0226   0.4664 0.6410 \n5:   -0.0080 0.0226  -0.3550 0.7226 \n6:   -0.0270 0.0226  -1.1942 0.2324 \n7:   -0.0009 0.0226  -0.0386 0.9692 \n8:   -0.0317 0.0226  -1.4041 0.1603 \n9:   -0.1207 0.0226  -5.3464 &lt;.0001 \n10:  -0.0899 0.0226  -3.9835 &lt;.0001 \n11:  -0.2793 0.0226 -12.3768 &lt;.0001 \n12:  -0.4562 0.0226 -20.2064 &lt;.0001 \n13:  -0.5303 0.0226 -23.4955 &lt;.0001 \n14:  -0.7043 0.0226 -31.2040 &lt;.0001 \n15:  -0.5990 0.0226 -26.4830 &lt;.0001 \n\nOmnibus Test of Hypotheses:\nQM(df = 15) = 2991119.6531, p-val &lt; .0001\n\n\nA matriz de contraste acima traz alguns resultados muito interessantes e importantes para fecharmos essa estória. Ela mostra que se compararmos as provas mais recentes em cada circuito com àquelas ocorridas:\n\naté 4 anos antes naquele mesmo circuito (intrcpt), existe uma tendência das provas serem ~6% mais curtas;\n\nentre 35 à 70 anos atrás (intrcpt + bin_intervalo35 à intrcpt + bin_intervalo70, respectivamente), também podemos observar que as provas também estão mais curtas. Todavia, essa tendência varia bastante ao longo daquele intervalo: comparadas a ele, as provas estão entre ~3% (e.g., vs 30 anos atrás) à ~70% mais curtas (e.g., vs 65 anos através); e,\n\nentre 5 e 34 anos atrás (intrcpt + bin_intervalo5 à intrcpt + bin_intervalo30, respectivamente), não existe evidência de que as provas estejam ficando mais curtas. Podemos tirar essa conclusão principalmente pelo fato do intervalo de confiança da estimativa (i.e., estimate \\(\\pm\\) 1.96 \\(\\times\\) se) cruzar o valor de 0.\n\nO que estes resultados mostram é que é muito claro que as provas estão mais curtas hoje do que há 4 décadas atrás. Todavia, embora as provas estejam ligeiramente mais curtas quando olhamos o seu passado muito recente (até 4 anos), elas ainda sim têm durações semelhantes àquelas de 1 à 3 décadas atrás. Se, assim como eu, você prefere entender estes resultados visualmente, basta olhar o gráfico abaixo. Um tanto curioso este padrão, não?\n\nCódigocontrastes %&gt;% \n  unclass %&gt;% \n  keep(names(.) %in% c('hyp', 'Xb', 'se', 'pval')) %&gt;% \n  map(.f = as.data.frame) %&gt;% \n  bind_cols() %&gt;% \n  as_tibble() %&gt;% \n  set_names(nm = c('intervalo', 'estimate', 'se', 'pval')) %&gt;% \n  mutate(\n    intervalo   = str_extract(string = intervalo, pattern = '(?&lt;=bin_intervalo)[0-9]{1,2}'),\n    intervalo   = as.numeric(intervalo),\n    significant = pval &lt;= 0.05\n  ) %&gt;% \n  replace_na(replace = list(intervalo = 0)) %&gt;% \n  ggplot(mapping = aes(x = intervalo, y = estimate)) +\n  geom_hline(yintercept = 0, color = 'grey40', linetype = 2) +\n  geom_errorbar(mapping = aes(ymin = estimate - 1.96 * se, ymax = estimate + 1.96 * se),\n                width = 1) +\n  geom_point(mapping = aes(fill = significant), shape = 21, size = 2, color = 'black') +\n  scale_x_continuous(breaks = seq(from = 0, to = 70, by = 5)) +\n  scale_y_continuous(breaks = seq(from = -1, to = 0.2, by = 0.2), limits = c(-0.9, 0.2)) +\n  scale_fill_manual(values = c('grey70', 'white')) +\n  labs(\n    title    = 'Diferença entre o tempo de prova mais recente e os anteriores',\n    subtitle = 'As provas ficaram cerca de 10% à 60% mais curtas quando comparadas àquelas mesmas provas quando ocorriam 50\nanos atrás. As provas ficam um pouco mais rápidas quando comparadas ao histórico mais recente (até 5 anos antes),\nmas a duração não muda muito quando comparada à 10 ou 35 anos atrás.',\n    x        = 'Anos de Diferença',\n    y        = 'Log Response Ratio'\n  ) +\n  theme(\n    legend.position = 'none',\n    panel.grid      = element_blank(),\n    axis.line       = element_line()\n  )"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "",
    "text": "Há algum tempo atrás eu explorei o caminho para raspar os dados do ranking do BoardGameGeek1, e consolidei o passo-a-passo nesse post e script. Meu principal interesse naquele momento era que eu precisava obter o código numérico identificador de cada título a fim de poder usar esse valor quando fosse interagir com a API XML do BGG. Como o único lugar em que encontrei essa informação foi no hyperlink para a página de cada título na tabela do ranking, resolvi criar aquele scrapper.\nUma outra fonte de informação sobre jogos de tabuleiro é o site brasileiro da Ludopedia. Este portal tem muita coisa em comum com o BGG, inclusive uma API e uma página de ranking. Todavia, diferente do equivalente gringo, a Ludopedia oferece (1) uma REST API e (2) um meio mais fácil de obter o código identificador de cada título a partir da própria API. De toda forma, no momento em que escrevo este post, ainda não é possível obter as informações da página do ranking diretamente pela API. Desta forma, aqui também existe a possibilidade de exercitar um pouco o web scrapping para a extração dessa informação.\nVou aproveitar esta oportunidade para continuar construindo uma trilha a partir da qual construiremos uma base de dados que nos permitirá responder muitas outras perguntas interessantes, e aplicar técnicas bastante legais de Machine Learning. Falo mais sobre essas idéias ao final desse post."
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#footnotes",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#footnotes",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nBGG daqui para a frente.↩︎\nEssa paginação não estará evidente na primeira vez que você visitar essa página. Entretanto, se você avançar para a próxima página e depois voltar, verá que ela aparecerá na url.↩︎\nEssa informação estava dentro de um atributo chamado Last Page em uma tag div, tornando a extração da informação bem fácil.↩︎\nHavíamos raspado apenas 5 páginas do ranking do BGG, mas cada página contém informações sobre 100 jogos. Portanto, dado que cada página do ranking da Ludopedia contém as informações de 50 jogos, tivemos que raspar 10 páginas.↩︎"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#identificar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#identificar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Identificar",
    "text": "Identificar\nA primeira coisa aqui é navegar até a página-alvo e entender como funciona a sua paginação e onde está o conteúdo que queremos raspar. A figura abaixo mostra um print da primeira página do ranking, onde podemos ver a url que precisaremos visitar bem como constatar que a paginação funciona incrementando a contagem da página (i.e., pagina=1, pagina=2,…)2.\nOutro ponto importante é que as informação que queremos parecem estar em uma tabela, como foi no caso do BGG. Além disso, cada página contém 50 jogos ordenados de forma sequencial de acordo com a sua posição no ranking.\n\nCódigoinclude_graphics(path = 'images/imagem_1.jpg')"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#navegar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#navegar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Navegar",
    "text": "Navegar\nO próximo passo é olhar o fluxo de informação da página a partir da aba Network, acessível através da ferramenta Inspecionar do navegador. Podemos ver que o conteúdo que queremos raspar não é produzido a partir de nenhuma API nem nada parecido, mas totalmente disponível a partir do código HTML mesmo. Além disso, podemos ver que o conteúdo não está organizado dentro de tags de tabela em HTML, mas sim dentro de várias tags div associadas à classe pad-top. Isto já torna o parser deste scrapper diferente daquele do BGG, onde foi bastante simples tabular as informações a partir do código HTML.\n\nCódigoinclude_graphics(path = 'images/imagem_2.jpg')"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#replicar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#replicar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Replicar",
    "text": "Replicar\nVamos então tentar fazer um request da primeira página do ranking e ver o que conseguimos. Isso é feito de forma bem simples, passando apenas a url base para acessar a página e deixando o valor correspondente à página como algo a ser determinado separadamente. Faremos isso usando a função GET do pacote httr.\n\nCódigo## url base do ranking\nbase_url &lt;- 'https://www.ludopedia.com.br/ranking?pagina='\n\n# fazendo o GET\nresultado &lt;- GET(url = str_glue(base_url, 1))\nresultado\n\nResponse [https://ludopedia.com.br/ranking?pagina=1]\n  Date: 2024-02-11 12:51\n  Status: 200\n  Content-Type: text/html; charset=UTF-8\n  Size: 160 kB\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml...\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"\n      xmlns:og=\"http://ogp.me/ns#\"\n      xmlns:fb=\"https://www.facebook.com/2008/fbml\" \n      lang=\"pt-BR\"\n&gt;\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximu...\n    \n...\n\n\nApesar da forma como o conteúdo está disponível nesta página ser diferente daquele do BGG, o request em si parace também ser bem simples!"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#parsear",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#parsear",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Parsear",
    "text": "Parsear\nComo vimos anteriormente, as informações que queremos não estão formatadas e organizadas dentro de tags de tabela em HTML. Portanto, precisaremos identificar e parsear cada uma das informações que queremos usando os respectivos xpath. Para começar, podemos ver que temos acesso ao hyperlink que leva à imagem da capa do jogo se extrairmos o atributo src a partir da classe img-capa dentro da tag img. Isto pode ser uma informação legal se, depois, e.g. quisermos plotar essa imagem como uma célula em uma tabela do reactable.\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando a imagem da capa\n  xml_find_all(xpath = '//img[@class=\"img-capa\"]') %&gt;% \n  # pegando o url\n  xml_attr(attr = 'src') %&gt;% \n  # pegando a primeira observação\n  head(1) %&gt;% \n  # plotando a imagem de uma capa\n  magick::image_read() %&gt;% \n  # aumentando a resolução da imagem\n  magick::image_scale(geometry = '300')\n\n\n\n\nOutra informação legal de buscar é o hyperlink para a página de cada jogo no domínio da Ludopedia. Esta informação está dentro da tag que contém o nome do título (i.e., classe media-heading dentro do header h4), e pode ser obtida extraindo o atributo href de dentro da tag a. Como já conheço a API REST da Ludopedia, sei que essa informação pode ser útil para e.g. raspar o campo de descrição completa do jogo, a fim de utilizar esse texto em alguma análise.\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando todos os links\n  xml_find_all(xpath = 'a') %&gt;% \n  # extraindo o atributo dos hiperlinks\n  xml_attr(attr = 'href') %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \"https://ludopedia.com.br/jogo/brass-birmingham\" \n[2] \"https://ludopedia.com.br/jogo/gaia-project\"     \n[3] \"https://ludopedia.com.br/jogo/terraforming-mars\"\n[4] \"https://ludopedia.com.br/jogo/gloomhaven\"       \n[5] \"https://ludopedia.com.br/jogo/terra-mystica\"    \n[6] \"https://ludopedia.com.br/jogo/brass-lancashire\" \n\n\nA posição do ranking também pode ser extraída a partir da classe media-heading dentro do header h4, olhando a classe rank dentro da tag span…\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o ranking\n  xml_find_all(xpath = 'span[@class=\"rank\"]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \"1º\" \"2º\" \"3º\" \"4º\" \"5º\" \"6º\"\n\n\n…enquanto o nome do jogo pode ser extraído a partir do atributo title dentro da tag a…\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o nome do jogo\n  xml_find_all(xpath = 'a[@title]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \"Brass: Birmingham\" \"Projeto Gaia\"      \"Terraforming Mars\"\n[4] \"Gloomhaven\"        \"Terra Mystica\"     \"Brass: Lancashire\"\n\n\n…o ano de lançamento de cada título vêm do atributo small…\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando o ano de lançamento do jogo\n  xml_find_all(xpath = 'small') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head()\n\n[1] \" (2018)\" \" (2017)\" \" (2016)\" \" (2017)\" \" (2012)\" \" (2017)\"\n\n\n…enquanto, finalmente, todas as informações relacionadas às notas podem ser extraídas a partir da classe rank-info dentro da tag div.\n\nCódigoresultado %&gt;% \n  # pegando o conteudo\n  content() %&gt;% \n  # pegando o conteudo do titulo do mini-box\n  xml_find_all(xpath = '//h4[@class=\"media-heading\"]') %&gt;% \n  # pegando as notas do jogo\n  xml_find_all(xpath = '//div[@class=\"rank-info\"]') %&gt;% \n  # pegando o texto\n  xml_text() %&gt;% \n  # pegando algumas instancias apenas\n  head() %&gt;% \n  # tirando um pouco o excesso de whitespace\n  str_squish()\n\n[1] \"Nota Rank: 9.04 | Média: 9.15 | Notas: 1126 | Sua Nota: -\"\n[2] \"Nota Rank: 9.03 | Média: 9.12 | Notas: 1253 | Sua Nota: -\"\n[3] \"Nota Rank: 9.00 | Média: 9.03 | Notas: 3154 | Sua Nota: -\"\n[4] \"Nota Rank: 8.99 | Média: 9.09 | Notas: 1075 | Sua Nota: -\"\n[5] \"Nota Rank: 8.99 | Média: 9.03 | Notas: 2567 | Sua Nota: -\"\n[6] \"Nota Rank: 8.98 | Média: 9.16 | Notas: 619 | Sua Nota: -\" \n\n\nCom isso, temos um sashimi de parsers para pegar todas as informações que queremos a partir da página do ranking. Vamos agora consolidar esse entendimento e validá-lo na segunda página."
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#validar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#validar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Validar",
    "text": "Validar\nPara auxiliar na tarefa de raspar e parsear a segunda página do ranking, vamos definir duas funções abaixo - uma para cada tarefa. A função pega_pagina recebe a url base do ranking e o número da página que queremos raspar, fazendo então o request da página e salvando o HTML resultante em disco, no diretório definido pelo argumento path. A outra função, parser_pagina, recebe como único argumento o path para o arquivo HTML que a função pega_pagina salvou, e faz o que o próprio nome da função já diz. Ela está bem verbosa, mas o objetivo é mesmo deixar claro o que estamos fazendo.\n\nCódigo# função para fazer o GET\npega_pagina &lt;- function(url_base, pagina, save_dir) {\n  ## junta a base url com o numero da pagina e salva no diretorio alvo\n  GET(url = str_glue(url_base, pagina), \n      write_disk(path = sprintf(fmt = '%s/pagina_%03d.html', save_dir, pagina), \n                 overwrite = TRUE)\n  )\n  \n  # esperanando antes de prosseguir\n  Sys.sleep(runif(n = 1, min = 1, max = 5))\n}\n\n# função para parsear uma pagina\nparser_pagina &lt;- function(path_to_html){\n  \n  ## lendo a pagina raspada\n  pagina_raspada &lt;- read_html(x = path_to_html)\n  \n  ## infos do heading\n  media_head &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//h4[@class=\"media-heading\"]')\n  \n  ## link para a imagem da capa\n  links_da_capa &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//img[@class=\"img-capa\"]') %&gt;% \n    xml_attr(attr = 'src')\n  \n  ## link para a pagina do jogo\n  link_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'a') %&gt;% \n    xml_attr(attr = 'href')\n  \n  ## posicao do ranking de cada titulo\n  posicao_ranking &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'span[@class=\"rank\"]') %&gt;% \n    xml_text()\n  \n  ## nome do jogo\n  titulo_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'a[@title]') %&gt;% \n    xml_text()\n  \n  ## ano de lancamento do jogo\n  ano_jogo &lt;- media_head %&gt;% \n    xml_find_all(xpath = 'small') %&gt;% \n    xml_text()\n  \n  ## informacoes gerais das notas\n  notas_jogo &lt;- pagina_raspada %&gt;% \n    xml_find_all(xpath = '//div[@class=\"rank-info\"]') %&gt;% \n    xml_text()\n  \n  ## colocando rsultados numa tibble\n  tibble(\n    ranking   = posicao_ranking, \n    titulo    = titulo_jogo, \n    ano       = ano_jogo, # \n    notas     = notas_jogo,\n    link_capa = links_da_capa,\n    link_jogo = link_jogo\n  )\n}\n\n\nCom as funções definidas, agora é hora de utilizá-las! Primeiro, vamos pegar a segunda página e salvá-la em disco…\n\nCódigo# criando uma pasta para colocar os arquivos caso ela nao exista\nif(!dir_exists(path = 'temp/')){\n  dir_create(path = 'temp/')\n}\n\n# pegando a segunda pagina do ranking\npega_pagina(url_base = base_url, pagina = 2, save_dir = 'temp/')\n\n# checando para ver se o html foi baixado\ndir_ls(path = 'temp/', regexp = '.html')\n\ntemp/pagina_002.html\n\n\n…agora vamos parsear a página a partir do arquivo salvo em disco.\n\nCódigoparser_pagina(path_to_html = dir_ls(path = 'temp/', regexp = '.html'))\n\n# A tibble: 50 × 6\n   ranking titulo                                ano   notas link_capa link_jogo\n   &lt;chr&gt;   &lt;chr&gt;                                 &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 51º     Agricola (Edição Revisada)            \" (2… \"\\r\\… https://… https://…\n 2 52º     As Viagens de Marco Polo              \" (2… \"\\r\\… https://… https://…\n 3 53º     Ticket to Ride: Europa                \" (2… \"\\r\\… https://… https://…\n 4 54º     On Mars                               \" (2… \"\\r\\… https://… https://…\n 5 55º     Robinson Crusoé: Aventuras na Ilha A… \" (2… \"\\r\\… https://… https://…\n 6 56º     Ticket to Ride: Europa - 15 Anos      \" (2… \"\\r\\… https://… https://…\n 7 57º     Tiranos da Umbreterna                 \" (2… \"\\r\\… https://… https://…\n 8 58º     World Wonders                         \" (2… \"\\r\\… https://… https://…\n 9 59º     Viticulture - Edição Essencial        \" (2… \"\\r\\… https://… https://…\n10 60º     Mombasa                               \" (2… \"\\r\\… https://… https://…\n# ℹ 40 more rows\n\n\nParece que está tudo ok!"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#iterar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#iterar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Iterar",
    "text": "Iterar\nA ideia agora seria repetir o processo acima, da página 1 até a última página disponível no ranking. Lá no post sobre a raspagem do ranking do BGG vimos que poderíamos descobrir qual o número da última página a partir do próprio código HTML que era raspado3. Faremos algo bem parecido aqui, embora a informação que buscamos não esteja disponível de forma tão clara. Se inspecionarmos o código HTML da página, podemos ver que é possível extrair o número da última página através da url que está em um dos atributos da classe pagination de uma tag ul.\n\nCódigoinclude_graphics(path = 'images/imagem_3.jpg')\n\n\n\n\nPara facilitar nosso trabalho de extração dessa informação aqui, vamos criar e usar a função pega_max_paginas: ela vai olhar dentro daquela classe e extrair o href do atributo title da tag a; a partir daí vamos ter que usar um pouquinho de regex para extrair o número da página em si, uma vez que o resultado original é uma string, e o que desejamos são os números que estão após o padrão pagina=.\n\nCódigo# função para definir o número máximo de páginas para raspar\npega_max_paginas &lt;- function(url_base) {\n  GET(url = str_glue(url_base, 1)) %&gt;% \n    # pegando o conteudo do GET\n    content() %&gt;% \n    # pegando o xpath da paginacao\n    xml_find_all(xpath = '//ul[@class=\"pagination\"]//a[@title=\"Última Página\"]') %&gt;% \n    # pegando o link que contem o numero da pagina maxima\n    xml_attr('href') %&gt;% \n    # pegando o numero da pagina\n    str_extract(pattern = '(?&lt;=pagina=)([0-9]+)') %&gt;% \n    # parseando para numero\n    parse_number()\n}\n\n## definindo qual o numero maximo de paginas para pegar\nultima_pagina &lt;- pega_max_paginas(url_base = base_url)\nultima_pagina\n\n[1] 75\n\n\nComo vimos, temos 75 para raspar, o que pode demorar um pouquinho. No entanto, como a ideia aqui é ser apenas ilustrativo, vou raspar apenas as 10 primeiras páginas e deixarei uma linha comentada com o que deveria ser passado para a função walk caso quiséssemos tudo.\n\nCódigo## pegando as paginas\nwalk(\n  .x = 1:10,\n  # .x = 1:ultima_pagina, # descomentar essa linha se for para raspar tudo\n  .f = pega_pagina,\n  url_base = base_url, save_dir = 'temp/'\n)"
  },
  {
    "objectID": "posts/2021-10-24_scrapper-ludopedia/index.html#faxinar",
    "href": "posts/2021-10-24_scrapper-ludopedia/index.html#faxinar",
    "title": "Raspando a Página do Ranking da Ludopedia",
    "section": "Faxinar",
    "text": "Faxinar\nCom o HTML das páginas, agora devemos organizar e tratar os dados. Para tal, vou extrair o path de todos os arquivos HTML baixados e passá-los para a função map_dfr. Esta função vai se encarregar de aplicar a função parser_pagina ao arquivo HTML associado à cada path e retornar um único tibble com todos os resultados parseados.\n\nCódigo## pegando o path para as paginas\npath_das_paginas &lt;- dir_ls(path = 'temp/', regexp = 'html')\n\n## colocando todas as tabelas em um dataframe so\ndf &lt;- map_dfr(.x = path_das_paginas, .f = parser_pagina)\nrmarkdown::paged_table(x = df)\n\n\n\n  \n\n\n\nJá temos os dados tabulados. Vamos aplicar alguns ajustes a eles: remover o excesso de espaço em branco nas strings, separar as informações sobre as notas em diversas colunas e passar o que for numérico para tal. O código abaixo dá conta disso e nos retorna os dados do ranking tratados.\n\nCódigodf &lt;- df %&gt;% \n  mutate(\n    # parseando o ranking para numerico\n    ranking = parse_number(ranking),\n    # tratando o string titulo do jogo\n    titulo  = str_squish(string = titulo),\n    # parseando o ano para numerico\n    ano     = parse_number(ano),\n    # ajustando a string do campo de nota\n    notas   = str_squish(string = notas),\n  ) %&gt;% \n  # separando a coluna com as informacoes de nota atraves do padrao da barra\n  separate(col = notas, into = c('nota_rank', 'nota_media', 'notas', 'leftover'), sep = '\\\\|') %&gt;% \n  # tratando as informacoes da coluna separada\n  mutate(\n    # nota do ranking\n    nota_rank  = parse_number(nota_rank),\n    # nota dos usuarios\n    nota_media = parse_number(nota_media),\n    # quantidade de notas\n    notas      = parse_number(notas) \n  ) %&gt;% \n  # removendo colunas que nao serao mais necessarias\n  select(-leftover)\nrmarkdown::paged_table(x = df)\n\n\n\n  \n\n\n\nPara concluir, vamos criar uma figura para verificar a relação entre as notas do ranking da Ludopedia, a nota média dada pelos usuários e a quantidade de votos para cada jogo. Essa figura é bastante similar àquela que havíamos criado para o BGG e, inclusive, raspamos 10 páginas neste exemplo aqui justamente para colocar as duas figuras em pé de igualdade4. Apesar desta pequena diferença entre os dois portais, podemos ver padrões similares aqueles já vistos no ranking do BGG:\n\na nota média do jogo de acordo com os usuários parece ser maior do que àquelas do ranking final da Ludopedia;\n\nparece existir uma tendência aos jogos que recebem mais votos também terem maiores notas no ranking; e,\n\nParece que os jogos mais votados são aqueles com menores notas dadas pelos usuários.\n\n\nCódigodf %&gt;% \n  # renomeando as colunas para ficar mais parecido com o plot que fizemos para o BGG\n  rename(nota_ludopedia = nota_rank, nota_usuarios = nota_media, votos = notas) %&gt;% \n  # criando a figura\n  ggplot() +\n  geom_autopoint(alpha = 0.7, shape = 21, fill = 'tomato') +\n  geom_autodensity(mapping = aes(x = .panel_x, y = .panel_y), \n                   fill = 'tomato', color = 'black', alpha = 0.7) +\n  facet_matrix(rows = vars(nota_ludopedia:votos), layer.diag = 2)\n\n\n\n\nNão sei qual era a sua expectativa, mas me surpreende o fato dos padrões serem tão parecidos entre os dois portais dado a diferença que acredito existir entre os públicos brasileiros e estrangeiros."
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "",
    "text": "Uma parte chata de um esporte competitivo e de velocidade como a Fórmula 1 é o risco de ocorrência de um acidente. A história da Fórmula 1 está marcada por estes eventos, e não é preciso ir muito longe para lembrar de casos que marcaram gerações. Em resposta a esse risco, diversas intervenções têm sido implementadas no intuito de reduzir e/ou mitigar a ocorrência de acidentes e suas consequências para os pilotos - conforme informações retiradas do site F1 Insider e resumidas na figura abaixo.\n\nCódigo# carregando os pacotes necessários\nlibrary(tidyverse) # core\n# library(tidytuesdayR) # ler os arquivos do tidytuesday\nlibrary(fs) # manipular paths\nlibrary(lubridate) # trablhar com datas\nlibrary(readxl) # carregar dados de excel\nlibrary(skimr) # descricao do dataframe\nlibrary(ggupset) # para criar um upset plot\nlibrary(patchwork) # para compor figuras\n\n# carregando os dados de dispositivos de segurança na F1\nsafety &lt;- read_excel(path = 'data/f1_safety.xlsx', skip = 1)\n\n# criando a figura principal\nsafety %&gt;% \n  # prepaarando os dados para criar a figura\n  mutate(\n    # imputando o ano atual\n    Now = year(today()),\n    # ordenando as intervencoes po rano\n    Intervention = fct_reorder(.f = Intervention, .x = Year, .fun = min, .desc = TRUE)\n  ) %&gt;% \n  # criando a figura do ano a partir do qual as intervenções foram implementadas\n  ggplot(mapping = aes(x = Year, xend = Now, y = Intervention, yend = Intervention)) +\n  geom_segment() +\n  geom_point(mapping = aes(x = Year), size = 3) +\n  geom_text(mapping = aes(x = Year, label = Year), nudge_x = -4, size = 3) +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 10)) +\n  labs(\n    title    = 'Principais intervenções associadas à segurança dos pilotos ao longo dos anos',\n    subtitle = 'Houve um aumento na quantidade de intervenções relacionadas à segurança entre as décadas de 90\ne o início dos anos 2000.',\n    caption  = 'Fonte: https://f1-insider.com/en/history-of-formula-1-safety/',\n    x        = 'Ano'\n  ) +\n  theme(\n    axis.title.y = element_blank()\n  )\n\n\n\n\nDado o investimento crescente na segurança dos pilotos, uma pergunta que me veio à mente é o quanto isto está relacionado à variação na ocorrência de acidentes entre provas e temporadas. Ao que àquela figura sugere, eu esperaria que a frequência de ocorrência destes eventos estaria em uma decrescente ao longo dos anos, talvez não chegando à zero, mas tornando-os cada vez mais raros. Por sorte, podemos tentar explorar esta ideia utilizando uma base de dados disponível através de um [#TidyTuesday]^(https://github.com/rfordatascience/tidytuesday) que ocorreu há alguns meses atrás. Você pode acessar essa base de dados usando as funções do pacote tidytuesdayR ou apontando diretamente para o arquivo no GitHub; eu optei por baixar uma imagem desses dados e deixá-los disponíveis localmente1.\n\nCódigo# carregando todos os dados a partir do github do tidytuesday\n# se você quiser baixar os dados direto da fonte\n# tt_dataset &lt;- tt_load(x = 2021, week = 37)\n\n# carregando a copia local dos dados\n## extraindo os paths das copias locais\npaths_copias_locais &lt;- dir_ls(path = 'data/', regexp = '.rds')\n\n## criando vetor de nomes dos arquivos\nnomes_arquivos &lt;- paths_copias_locais %&gt;%\n  path_file() %&gt;%\n  path_ext_remove()\n\n## carregando os arquivos em uma lista\ntt_dataset &lt;- map(.x = paths_copias_locais, .f = read_rds)\n\n## renomeando os elementos da lista\nnames(tt_dataset) &lt;- nomes_arquivos\n\n\nAs informações que precisamos para analisar a ocorrência dos acidentes nas provas da Fórmula 1 estão separadas em duas tabelas. A primeira delas, results, contém os resultados e outras informações sobre cada piloto em cada prova. A segunda tabela, status, é uma base de-para que nos ajuda a converter a coluna numérica statusId, comum às duas bases, em um string associado ao que ocorreu com aquele piloto naquela prova. É nesta coluna que está a informação se cada piloto esteve envolvido ou não em um acidente em uma dada prova. Assim, agrupando as observações pelo código identificador da prova, podemos determinar se existe pelo menos um piloto que esteve envolvido em um acidente ou não em cada prova.\nUm ponto importante é que vou considerar apenas os status de Accident e Fatal accident para determinar se um piloto esteve envolvido em um acidente ou não. Até existem outras categorias de status que poderiam caracterizar estes mesmos eventos, tais como, Collision, Injury, Eye injury. No entanto, vou ficar no lado mais seguro, e assumir que apenas àquelas duas primeiras categorias de status é que caracterizam a ocorrência de um acidente. O código abaixo dá conta de juntar as duas tabelas e de definir se houve pelo menos um acidente associado à cada uma das provas.\n\nCódigo## adicionando o dicionario de resultados\nresultados &lt;- left_join(x = tt_dataset$results,\n                        y = tt_dataset$status,\n                        by = 'statusId')\n\n## vetor com as categorias que vou usar como acidente\ncategorias_acidente &lt;- c('Accident', 'Fatal accident')\n\n## juntando padrao de regex para os acidentes\nregex_acidentes &lt;- paste0(categorias_acidente, collapse = '|')\n\n## mapeando os acidentes por prova\nacidentes_por_prova &lt;- resultados %&gt;% \n  # testando se aquele padrão de regex ocorre em cada linha\n  mutate(tem_acidente = str_detect(string = status, pattern = regex_acidentes)) %&gt;% \n  # agrupando as observações pelo identificador da prova\n  group_by(raceId) %&gt;% \n  # testando se existe qualquer linha onde existe algum TRUE\n  summarise(tem_acidente = any(tem_acidente)) \ncount(acidentes_por_prova, tem_acidente, name = 'ocorrencias')\n\n# A tibble: 2 × 2\n  tem_acidente ocorrencias\n  &lt;lgl&gt;              &lt;int&gt;\n1 FALSE                607\n2 TRUE                 441\n\n\nA base de dados sobre a ocorrência de acidentes ao nível das provas é levemente desbalanceada, uma vez que existem 607 provas sem a ocorrência de um acidente contra 441 onde eles ocorreram. Mas com que frequência estes acidentes foram registrados ao longo das temporadas da Fórmula 1 comparado ao aumento nas medidas de segurança para preveni-los?\n\nCódigo# calculando o volume acumulado de medidas de seguranca\nacumulado_medidas &lt;- tibble(\n  # criando uma sequência completa de anos\n  year = seq(from = 1950, to = 2020, by = 1)\n) %&gt;% \n  # juntando com as informacoes das medidas de seguraca por ano\n  left_join(y = safety %&gt;% \n              distinct(Year) %&gt;% \n              mutate(medida = 1), \n            by = c('year' = 'Year')) %&gt;% \n  # adicionando um contador zero para ajudar a fazer a soma acumulada\n  replace_na(replace = list(medida = 0)) %&gt;% \n  # calculando o volume acumulado de medidas de seguranca existentes por temporada\n  mutate(n_medidas = cumsum(medida))\n\n# criando a figura\n## juntando as informacoes da ocorrencia de acidentes por prova com o ano em que a prova ocorreu\nleft_join(x = acidentes_por_prova,\n          y = select(tt_dataset$races, raceId, year),\n          by = 'raceId') %&gt;% \n  # agrupando pela temporada\n  group_by(year) %&gt;% \n  # calculando a proporcao de provas com acidentes por temporada\n  summarise(proporcao = mean(tem_acidente)) %&gt;% \n  # juntando informacoes da quantidade acumulada de medidas de seguranca existentes por temporada\n  left_join(y = acumulado_medidas, by = 'year') %&gt;% \n  # criando a figura\n  ggplot() +\n  geom_line(mapping = aes(x = year, y = n_medidas / 20), color = '#3399E6', linewidth = 1) +\n  geom_line(mapping = aes(x = year, y = proporcao), color = 'tomato1', linewidth = 1) +\n  scale_x_continuous(breaks = seq(from = 1950, to = 2020, by = 5)) +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ . * 20, \n                                         name = 'Quantidade de medidas de segurança existentes')) +\n  labs(\n    title    = 'Histórico da proporção de provas com acidente e medidas de segurança existentes por temporada',\n    subtitle = 'A frequência de ocorrência de acidentes caiu no final da década de 80 e se manteve assim até o início dos anos 2000, quando ela novamente voltou a aumentar um pouco.\\nNo mesmo período, uma número cada vez mais de medidas de segurança têm sido implementado.',\n    x        = 'Ano/Temporada',\n    y        = 'Proporção de provas com acidente na temporada'\n  ) +\n  theme(\n    axis.title.y.left  = element_text(colour = 'tomato1'),\n    axis.text.y.left   = element_text(colour = 'tomato1', face = 'bold'),\n    axis.title.y.right = element_text(colour = '#3399E6'),\n    axis.text.y.right = element_text(colour = '#3399E6', face = 'bold')\n  )\n\n\n\n\nAtravés da figura acima podemos observar que acidentes eram muito comuns na Fórmula 1 até o final da década de 80, quando eles despencaram2. A partir daí, eles se mantiveram em baixa até meados dos anos 2000, quando eles voltaram a subir, se mantendo em um patamar entre 2003 à 2012 e outro até hoje. Por outro lado, o número de medidas de segurança só têm aumentado, principalmente durante o período entre 1990 e 2005, quando elas praticamente dobraram. Ainda assim, a frequência de ocorrência de acidentes não zerou e, tampouco, continuou apresentando uma tendência de queda com o aumento no número de medidas de segurança nos últimos anos.\nDado os padrões observados, a ocorrência de acidentes parece não estar associada simplesmente à quantidade ou natureza das medidas de segurança implementadas. Isto abre a possibilidade para que outros fatores associados às provas e/ou às temporadas também estejam contribuindo para a variação na frequência destes eventos. Neste contexto, que outros fatores poderiam ser estes? Se nós os conhecermos, seria possível determinar a probabilidade e/ou o potencial de que estes eventos ocorram? São essas as perguntas que eu buscarei abordar aqui[Só para frisar: não é minha intenção prever acidentes! Meu intuito principal aqui será olhar para o histórico dos eventos e tentar entender o conjunto de fatores que parece estar mais frequentemente associado à sua ocorrência. Gerar predições que impeçam estes eventos de ocorrerem está fora do escopo deste post.].[Disclaimer: eu já fui muito fã de Fórmula 1, mas deixei de acompanhar há muito tempo atrás. Neste contexto, é muito provável que eu deixe de fora das minhas análises algumas informações muito valiosas ou de conhecimento geral para os fãs mais assíduos. Assim, peço desculpa de antemão caso isso ocorra e, reforço, que as análises aqui apresentadas estão mais orientadas ao exercício do que algo que seja para valer.]"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#footnotes",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#footnotes",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nUma diferença dos dados que estão disponíveis lá no tidytuesday é que eu acabei criando mais uma tabela com informações adicionais sobre cada prova, que estavam disponíveis na Wikipedia - mas explico sobre isso mais adiante.↩︎\nCoincidentemente ou não, isto parece ter ocorrido um pouco depois que os crash tests foram implementados.↩︎\nUma exceção seria, por exemplo, uma análise orientada à geração de entendimento do fenômeno ou causal, mas aí é outro assunto.↩︎\nE como fazer no caso da primeira prova da temporada, quando não há prova anterior para definir a intensidade da competição? Aqui eu optei por imputar o valor 0, a fim de representar o fato de que a competição estaria em pé de igualdade no início de cada temporada.↩︎\nNo meu uso do tidymodels, acabei vendo que quando passava o 0 ou o 1 como fator e pedia para calcular a sensibilidade, ele estava calculando a especificidade. Por outro lado, quando usava a função que retorna a especificidade, ela me retornava o valor que eu esperava para a sensibilidade. Assim, no código abaixo eu estou invocando a função specificity ou specificity_vec, mas o que ela está me retornando é a métrica de sensibilidade. Examinando os argumentos da função, posteriormente, vi que esse problema seria facilmente resolvido setando alguns dos parâmetros ali, mas optei por não fazer isso e deixar como já estava.↩︎\nSó para lembrar, temos 607 instâncias de provas sem acidentes e 441 provas com acidentes.↩︎\nExistem muito mais hiperparâmetros disponíveis na implementação original do pacote ranger no R e no Scikit-Learn do Python, mas apenas estes três estão disponíveis dentro do wrapper do tidymodels↩︎\ne.g., já ouvi uma palestra uma vez falando da resistência que os desenvolvedores tinham em criar o pacote workflowsets, relacionado ao risco de transformar o tidymodels em algo que você só fica ajustando algoritmo sem pensar no que está fazendo.↩︎"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#considerações-gerais",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#considerações-gerais",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Considerações gerais",
    "text": "Considerações gerais\nDefinir precisamente a pergunta que queremos responder e/ou o objetivo que queremos alcançar é o passo principal para a criação da base analítica correta: é isto que define qual será a variável resposta bem como a sua granularidade. No caso apresentado aqui, buscarei definir a probabilidade de ocorrência de um acidente em cada uma das provas da Fórmula 1 no registro histórico. Como consequência, precisarei montar uma base analítica na qual tenhamos uma observação por prova (i.e., a granularidade) e, associado à elas, um indicador se houve ou não um acidente (i.e., a variável resposta).\nO segundo passo importante é definir quais variáveis utilizar para auxiliar na tarefa de previsão. Meu ponto com isto não está na seleção de variáveis per se, mas sim na criação de variáveis para ajudar a endereçar a pergunta e/ou objetivo proposta. É bem relevante traçar essa diferença, pois é bastante comum encontrar bases de dados prontas para ajustar os algoritmos mas, na vida real, é muito raro encontrar essas coisas já desenhadas para nós. Além disso, eu tenho a impressão de que raramente refletimos sobre o processo de ideação que levou àquelas variáveis à estarem naquela tabela. Isso daria uma discussão longa por si só, mas gostaria de aproveitar a oportunidade para comentar sobre três coisas relevantes nesta tarefa. Todas elas parecerão óbvias mas, novamente, acho que de tempos em tempos é relevante relembrarmos destas coisas.\n\nAs variáveis criadas precisam estar na mesma granularidade da variável resposta. Isto significa que as variáveis que vou utilizar precisam descrever características ao nível de cada prova. Se eu estivesse buscando gerar previsões para a temporada, então faria mais sentido que as variáveis utilizadas descrevessem características relacionadas à cada temporada. Em outras palavras, as variáveis devem sempre descrever aquilo que estamos buscando analisar;\n\nAs variáveis utilizadas para a previsão precisam estar disponíveis antes que o fenômeno analisado ocorra. Isto é, se queremos antecipar a probabilidade de ocorrência de um acidente em uma prova, não podemos utilizar informações que estariam disponíveis apenas após a conclusão daquela prova3. Relacionado a este ponto, também é importante estar atento para não incluir variáveis que estão obviamente confundidas com aquilo que queremos prever. Por exemplo, a ocorrência de acidentes pode colocar o safety car na pista, de forma que a distância percorrida pelos pilotos em um circuito ou o tempo de prova podem acabar aumentando. Neste caso, ambas as informações estariam disponíveis apenas após a conclusão da prova e, também, confundidas com aquilo que queremos prever. Assim, estas duas variáveis teriam um potencial bastante pequeno de fornecer insights úteis para a análise.\nFinalmente, é muito importante que as variáveis criadas estejam bem embasadas em hipóteses, perguntas e expectativas pré-definidas. Em outras palavras, nós não saímos jogando variáveis dentro de uma base analítica só porque podemos fazer isso. É preciso pensar sobre o que elas estão representando, de que forma podem estar contribuindo para o fenômeno analisado e, também, quão razoável é a relação que imaginamos existir. Dificilmente conseguiremos representar uma relação complexa entre um padrão e um processo usando uma única variável; se o objetivo da análise for a predição e você se encontrar em uma situação como essa, talvez valha a pena redesenhar a pergunta ou a abordagem que está sendo seguida.\n\nCom estas considerações em mente, vou criar algumas variáveis que estarão associadas direta ou indiretamente à hipóteses e perguntas relacionadas à ocorrência dos acidentes em cada prova."
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#criação-das-variáveis",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#criação-das-variáveis",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Criação das variáveis",
    "text": "Criação das variáveis\nComeço falando de três hipóteses relacionadas à informações que não temos na base de dados original. Uma hipótese é de que provas que ocorram sob condições do tempo mais chuvosas ou similares têm maior probabilidade de ter um acidente; o porquê disso me parece bem intuitivo, então não vou me ater muito aqui. Uma segunda hipótese é que provas que ocorram fora dos autódromos (e.g., em circuitos de rua ou adaptados) possuem maior chance de ocorrência de um acidente, talvez por não possuírem a infraestrutura necessária para preveni-los ou por apresentarem uma rota muito sinuosa e errática. Finalmente, uma outra hipótese seria que provas em circuitos mais longos teriam maior probabilidade de ocorrência de um acidente, uma vez que e.g. dirigir nestes circuitos seria mais cansativo. Apesar das informações relacionadas à estas hipóteses não estarem na base original, é muito fácil obtê-las a partir da Wikipedia.\nPara extrair àquelas informações da Wikipedia utilizei a url da Wikipedia que está relacionada à cada prova, e disponível na tabela races. Eu criei um scrapper para ir até uma dessas páginas e extrair as informações e metadados sobre cada cada uma das provas. A partir daí eu fiz um parser e montei a base de dados que ajusto abaixo. Você encontra esse scrapper dentro da pasta deste post, neste link.\n\ntempo_durante_prova &lt;- tt_dataset$weather_and_other %&gt;% \n  # removendo a coluna de voltas, pois ja temos ela na base de dados\n  select(-voltas) %&gt;% \n  # agrupando operacao seguinte linha a linha\n  rowwise(raceId) %&gt;% \n  # ajustando features relacionadas ao tempo durante a prova\n  mutate(\n    # flag para quando a informação estiver indisponivel\n    ausente = as.numeric(is.na(sum(c_across(nublado:quente)))),\n    # flag para quando o tempo nao for nenhuma das outras alternativas \n    outro   = as.numeric(sum(c_across(nublado:quente)) == 0)\n  ) %&gt;%\n  # desagrupando a estrutura rowwise da tabela\n  ungroup() %&gt;% \n  # preenchendo os NAs com 0 apenas nas colunas com as flags do tempo durante a prova\n  mutate(across(nublado:outro, \\(x) replace_na(data = x, replace = 0)))\nrmarkdown::paged_table(x = tempo_durante_prova)\n\n\n\n  \n\n\n\nOutra coisa que poderia estar associada à ocorrência de acidentes está na própria intensidade e assimetria da disputa pelo campeonato em cada temporada. Por exemplo, poderia ser o caso de que observemos uma maior frequência de acidentes quando a disputa pelo título da temporada estiver mais intenso. Por outro lado, também seria razoável imaginar que quando os pilotos diferem muito em sua habilidade e, consequentemente, na pontuação obtida ao longo da temporada, maior seria a probabilidade de ocorrência de um acidente. Neste caso, isto ocorreria pois os pilotos menos habilidosos acabariam ‘atrapalhando’ os demais, provocando e/ou os envolvendo em mais acidentes. Finalmente, também é possível que a ocorrência de acidentes esteja associada à combinação entre estes dois fatores: pilotos muito habilidosos e competitivos ousando na pista e pilotos menos habilidosos só atrapalhando\nVou representar estas hipóteses utilizando as informações sobre a pontuação de cada piloto existente na tabela driver_standings. Esta tabela traz o somatório das pontuações dos pilotos em cada prova da temporada e, portanto, nos dá uma ideia da evolução da competição pelo campeonato prova a prova. Com base nestes dados, calculei o coeficiente de variação da pontuação entre os cinco pilotos mais bem posicionados à cada rodada em cada temporada e, também, o coeficiente de variação da pontuação entre todos os pilotos em cada rodada de cada temporada. Em ambos os casos, um coeficiente de variação menor do que 1 sugere que diferenças na pontuação entre pilotos é muito pequena e, portanto, a intensidade da competição é mais forte e simétrica, enquanto valores maiores que 1 indicam o contrário.\nUm detalhe importante aqui é que estas variáveis estariam disponíveis apenas após a conclusão de cada prova. No entanto, a ideia é que estas variáveis sejam utilizadas para representar a intensidade e assimetria da competição antes de cada prova iniciar. Assim, vou utilizar o coeficiente de variação da pontuação até a prova anterior como input para caracterizar a intensidade da competição na prova atual. Isto é, a intensidade e assimetria da competição prova i será caracterizada pela informação disponível até a prova i - 1, garantindo assim que tenhamos acesso à informação antes de cada prova se iniciar4.\n\n## juntando as informacoes das provas com a pontuacao por piloto por prova, round e temporada\ndisputa_por_prova &lt;- left_join(x = tt_dataset$driver_standings,\n                               y = select(tt_dataset$races, raceId, year, round),\n                               by = 'raceId') %&gt;% \n  # organizando pontuação por ano e prova em ordem decrescente, da primeira à última prova\n  arrange(year, round, desc(points)) %&gt;% \n  # agrupando por ano e prova\n  group_by(year, raceId) %&gt;% \n  # criando uma mascara para nos ajuda a calcular as metricas apenas considerando \n  # a pontuacao dos pilotos no topo do ranking da temporada por prova e ano\n  mutate(\n    mask_positions = ifelse(test = row_number() &lt;= 5, yes = 1, no = NA)\n  ) %&gt;%\n  # sumarizando metricas para representar força da disputa à cada prova e ano\n  summarise(\n    # coeficiente de variação da pontuação dos cinco pilotos com mais pontos à \n    # cada prova concluída em cada temporada\n    cv_top_pilots = sd(points * mask_positions, na.rm = TRUE) / mean(points * mask_positions, na.rm = TRUE),\n    # coeficiente de variação da pontuação de todos os pilotos à cada prova\n    # concluída em cada temporada\n    cv_all_pilots = sd(points) / mean(points), \n    .groups = 'drop'\n  ) %&gt;% \n  # garantindo ordenamento por ano e prova\n  arrange(year, raceId) %&gt;% \n  # agrupando por ano\n  group_by(year) %&gt;% \n  # imputando o coeficiente de variacao das metricas da corrida prova anterior \n  # para prever o resultado da corrida atual - não dá para usar o dado de uma\n  # informação obtida após a prova para prever o que \n  # acontecerá durante a prova\n  mutate(cvl_top_pilots = lag(cv_top_pilots), cvl_all_pilots = lag(cv_all_pilots)) %&gt;% \n  # dropando o grupo\n  ungroup %&gt;% \n  # substituindo os NAs por zero - no inicio de cada temporada não houve nenhuma\n  # prova anterior e, portanto, vamos assumir que não há uma competição intensa \n  # neste momento\n  replace_na(replace = list(cvl_top_pilots = 0, cvl_all_pilots = 0)) %&gt;% \n  # dropando a coluna de ano\n  select(-year)\nrmarkdown::paged_table(x = disputa_por_prova)\n\n\n\n  \n\n\n\nTambém aproveito os dados disponíveis para criar um outro conjunto de variáveis que podem estar relacionadas à frequência de ocorrência de acidentes. Mais especificamente, estas variáveis ajudam a respoder algumas perguntas, tais como:\n\nComo a quantidade de participantes (i.e., pilotos e construtores) contribui para a ocorrência de acidentes?\n\nProvas com mais voltas têm maior frequência de acidentes?\n\nA frequência de acidentes está relacionada ao período do ano (i.e., o mês) ou do dia (i.e., manhã ou tarde) em que as provas são realizadas?\n\nExistem diferenças entre os circuitos em termos da probabilidade de ocorrência de um acidente?\n\nO pedaço de código abaixo cria estas variáveis e consolida muitas outras relacionadas à estas mesmas perguntas.\n\n## quantidade de pilotos e construtores por prova\nparticipantes_por_prova &lt;- resultados %&gt;% \n  # pegando os valores unicos das chaves primarias por prova\n  distinct(raceId, driverId, constructorId) %&gt;% \n  # agrupando por prova\n  group_by(raceId) %&gt;% \n  # quantidade de pilotos e construtores por prova\n  summarise(\n    n_pilotos      = n_distinct(driverId),\n    n_construtores = n_distinct(constructorId)\n  )\n\n## calculando a quantidade de voltas em cada prova\nvoltas_por_prova &lt;- resultados %&gt;% \n  # considerando apenas os pilotos que concluiram cada prova\n  filter(status == 'Finished') %&gt;% \n  # selecionando as colunas de interesse\n  select(raceId, laps) %&gt;% \n  # pegando o valor maximo da quantidade de voltas por prova\n  group_by(raceId) %&gt;% \n  summarise(laps = max(laps))\n\n## mapeando cada circuito à uma prova\nprovas &lt;- left_join(x = tt_dataset$races,\n                    y = tt_dataset$circuits,\n                    by = 'circuitId') %&gt;% \n  # removendo URL da wikipedia\n  select(-contains('url'), -circuitRef, -circuitId, -round) %&gt;%\n  # renomeando o nome do GP e do circuito\n  rename(gp = name.x, circuit = name.y) %&gt;% \n  # levantando informacoes de data\n  mutate(\n    data     = paste(date, time),\n    data     = as_datetime(data),\n    mes      = month(data),\n    semana   = isoweek(x = data),\n    turno_pm = pm(data),\n    decada   = (year %/% 10) * 10\n  ) %&gt;% \n  select(-date, -time)\n\nUma vez que tenhamos calculado todas as variáveis para ajudar na modelagem, vou juntá-las em uma tabela só. Farei isso usando um left_join entre todas as bases, com a raceId (i.e., identificador único de cada prova) como chave primária para todas as tabelas.\n\n## juntando informacoes das provas com a quantidade de participantes e construtores\nfeatures_por_prova &lt;- left_join(x = provas,\n                             y = participantes_por_prova,\n                             by = 'raceId') %&gt;% \n  ## juntando de voltas por prova\n  left_join(y = voltas_por_prova, by = 'raceId') %&gt;% \n  ## juntando informacoes da intensidade da disputa\n  left_join(y = disputa_por_prova, by = 'raceId') %&gt;% \n  ## juntando informacoes do tempo em cada prova\n  left_join(y = tempo_durante_prova, by = 'raceId')\nrmarkdown::paged_table(x = features_por_prova)\n\n\n\n  \n\n\n\nPara concluir a preparação da base analítica, vou usar mais um left_join para juntar as observações da variável resposta (i.e., ocorrência ou não de um acidente em cada prova) com as informações das variáveis preditoras (i.e., todas as informações que caracterizam cada uma das provas).\n\ndf &lt;- left_join(x = acidentes_por_prova, y = features_por_prova, by = 'raceId')\nrmarkdown::paged_table(x = df)"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#definição-das-métricas",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#definição-das-métricas",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Definição das métricas",
    "text": "Definição das métricas\nO objetivo aqui é gerar um modelo que seja capaz de definir a probabilidade de ocorrência de um acidente em uma prova. Além disso, gostaria que este modelo fosse tão bom em dizer quando esperar que um acidente ocorra quanto quando ele não ocorra. Finalmente, é importante que este modelo seja capaz de prever corretamente as vezes nas quais um acidente ocorrerá. Dado estes três requisitos requisitos, vou focar em avaliar o modelo através dos valores de AUC. Esta métrica representa o trade-off entre a sensibilidade (i.e., capacidade de prever os acidentes que ocorreram) e a especificidade (i.e., capacidade de prever os acidentes que não ocorreram), e é baseada nas probabilidades que o modelo gera. Esta métrica também pode ser interpretada como a probabilidade de que uma observação amostrada aleatoriamente a partir das previsões do modelo esteja certa ao dizer que haverá um acidente. Também aproveitarei para monitorar o log loss (i.e., representa quão bem calibradas as probabilidades geradas pelo modelo estão) e a sensibilidade (i.e., métrica baseada na classificação binária, obtida após a conversão das probabilidades em uma decisão)5.\n\nCódigo# carregando os pacotes para a analise de dados\nlibrary(tidymodels) # core para ajustar os modelos\nlibrary(finetune) # selecao de hiperparametros\n\n# definindo as tres metricas que vamos monitorar: log loss, AUC e especificidade\nmetricas &lt;- metric_set(roc_auc, mn_log_loss, specificity)"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#separação-dos-dados",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#separação-dos-dados",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Separação dos dados",
    "text": "Separação dos dados\nVou começar separando a base de treino da base de teste, separando 75% dos dados para a primeira e o restante para a segunda. Vou aproveitar e estratificar esta separação, garantindo que haverá a mesma proporção de provas com e sem acidentes entre as duas bases.\n\nCódigo# passando o target para um fator e a variavel de turno para numerico\ndf &lt;- mutate(df, \n             tem_acidente = as.factor(ifelse(test = tem_acidente, yes = 1L, no = 0L)),\n             turno_pm     = as.numeric(turno_pm)\n)\n\n# definindo a seed para a reprodutibilidade\nset.seed(64)\n# fazendo o split da base em treino e teste\ntrain_test_split &lt;- initial_split(data = df, prop = 0.75, strata = tem_acidente)\ntrain_test_split\n\n&lt;Training/Testing/Total&gt;\n&lt;785/263/1048&gt;\n\n\nComo vou usar uma validação cruzada para avaliar a performance do modelo, faço a separação da base de treino em 10 folds abaixo - novamente, estratificando a separação de forma que tenhamos uma proporção similar de provas com e sem acidentes entre os folds.\n\nCódigo# definindo a seed para a reprodutibilidade\nset.seed(128)\n# criando o esquema para a validação cruzada\nkfold &lt;- vfold_cv(data = training(x = train_test_split), v = 10, strata = tem_acidente)\nkfold\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [706/79]&gt; Fold01\n 2 &lt;split [706/79]&gt; Fold02\n 3 &lt;split [706/79]&gt; Fold03\n 4 &lt;split [706/79]&gt; Fold04\n 5 &lt;split [706/79]&gt; Fold05\n 6 &lt;split [707/78]&gt; Fold06\n 7 &lt;split [707/78]&gt; Fold07\n 8 &lt;split [707/78]&gt; Fold08\n 9 &lt;split [707/78]&gt; Fold09\n10 &lt;split [707/78]&gt; Fold10"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-dos-dados-1",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-dos-dados-1",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Preparação dos dados",
    "text": "Preparação dos dados\nO próximo passo é preparar uma receita que vai definir a fórmula do modelo (i.e., variável resposta e preditoras) bem como todas as etapas de pré-processamento dos dados. Os passos principais dessa receita são tratar a coluna raceId como a chave primária da base, criar uma feature de mês a partir da coluna de data, remover colunas que não serão usadas e fazer um one-hot-encoding das variáveis categóricas de identidade do circuito e mês. Nesse último ponto, aproveito também para criar um nível para os níveis não vistos da variável de circuito (útil quando um nível está no fold de treino mas não no de validação) e para agregar os níveis menos frequentes desta variável em um nível único.\n\nCódigopre_processamento &lt;- recipe(tem_acidente ~ ., data = training(x = train_test_split)) %&gt;%\n  # passando a raceId para a role de id da base\n  update_role(raceId, new_role = 'id') %&gt;%\n  # adicionando o mes\n  step_date(data, features = 'month', abbr = FALSE) %&gt;% \n  # removendo as colunas que nao sao necessarias para a modelagem\n  step_rm(c(gp, location:data, semana, distancia, decada, \n            cv_top_pilots, cv_all_pilots, n_construtores, mes)) %&gt;%\n  # criando um step para associar circuitos nao vistos a um novo valor\n  step_novel(circuit) %&gt;%\n  # agrupando circuitos menos frequentes\n  step_other(circuit, threshold = 5, other = 'Other') %&gt;%\n  # fazendo o one hot encoding dos circuitos\n  step_dummy(circuit, data_month, one_hot = FALSE)\npre_processamento"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#definição-do-baseline",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#definição-do-baseline",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Definição do baseline",
    "text": "Definição do baseline\nÉ sempre importante definirmos tantos baselines quantos forem necessários, a fim de verificar se a solução que estamos buscando desenvolver agrega mais valor do que uma regra de negócio ou uma abordagem mais simples. Relacionado a este último ponto, um modelo preditivo em sua versão default pode ser utilizado como um baseline para determinar se o esforço de tunagem de hiperparâmetros está resultando em um ganho de performance ou não. Neste contexto, vou traçar dois baselines aqui antes de implementar a tunagem de hiperparâmetros.\nNaive\nA primeira abordagem será muito simples, e seguirá uma regrinha bastante comum para problemas de classificação quando temos uma base de dados desbalanceada6: prever que todas as instâncias descrevem a categoria majoritária. Para isso, vou desempacotar os dados de cada fold e criar uma coluna toda cheia de zeros (i.e., a representação das provas sem acidente em nossa base de dados); com isso, calcularei então o valor das métricas que serão monitoradas para cada fold e, então, tirar a média destes valores entre todos eles. Os valores obtidos com isso representam o baseline das nossas métricas de classificação se seguíssemos uma regra de negócio que diria que não haverá acidente em nenhuma prova.\n\nCódigokfold %&gt;% \n  # extraindo o target a partir de cada fold\n  mutate(\n    obs_target = map(.x = splits, .f = ~ training(.x) %&gt;% pull(tem_acidente))\n  ) %&gt;% \n  # pegando so o id do fold e a list column com o target\n  select(id, obs_target) %&gt;% \n  # desempacotando os targets\n  unnest(obs_target) %&gt;% \n  # criando colunas com a previsao naive a fim de definir um baseline sem um modelo\n  # nesse caso, vamos sempre prever que ocorrera um acidente em cada prova, independentemente\n  # de qualquer informacao que tenhamos sobre elas\n  mutate(\n    # criando um target numerico para que ele seja usado para calcular o auc e log loss\n    prd_target_num = 0, \n    # criando um target no formato fator para calcular a especificidade\n    prd_target_fct = factor(x = 0L, levels = c(0L, 1L)),\n    # target\n  ) %&gt;% \n  # agrupando pelo id do fold\n  group_by(id) %&gt;% \n  # calculando cada uma das metricas para cada fold\n  summarise(\n    log_loss       = mn_log_loss_vec(truth = obs_target, estimate = prd_target_num),\n    auc            = roc_auc_vec(truth = obs_target, estimate = prd_target_num),\n    sensibilidade  = specificity_vec(truth = obs_target, estimate = prd_target_fct),\n    .groups = 'drop'\n  ) %&gt;% \n  # tirando a media de cada metrica entre todos os folds\n  summarise(\n    across(log_loss:sensibilidade, mean)\n  )\n\n# A tibble: 1 × 3\n  log_loss   auc sensibilidade\n     &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1     20.9   0.5             0\n\n\nModelo\nO segundo baseline que vou definir será através de um algoritmo em sua versão default. Vou focar em um algoritmo de Random Forest neste post, só para praticar mesmo. Este algoritmo é bastante popular e, normalmente, consegue uma performance razoável em problemas de classificação e regressão dado a baixa complexidade envolvida em seu processamento. A ideia geral deste algoritmo é gerar uma previsão com base em um ensemble de árvores de decisão (daí a parte Forest no nome do algoritmo), cada uma das quais é crescida com base em um subconjunto diferente das instâncias e das variáveis disponíveis (daí a parte Random no nome do algoritmo).\nNo código abaixo, instanciamos a Random Forest utilizando a engine do pacote ranger e, então, consolidamos um workflow onde os dados de cada fold serão pré-processados utilizando a receita que havíamos preparado anteriormente. Na sequência, vamos fazer este ajuste e coletar a performance da Random Forest default entre todos os folds da validação cruzada.\n\nCódigo# cria uma instancia do algoritmo com valores default\nalgoritmo_baseline &lt;- rand_forest() %&gt;%\n  set_engine(engine = 'ranger', importance = 'impurity') %&gt;%\n  set_mode(mode = 'classification')\n\n# define um workflow com a receita do pre-processamento e o algoritmo\nwf_baseline &lt;- workflow() %&gt;% \n  add_recipe(recipe = pre_processamento) %&gt;% \n  add_model(spec = algoritmo_baseline)\n\n# setando a seed para a reprodutibilidade\nset.seed(256)\n\n# ajusta o workflow do baseline aos folds\nfit_baseline &lt;- wf_baseline %&gt;% \n  fit_resamples(resamples = kfold, \n                metrics = metricas, \n                control = control_resamples(verbose = TRUE, allow_par = TRUE)\n  )\n\n# extrai as metricas do baseline\ncollect_metrics(x = fit_baseline)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary     0.603    10  0.0105 Preprocessor1_Model1\n2 roc_auc     binary     0.730    10  0.0125 Preprocessor1_Model1\n3 specificity binary     0.597    10  0.0136 Preprocessor1_Model1\n\n\nFica muito claro que o modelo preditivo supera e muito o baseline definido usando a regra de negócio anterior. Posto de outra forma, os valores obtidos aqui sugerem que um modelo de aprendizado de máquina pode agregar valor para tentar antecipar quando existe um maior potencial de ocorrência de acidentes em uma prova. Como este baseline pronto, vamos agora à tunagem dos hiperparâmetros da Random Forest, na tentativa de melhorar um pouco mais a sua performance."
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-do-algoritmo",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#preparação-do-algoritmo",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Preparação do algoritmo",
    "text": "Preparação do algoritmo\nVou criar uma instância da Random Forest para que façamos a seleção de hiperparâmetros. Para tal, vou passar o argumento tune() para os três hiperparâmetros disponíveis na implementação do algoritmo no tidymodels7. Além disso, precisamos finalizar o desenho do hiperparâmetro mtry antes de consolidar o algoritmo e a receita de pré-processamento em um workflow.\n\nCódigo# cria uma instancia do algoritmo com hiperparametros a serem tunados\nalgoritmo_tuning &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%\n  set_engine(engine = 'ranger', importance = 'impurity') %&gt;%\n  set_mode(mode = 'classification')\n\n# concluindo a preparacao dos ranges de hiperparametros que serao testados\nhiperparametros &lt;- extract_parameter_set_dials(algoritmo_tuning) %&gt;% \n  update(mtry = mtry(range = c(1L, 25L)))\n\n# define um workflow com a receita do pre-processamento e o algoritmo para ser tunado\nwf_tuning &lt;- workflow() %&gt;% \n  add_recipe(recipe = pre_processamento) %&gt;% \n  add_model(spec = algoritmo_tuning)"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#seleção-de-hiperparâmetros",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#seleção-de-hiperparâmetros",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Seleção de hiperparâmetros",
    "text": "Seleção de hiperparâmetros\nCom tudo pronto, agora é fazer a seleção de hiperparâmetros. Para isso, vou utilizar a técnica de anilhamento simulado (i.e., Simulated Annealing). Este é um algoritmo de otimização estocástica global, que utiliza um pouco de aleatoriedade como parte do processo de busca pela solução final (neste caso específico, àquela que oferecer o maior valor de AUC). Mais especificamente, este algoritmo funciona buscando na vizinhança de uma solução inicial outras alternativas que possam melhorar a sua performance, podendo, inclusive, aceitar soluções sub-ótimas com uma determina probabilidade. Esta probabilidade é alta nas primeiras iterações do algoritmo e vai caindo conforme a busca progride, de forma que esta taxa de decaimento é também um hiperparâmetro do algoritmo de busca que podemos modificar. De toda forma, este procedimento colabora para que o algoritmo localize a região do mínimo global de forma mais rápida, escapando de mínimos locais e passando a explorar outras áreas do espaço de busca que uma busca determinística não faria.\nVou gerar 10 iterações iniciais para alimentar o motor de busca do algoritmo e, então, realizar a busca dos hiperparâmetros que maximizem o AUC através de 50 iterações. Além disso, também vou permitir que o algoritmo teste novas combinações de hiperparâmetros caso nenhuma combinação testada durante 5 iterações consecutivas tragam resultados melhores do que aquele já existente.\n\nCódigo# setando a seed para a reprodutibilidade\nset.seed(512)\n\n# ajusta o workflow do baseline aos folds\nfit_tuning &lt;- wf_tuning %&gt;% \n  tune_sim_anneal(\n    resamples  = kfold,\n    metrics    = metricas,\n    param_info = hiperparametros,\n    iter       = 50,\n    initial    = 10,\n    control    = control_sim_anneal(verbose = TRUE, restart = 5)\n  )\n\n\nPodemos ver de que forma a otimização de hiperparâmetros progrediu ao longo das iterações na figura abaixo. O painel A demonstra que o hiperparâmetro que define o número de árvores tendeu a uma convergência bastante clara ao longo das iterações, enquanto isto não foi tão bem o caso para os hiperparâmetros que definem o número de variáveis preditoras disponíveis para a construção de cada árvore e o número mínimo de instâncias no ramo final das árvores. Já o painel B demonstra que o algoritmo tendeu a convergir rapidamente para um valor de AUC próximo à 0.75, com a performance de alguns folds chegando próximo ao patamar de AUC de 0.78; por outro lado, é possível ver também que a performance dos hiperparâmetros utilizados em algumas iterações foi bem ruim.\n\nCódigo# hiperparametros utilizados por iteracao\nparams &lt;- autoplot(object = fit_tuning, metric = 'roc_auc', type = 'parameters') +\n  labs(y = 'Valor do hiperparâmetro') +\n  theme(axis.title.x = element_blank())\n# evolucao da performance por iteracao\nperf &lt;- autoplot(object = fit_tuning, metric = 'roc_auc', type = 'performance') +\n  labs(x = 'Iteração', y = 'AUC')\n# compondo o plot\n(params / perf) +\n  plot_annotation(title = 'Evolução da seleção de hiperparâmetros por iteração do Simulated Annealing',\n                  tag_levels = 'A')\n\n\n\n\nOs cinco modelos cuja combinação de hiperparâmetros geraram os maiores valores de AUC são apresentados abaixo. Ainda que todos eles apresentem valores de AUC acima daquele obtido para o baseline com as configurações default da Random Forest, podemos ver que existe alguma incerteza em cima da estimativa pontual destes valores - representado pelo std_err associado à cada um deles. Neste contexto, será que os modelos tunados para melhor performance são de fato melhores que o baseline?\n\nCódigoshow_best(x = fit_tuning, metric = 'roc_auc', n = 5)\n\n# A tibble: 5 × 10\n   mtry trees min_n .metric .estimator  mean     n std_err .config .iter\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n1    24   670    38 roc_auc binary     0.754    10  0.0131 Iter41     41\n2    25   818    24 roc_auc binary     0.754    10  0.0123 Iter1       1\n3    24   501    38 roc_auc binary     0.754    10  0.0125 Iter47     47\n4    25   726    34 roc_auc binary     0.753    10  0.0136 Iter43     43\n5    25  1050    32 roc_auc binary     0.753    10  0.0127 Iter30     30\n\n\nPara responder à essa pergunta, utilizei um teste-t para testar a hipótese nula de que a diferença entre os valores de AUC do modelo baseline e do melhor modelo tunado não é diferente de 0. Como podemos ver abaixo, o p-valor deste teste supera o valor crítico de 0.05 e, portanto, não podemos rejeitar àquela hipótese nula. Em outras palavras, isto quer dizer que, na prática, tunar a Random Forest não trouxe benefícios substanciais em termos das métricas de performance avaliadas. Tendo este resultado em vista, vou seguir utilizando aqui o modelo baseline para gerar as probabilidades e tentar entender os padrões que o modelo capturou.\n\nCódigo# pegando o resultado do AUC do baseline\nbaseline &lt;- collect_metrics(x = fit_baseline) %&gt;% filter(.metric == 'roc_auc')\n# pegando as metricas do melhor modelo tunado\ntuned_results &lt;- show_best(x = fit_tuning, metric = 'roc_auc', n = 1)\n\n# pre-calculando o quadrado dos erros padroes\nse_baseline &lt;- baseline$std_err^2\nse_tuned &lt;- tuned_results$mean^2\n\n# extraindo a quantidade de observacoes (igual para os dois)\nn_obs &lt;- baseline$n\n\n# calculando a estatística t da diferenca entre as medias\nestatistica &lt;- (baseline$mean - tuned_results$mean) / sqrt((se_baseline + se_tuned))\n\n# numero de graus de liberdade ajustado\ndf_welch &lt;- ((se_baseline + se_tuned)^2) / ((se_baseline^2/(n_obs - 1)) + (se_tuned^2/(n_obs - 1)))\n\n# calculando o p-valor associado a estatistica calculada, utilizando um teste bicaudal, uma\n# vez que a hipótese nula trata de uma diferenca entre medias que pode ser maior ou menor que 0\np_valor &lt;- 2 * pt(q = estatistica, df = df_welch, lower.tail = TRUE)\n\n# printando o resultado\nsprintf(fmt = 'Estatistica t: %.3f | df: %.2f| p-valor: %5f', estatistica, df_welch, p_valor)\n\n[1] \"Estatistica t: -0.032 | df: 9.00| p-valor: 0.975478\""
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#ajuste-do-modelo-selecionado",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#ajuste-do-modelo-selecionado",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Ajuste do modelo selecionado",
    "text": "Ajuste do modelo selecionado\nFaremos o ajuste do modelo uma última vez, utilizando todos os dados da base de treino para ajustar o algoritmo e os dados da base de teste para gerar as probabilidades que serão usadas para avaliar a sua performance.\n\nCódigo## ajustando o modelo aos dados uma ultima vez\nmodelo_ajustado &lt;- last_fit(object = wf_baseline, split = train_test_split, metrics = metricas)\n\n## pegando as metricas do modelo\ncollect_metrics(x = modelo_ajustado)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 specificity binary         0.550 Preprocessor1_Model1\n2 roc_auc     binary         0.732 Preprocessor1_Model1\n3 mn_log_loss binary         0.598 Preprocessor1_Model1\n\n\nOs valores de AUC deste modelo final ficaram praticamente idênticos aquele obtido pelo modelo baseline. Todavia, temos um valor de log loss menor (i.e., probabilidades mais bem ajustadas) e uma sensibilidade maior (i.e., melhor capacidade do prever os acidentes que ocorreram), algo que acredito ser muito bom também. Mas como será que este modelo performou gerando previsões binárias, i.e. há ou não um acidente associado à cada prova? Como já havíamos antecipado pela métrica de sensibilidade, o modelo preveu corretamente que houve um acidente em uma prova um pouco mais de metade das vezes; por outro lado, ele errou em 34% das vezes em que preveu que haveria um acidente. Por fim, o modelo também não foi tão mal em prever quando não haveria um acidente, acertando em quase 79% dos casos. Para um modelo tão simples e com tão pouca informação, acredito que estes resultados estão bem OK para o esforço empregado.\n\nCódigo# pegando as previsoes do modelo ajustado\ncollect_predictions(x = modelo_ajustado) %&gt;% \n  # tabulando as previsoes e observacoes\n  count(tem_acidente, .pred_class) %&gt;% \n  # ajustando o texto das previsoes e target\n  mutate(across(where(is.factor), \\(x) ifelse(test = x == 1, yes = 'Sim', no = 'Não'))) %&gt;% \n  # recodificando o nivel dos fatores\n  mutate(\n    tem_acidente = fct_relevel(.f = tem_acidente, 'Sim'),\n    .pred_class = fct_relevel(.f = .pred_class, 'Não')\n  ) %&gt;% \n  # criando a figura\n  ggplot(mapping = aes(x = .pred_class, y = tem_acidente, fill = n, label = n)) +\n  geom_tile(color = 'black', show.legend = FALSE) +\n  geom_text(fontface = 'bold', size = 4) +\n  scale_fill_viridis_c(begin = 0.3) +\n  labs(\n    title    = 'Previsões feitas pelo modelo vs ocorrência dos acidentes',\n    subtitle = 'A matriz de confusão apresenta os acertos no modelo na diagonal principal e os\\nerros na diagonal oposta.',\n    x        = 'Previsão de acidente?',\n    y        = 'Ocorreu o acidente?'\n  )"
  },
  {
    "objectID": "posts/2021-11-15_acidentes-formula-1/index.html#explicabilidade",
    "href": "posts/2021-11-15_acidentes-formula-1/index.html#explicabilidade",
    "title": "Previsão de acidentes com os dados da Fórmula 1",
    "section": "Explicabilidade",
    "text": "Explicabilidade\nUma vez que cheguei a um modelo preditivo aceitável, vou dar uma olhada agora nos padrões que ele foi capaz de capturar nos dados. A primeira coisa que acho legal olhar é o ranking de importância das variáveis que a Random Forest fornece.\n\nCódigo# pegando o modelo ajustado\nextract_fit_parsnip(x = modelo_ajustado) %&gt;% \n  # extraindo a importancia das variaveis de dentro do objeto do modelo\n  pluck('fit', 'variable.importance') %&gt;% \n  # colocando o vetor nomeado em uma dataframe\n  enframe(name = 'variavel', value = 'importance') %&gt;% \n  # pegando as 15 variaveis mais importantes\n  top_n(n = 15, wt = importance) %&gt;% \n  # reordenando as variaveis para plotar\n  mutate(\n    variavel = str_remove(string = variavel, pattern = 'circuit_'),\n    variavel = str_replace_all(string = variavel, pattern = '(_|\\\\.)', replacement = ' '),\n    variavel = str_to_title(string = variavel, locale = 'pt_BR'),\n    variavel = fct_reorder(.f = variavel, .x = importance, .desc = FALSE)\n  ) %&gt;% \n  # criando a figura\n  ggplot(aes(x = importance, y = variavel)) +\n  geom_col(aes(fill = importance), color = 'black') +\n  geom_text(aes(label = round(importance, 2)), nudge_x = 3) +\n  scale_fill_viridis_c(begin = 0.1) +\n  scale_x_continuous(breaks = seq(from = 0, to = 50, by = 10)) +\n  labs(title    = 'Importância das variáveis para o modelo',\n       subtitle = 'As 15 variáveis mais importantes de acordo com o critério de impureza de Gini',\n       x        = 'Impureza de Gini') +\n  theme(\n    legend.position = 'none',\n    axis.title.y = element_blank()\n  )\n\n\n\n\nPodemos ver que o ano em que a prova foi realizada foi, de longe, a variável mais importante para gerar as previsões de ocorrência dos acidentes. Na sequência, outras 5 variáveis pareceram importantes: o coeficiente de variação na pontuação dos pilotos até a prova anterior (cvl_all_pilots), a extensão do circuito, o coeficiente de variação na pontuação dos 5 pilotos com mais pontos até a prova anterior (cvl_top_pilots), a quantidade de voltas e a quantidade de pilotos participando na prova. Todas essas variáveis estão bastante relacionadas com algumas das hipóteses que defini. Todavia, ainda não sei se a contribuição destas variáveis é similar àquela que eu havia hipotetizado ou não, uma vez que este gráfico só mostra quais variáveis são importantes.\nPara entender de que forma estas variáveis estão relacionadas à probabilidade de ocorrência de um acidente em uma prova, vou utilizar um plot de dependência parcial. Para criar este plot é necessário extrair as previsões da probabilidade de ocorrência de um acidente para cada instância na base de treino, fixando-se os valores de todas as variáveis preditoras a não ser àquela que estamos interessados. Com isso, geramos algo parecido com um profile sobre a forma pela qual a previsão do modelo para àquela instância varia apenas em função da variável analisada. Uma vez que tenhamos esse profile calculado para cada instância, sumarizamos o padrão obtido entre todas as instâncias para cada valor xi da variável analisada, de forma a obter a curva ‘média’ que descreve a relação daquela variável preditora com a variável resposta.\nPara facilitar a criação dos plots de dependência parcial, vou carregar os pacotes DALEX e DALEXtra, criar uma instância de um explicador do tidymodels e passar três argumentos para ela: o workflow ajustado, os dados de treino do modelo e o target (convertido para um número inteiro).\n\nCódigo# carregando o pacote\nlibrary(DALEX)\nlibrary(DALEXtra)\n\n# criando instancia do DALEX para explicar o modelo\nexplainer &lt;- explain_tidymodels(\n  extract_workflow(x = modelo_ajustado), \n  data = training(train_test_split) %&gt;% select(-tem_acidente),\n  y = as.integer(training(train_test_split)$tem_acidente)\n)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  workflow  (  default  )\n  -&gt; data              :  785  rows  35  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  785  values \n  -&gt; predict function  :  yhat.workflow  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package tidymodels , ver. 1.1.1 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0336633 , mean =  0.4202269 , max =  0.9253802  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  0.3696457 , mean =  1.000155 , max =  1.687087  \n  A new explainer has been created!  \n\n\nAgora que temos esta instância, podemos gerar o plot de dependência parcial. Vou focar aqui em apenas três daquelas seis variáveis que vimos serem mais importantes para o modelo de Random Forest: o ano, o coeficiente de variação na pontuação dos pilotos até a prova anterior (cvl_all_pilots) e o coeficiente de variação na pontuação dos 5 pilotos com mais pontos até a prova anterior (cvl_top_pilots). De que forma estas variáveis contribuem para definir a probabilidade de ocorrência de acidentes de acordo com o modelo preditivo gerado?\n\nCódigo# pegando os dados do partial dependence plot\nmodel_profile(\n  explainer = explainer, \n  variables = c('year', 'cvl_all_pilots', 'cvl_top_pilots'),\n  type = 'partial'\n) %&gt;%\n  # extraindo os dados para criar a figura no ggplot\n  pluck('agr_profiles') %&gt;% \n  # ajustando a ordem dos graficos\n  mutate(\n    `_vname_` = case_when(`_vname_` == 'cvl_top_pilots' ~ 'CV pontuação dos cinco pilotos no topo do ranking',\n                          `_vname_` == 'cvl_all_pilots' ~ 'CV pontuação de todos os pilotos',\n                          TRUE ~ 'Ano')\n  ) %&gt;%\n  # criando a figura\n  ggplot(mapping = aes(x = `_x_`, y = `_yhat_`)) +\n  facet_wrap(~ `_vname_`, scales = 'free', strip.position = 'bottom') +\n  geom_line() +\n  labs(\n    title    = 'Efeitos parciais de três variáveis sobre a probabilidade de ocorrência de acidentes',\n    subtitle = 'O efeito parcial de uma variável sobre a resposta é aquele que presenta o impact de uma variável sobre a resposta fixando-se o impacto de todas as demais variáveis',\n    y        = 'Probabilidade de ocorrência de um acidente'\n  ) +\n  theme(axis.title.x = element_blank())\n\n\n\n\nMuitos dos padrões capturados pelo modelo para estas três variáveis são similares aqueles observados na análise exploratória. O modelo foi capaz de identificar o padrão temporal da frequência de ocorrência de acidentes, mostrando que houve uma queda de cerca de 20% na probabilidade de ocorrência destes eventos a partir da década de 90. Não dá para saber o quanto isto pode estar relacionado às intervenções de segurança implementadas no período, mas é importante ver que algo que aconteceu a partir daí contribuiu bastante para a queda na frequência de acidentes. Outro padrão interessante é aquele relacionado à intensidade e assimetria da disputa pelo campeonato, que também havíamos visto na análise exploratória. De fato, parece que quando a disputa pelo campeonato está muito acirrada e existem alguns pilotos que estão ali só de coadjuvante, é quando existe uma maior probabilidade de ocorrência de acidentes. De toda forma, a contribuição média destes fatores parece não ser tão expressiva assim, uma vez que seu impacto sobre as probabilidades é de cerca de 5% a 6%."
  },
  {
    "objectID": "posts/2021-11-30_scrapper-gwent/index.html",
    "href": "posts/2021-11-30_scrapper-gwent/index.html",
    "title": "Raspando a biblioteca de decks de Gwent",
    "section": "",
    "text": "The Witcher é uma franquia lançada como uma série de livros de fantasia que contam as aventuras do bruxo (i.e., Witcher) Geralt de Rivia. Essas estórias foram popularizadas através da série da Netflix de mesmo nome e, também, através do jogo The Witcher 3: The Wild Hunt1. Esse jogo é bastante complexo e tem uma experiência bem imersiva, trazendo inclusive diversas tradições do universo à ela. Uma delas é o Gwent, um jogo de cartas entre dois jogadores, onde ganha aquele que mais pontuar em pelo menos 2 de 3 turnos. Parece ser um mini-jogo bobo dentro do título, mas ele próprio invoca muito da fantasia da série na disputa.\nGwent é um jogo que lembra muito o Magic, onde você deve construir um deck de no mínimo 25 cartas pertencentes à uma facção de sua escolha, respeitando algumas restrições (e.g., custo total das cartas no deck, quantidade de unidades e etc). No momento em que escrevo este post, existem cerca de 200 cartas pertencentes à cada uma de 6 facções distintas, além de outras 200 à 400 cartas neutras (i.e., que não pertencem à nenhuma facção) que podem ser usadas para montar um deck Existe uma diferença inerente ao modo de jogar com cada facção (e.g., foco em dano direto, foco bloqueio e roubo de cartas,…) e, dentro de uma dada facção, também existe uma pequena diversidade de formas de favorecer uma estratégia de jogo (e.g., cartas que juntas reforçam muito umas as outras, cartas que ajudam a ativar a habilidade de outras cartas mais frequentemente). Neste sentido, montar um deck forte e consistente passa a ser quase uma arte, mas que poderia ser aprimorado com um pouquinho de acesso aos metadados das cartas.\nExiste bastante conteúdo na internet que é produzido pela própria comunidade que joga o Gwent. Em particular, a própria comunidade contribui compartilhando a composição de cartas nos seus decks, as estratégias de jogo e votando nestas a partir do próprio site oficial do jogo. E é aqui que entra o meu interesse: se pudermos obter estas informações e estruturá-las, ficaria muito mais fácil entender os padrões dentro e entre os decks e usar isso em favor de aprimorar a jogabilidade. Além disso, acredito que estes dados podem dar um bom modelo de estudo para responder à algumas perguntas e praticar algumas outras técnicas. Falarei sobre essas idéias ao final do post, mas por agora vou mostrar como obter essas informações."
  },
  {
    "objectID": "posts/2021-11-30_scrapper-gwent/index.html#footnotes",
    "href": "posts/2021-11-30_scrapper-gwent/index.html#footnotes",
    "title": "Raspando a biblioteca de decks de Gwent",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nUm dos jogos mais tops que já joguei.↩︎\nEsse passo não é obrigatório, mas decidi colocá-lo aqui só para não bombardear o servidor com um monte de requests de uma vez quando formos escalar o seu uso.↩︎\ne.g., https://www.playgwent.com/pt-BR/decks/2, https://www.playgwent.com/pt-BR/decks/3,…↩︎\nIsso pode estar relacionado ao comportamento aparentemente linear que pode ser visto no painel B.↩︎"
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html",
    "href": "posts/2021-12-11_notas-boardgames/index.html",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "",
    "text": "Se você quiser saber quão legal pode ser um jogo de tabuleiro específico, uma opção seria recorrer aos sites especializados no assunto para ter uma ideia. Dois destes portais são a Ludopedia e o BoardGameGeek: o primeiro é um site brasileiro e o segundo é internacional. Apesar de compartilharem muitas similaridades em torno do conteúdo, uma diferença fundamental entre eles está na quantidade de títulos disponíveis, onde o BoardGameGeek (BGG, daqui em diante) tem quase 50 vezes mais títulos registrados do que a Ludopedia. Outro ponto que à princípio pode nos enganar está no próprio ranking dos jogos que estes sites possuem: se você reparar bem, apesar de muitos títulos se repetirem entre eles, as notas não são exatamente próximas. Desta forma, quão similar será que os rankings e as notas dadas aos títulos são entre eles?\nExistem muitas formas de pensar nessa pergunta, mas uma que aguçou a minha curiosidade foi a possibilidade do público de um dos portais tender a ser mais ‘bonzinho’ do que o do outro no momento de avaliar os jogos. Isto se manifestaria como um viés quando comparássemos a média das notas dos rankings entre os dois portais - isto é, a diferença na média das notas no portal x é sempre maior do que àquela no portal y. Neste contexto, seria interessante entender não só se existe viés existe mas, caso positivo, qual a direção e o tamanho do mesmo. Não que eu espere que isso vá interferir de alguma forma em uma decisão de compra ou de jogar um título específico…mas às vezes é legal saber se as informações que estamos recebendo possuem algum tipo de viés quando comparado com outras fontes disponíveis.\nOs dados que nos permitem ganhar àquela compreensão estão prontamente disponíveis nos respectivos portais e, ao longo dos últimos posts, já construímos uma compreensão de como obtê-los. Você pode buscar o código para raspar a página do ranking da Ludopedia aqui e o do ranking do BGG aqui e, se quiser, também ganhar uma compreensão de como eles funcionam aqui e aqui. Como eu já havia adiantado naqueles posts anteriores, meu intuito é utilizar as informações extraídas a partir dos scrappers não só pela prática, mas também para responder à alguma pergunta, entender algum padrão interessante ou testar algum pacote.\nFalando nisso, um objetivo secundário deste post é, de fato, testar o pacote infer1. De acordo com o próprio site, o infer tem por objetivo implementar a inferência estatística utilizando uma gramática estatística expressiva que seja coerente com o framework existente no tidyverse. Esse pacote traz algumas funções para atingir este objetivo e, através de sua documentação, podemos ver que a análise acaba ficando bastante verbosa - mas bem aderente às idéias e conceitos da estatística inferencial. O R já possui muitas funcionalidades nativas orientadas à mesma finalidade, mas acho interessante testar essa outra opção que parece conversar tão bem com o universo tidy.\nVamos começar preparando os dados e entendendo de que forma vamos avaliar a similaridades nas notas dos rankings. A partir daí, avançaremos para ganhar um entendimento geral do padrão estudado e concluiremos mostrando três formas através das quais podemos analisar este mesmo dado - sendo àquela do pacote infer uma delas."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#stats",
    "href": "posts/2021-12-11_notas-boardgames/index.html#stats",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "stats",
    "text": "stats\nA pergunta que estamos abordando é se existe alguma diferença entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG. A hipótese estatística aqui seria a de que a diferença média entre as duas notas seria igual a zero; posto de outra forma, a diferença entre cada par de notas associada à cada uma das posições do ranking de i à 2810 tenderia à zero. Como estamos considerando a diferença de notas entre um par de observações, o teste estatístico que precisamos utilizar é um teste-t pareado. Este teste é implementado no R através da função stats::t.test.\nQuando formos especificar o teste abaixo, é importante estar atento à duas coisas. Uma delas é que a hipótese que estamos testando diz que a diferença média deve ser diferente 0, ou seja, um número maior ou menor que 0. Neste contexto, precisamos especificar que a nossa hipótese alternativa é bicaudal - argumento alternative = 'two.sided'. O segundo ponto é que a variância nas notas entre os dois portais é diferente, conforme pode ser visto em uma das figura acima onde apresentamos a distribuição dos valores de nota para cada portal3. Poderíamos fazer um teste estatístico para confirmar o fato de que as variâncias não são homogêneas (e.g., o teste de Barttlet), mas podemos dispensar essa formalidade aqui pois o padrão já é muito claro naquela figura que mencionei - ainda assim, esse seria um passo importante a se tomar em uma análise mais formal. Para considerar que as variâncias não são homogêneas, basta utilizar var.equal = FALSE.\nVamos implementar o teste-t pareado abaixo.\n\nCódigo# teste-t pareado com variancias diferentes conforme disponivel no base R\nt_test_stats &lt;- t.test(x = ranks$ludopedia, y = ranks$bgg, \n                       alternative = 'two.sided', paired = TRUE, var.equal = FALSE)\nt_test_stats\n\n\n    Paired t-test\n\ndata:  ranks$ludopedia and ranks$bgg\nt = 24.418, df = 2809, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.3413125 0.4009153\nsample estimates:\nmean difference \n      0.3711139 \n\n\nO resultado desta análise sugere que devemos rejeitar a hipótese nula, sugerindo que a diferença média entre as notas da Ludopedia e do BGG para uma mesma posição do ranking não tende à zero. Na realidade, podemos ver que a estimativa da diferença média estimada entre as notas é de 0.371 pontos, com um intervalo de confiança de 95% de 0.341 à 0.401 pontos. Isto seria o tanto, em média, que as notas do ranking da Ludopedia superam àquelas do BGG. Vamos agora examinar o que o pacote infer nos oferece."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#infer",
    "href": "posts/2021-12-11_notas-boardgames/index.html#infer",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "infer",
    "text": "infer\nO infer não possui uma implementação explícita do teste-t pareado, mas isso não é necessariamente um problema. Como a própria documentação do pacote mostra, o que o teste-t pareado faz por baixo dos panos é calcular a diferença entre os valores de x e y, e calcular a estatística em cima desta diferença. Ou seja, este teste seria um caso especial de um teste-t univariado. Assim, para utilizarmos o teste-t pareado no infer, precisaremos adicionar uma coluna no tibble original com a diferença par a par entre as notas já calculada.\n\nCódigolibrary(infer) # para fazer o teste de hipótese\n\n# colocando a diferenca no tibble\nranks &lt;- ranks %&gt;% \n  # calculando a coluna de diferenca\n  mutate(\n    diferenca = ludopedia - bgg\n  )\n\n\nA partir daqui, usaremos os verbos do infer para especificar (specify) a variável resposta e calcular (calculate) a estatística de teste que vamos usar. Como estamos utilizando um teste-t, o ideal seria focarmos no valor de t como a nossa estatísticade de teste. No entanto, nosso maior interesse está na estimativa da diferença média entre as notas e o seu intervalo de confiança. Por conta disso, vamos especificar que a nossa estatística de teste será o valor da média. O problema ao fazermos isso é que qualquer cálculo relacionado à nossa estatística de teste deverá ser feito criando uma distribuição de probabilidade a partir da própria amostra estudada. Para isso, o pacote infer faz uso de técnicas de permutação e reamostragem. Antes de passar para esse ponto, no entanto, vamos medir o valor observado da nossa estatística de teste - a diferença média entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG.\n\nCódigo# rodando um teste-t pareado original\nobs_statistic &lt;- ranks %&gt;% \n  # especificando a variável analisada para o infer\n  specify(\n    response = diferenca\n  ) %&gt;% \n  # calculando estatistica de teste\n  calculate(stat = 'mean') %&gt;% \n  # extraindo a coluna com a estatistica\n  pull(stat)\nobs_statistic\n\n[1] 0.3711139\n\n\nAs técnicas de reamostragem são uma ferramenta muito útil em diversas situações. Alguns exemplos de casos de aplicação são quando:\n\nNão sabemos a que família de distribuição estatística a variável resposta e/ou seus resíduos pertence;\n\nPrecisamos fazer uma inferência ou estimar intervalos de confiança;\n\nTemos uma hipótese específica sobre o processo gerador dos dados e queremos testá-lo explicitamente;\n\nPrecisamos realizar comparações entre duas ou mais amostras e temos um número diferente de observações entre elas;\n\nPrecisamos realizar comparações entre duas ou mais amostras e temos um número muito grande de observações;\n\n…\n\nComo a nossa estatística de teste é a diferença média entre as notas, utilizaremos a reamostragem para criar uma estimativa de seu intervalo de confiança. Isto vai nos permitir dizer se existe ou não uma diferença nas notas entre os portais no caso onde a estimativa deste intervalo não contenha o valor 0. Para implementar a reamostragem no infer, utilizaremos o verbo generate para gerar 1.000 amostras aleatórias a partir dos dados originais utilizando a técnica de bootstrap. Através deste técnica, o algoritmo gera uma amostra com substituição a partir de um conjunto de dados e com o mesmo número de observações que ele - i.e., a mesma observação pode aparecer mais de uma vez em uma mesma amostra, e cada amostra tem o mesmo número de observações que os dados que o geraram. Uma vez que tenhamos as 1.000 amostras do bootstrap, vamos calcular (calculate) o valor da média da diferença entre as notas para cada uma delas, e vamos determinar o intervalo que contém 95% das estimativas. Este será o intervalo de confiança na estatística de teste, e que será obtido através do verbo get_confidence_interval.\n\nCódigo## criando reamostragem para estimar o intervalo de confiança\nboot_ci_tbl &lt;- ranks %&gt;% \n  # especificando a variável analisada para o infer\n  specify(\n    response = diferenca\n  ) %&gt;% \n  # gerando bootstrap\n  generate(reps = 1000, type = 'bootstrap') %&gt;% \n  # calculando estatistica do bootstrap\n  calculate(stat = 'mean')\n\n## criando o histograma de distribuição de frequência da médias\nggplot(data = boot_ci_tbl, mapping = aes(x = stat)) +\n  geom_histogram(color = 'black', fill = 'grey80') +\n  scale_x_continuous(n.breaks = 8) +\n  labs(\n    title = 'Distribuição de frequência da diferença média entre notas nos dados reamostrados',\n    x     = 'Diferença média entre as notas (Ludopedia - BGG)',\n    y     = 'Frequência'\n  )\n\n\n\nCódigo## calculando o intervalo de confiança\nboot_ci &lt;- boot_ci_tbl %&gt;% \n  # pegando o intervalo de confianca\n  get_confidence_interval(type = 'percentile')\nboot_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.342    0.401\n\n\nComo podemos ver, a estatística de teste e o intervalo de confiança extraído do bootstrap são muito parecidos com aqueles que tínhamos observado com a função stats::t.test. Muito bom!\nMas e se eu tivesse outra hipótese nula?\nUma coisa que achei legal no infer é que ele também deixa você criar uma distribuição de valores que sigam uma hipótese nula específica. Isso pode ser útil, por exemplo, caso tivéssemos declarado que a hipótese nula que queremos testar é que a diferença entre as notas é um outro valor que não 0. No caso abaixo, podemos utilizar o verbo hypothesize para declarar qual é a hipótese nula que queremos testar e o valor esperado para a estatística de teste de acordo com a nossa hipótese nula. Neste exemplo, testarei se a diferença média entre as notas é diferente de 0,2 pontos, usando o verbo generate para gerar 500 amostras aleatórias usando a técnica de boostrap a partir de algum conjunto de dados - não ficou claro para mim de que distribuição o pacote está tirando essa estimativa…assumo que seja de uma distribuição normal, com a média que definimos na chamada da função e desvio padrão igual a 1…mas o pacote falhou em documentar isso direito.\n\nCódigo## calculando a distribuicao da estatistica de teste com o bootstrap\nnull_statistic &lt;- ranks %&gt;% \n  # especificando a variável analisada para o infer\n  specify(\n    response = diferenca\n  ) %&gt;% \n  # especificando a hipotese nula que queremos testar\n  # não existe diferença entre as notas medias para uma mesma posicao entre rankins\n  hypothesize(null = 'point', mu = 0.2) %&gt;%\n  # gerando bootstrap\n  generate(reps = 500, type = 'bootstrap') %&gt;% \n  # calculando estatistica do bootstrap\n  calculate(stat = 'mean')\nnull_statistic\n\nResponse: diferenca (numeric)\nNull Hypothesis: point\n# A tibble: 500 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.229\n 2         2 0.206\n 3         3 0.210\n 4         4 0.205\n 5         5 0.190\n 6         6 0.205\n 7         7 0.212\n 8         8 0.192\n 9         9 0.215\n10        10 0.198\n# ℹ 490 more rows\n\n\nUma vez que tenhamos montado a distribuição de valores que descreve a hipótese nula, podemos usar um outro conjunto de verbos para visualizar os resultados. O verbo visualize vai plotar a distribuição dos valores relacionados à hipótese nula, enquanto que os verbos shade_confidence_interval e shade_p_value servem para plotar na mesma figura o intervalo de confiança e a estatística de teste original - caso a tenhamos calculado anteriormente.\n\nCódigonull_statistic %&gt;% \n  # criando a visualização\n  visualize() +\n  # sombreando o intervalo de confianca\n  shade_confidence_interval(endpoints = boot_ci, color = 'white', fill = 'tomato') +\n  # colocar uma linha para a estimativa\n  shade_p_value(obs_stat = obs_statistic, direction = 'two-sided', color = 'tomato', size = 1) +\n  # ediitando a figura\n  scale_x_continuous(breaks = seq(from = -0.05, to = 0.4, by = 0.05)) +\n  labs(\n    title    = 'Distribuição dos valores nulos e observados',\n    subtitle = 'O histograma representa a distribuição dos valores da diferença entre médias de acordo com a hipótese nula. A linha\nvertical vermelha indica a estimativa da diferença média entre as notas, enquanto a barra vertical o intervalo de\nconfiança 95%. Como não há sobreposição entre a distribuição dos dados de acordo com a hipótese nula e o intervalo de\nconfiança, devemos rejeitar a hipótese nula.',\n    caption  = 'Utilizamos o método de reamostragem por bootstrap para a estimativa do intervalo de confiança, onde o calculamos através\ndo percentil de distribuição das médias reamostradas.',\n    x        = 'Diferença entre médias (Ludopedia - BGG)',\n    y        = 'Frequência'\n  )\n\n\n\n\nTá aí uma forma fácil de implementar um bootstrap e testar diferenças entre médias. Vamos agora à mais uma forma que eu pensei para tentar abordar a pergunta proposta."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#subamostragem",
    "href": "posts/2021-12-11_notas-boardgames/index.html#subamostragem",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "Subamostragem",
    "text": "Subamostragem\nUm ponto importante e comum à todos esses métodos estatísticos é que tanto maior for o tamanho da amostra, mais estreitos serão as estimativas de intervalo de confiança e maior a probabilidade de rejeitarmos a hipótese nula quando não deveríamos ter feito isso (i.e., um erro do Tipo II ou um falso positivo). Nossa base de dados não é tão grande assim (2.810 observações), mas acredito já ser grande o suficiente para que estes tipos de problema comecem a se manifestar - especialmente para um teste estatístico como o teste-t, que foi concebido sob a ideia de amostras contendo até umas 30 observações. É claro que isto não invalida o que fizemos até aqui, mas é bastante provável que isto esteja tornando as estimativas feitas muito otimistas.\nPara resolver este problema, podemos buscar uma inspiração nas técnicas de reamostragem descritas anteriormente, e implementar uma subamostragem da base de dados seguida de diversas reamostragens. Isto é, vamos amostrar aleatoriamente uma fração de tamanho muito pequeno n dos dados sem substituição, e repetir este procedimento m vezes. Daí então vamos calcular a estatística de teste para cada uma das m amostras e combiná-las de alguma forma - pode ser através de uma média, de um percentil ou até mesmo utilizando uma técnica de meta-análise.\nVamos começar construindo a solução através da função que vai criar uma subamostra.\n\nCódigo# criando funcao para fazer a reamostragem\ncreate_sample &lt;- function(dataset, tamanho) {\n  dataset %&gt;% \n    # retira uma amostra aleatoria de um determinado tamanho\n    slice_sample(n = tamanho)\n}\n\n\nPodemos então utilizar a função purrr::rerun para repetir a execução da função create_sample 500 vezes, sendo que amostraremos apenas 30 das 2.810 observações disponíveis em cada uma delas. Esta função retornará uma lista de listas e, portanto, utilizaremos a função bind_rows para juntá-las em um tibble e aninhar todas as colunas utilizando um nest, deixando de fora apenas a coluna com a identidade da amostra.\n\nCódigo## criando dataframe reamostrado\nreamostragem &lt;- rerun(.n = 500,\n                      # rodando a funcao para criar amostrar 500 vezes, cada uma \n                      # das quais amostrando 50 observacoes aleatoriamente\n                      create_sample(dataset = ranks, tamanho = 30)) %&gt;% \n  # juntando cada um dos dataframes e adicioando um sample id\n  bind_rows(.id = 'sample_id') %&gt;% \n  # aninhando o dataframe com os dados que vamos usar para ajustar o test-t dentro \n  # de cada sample_id\n  nest(data = -sample_id)\nreamostragem\n\n# A tibble: 500 × 2\n   sample_id data             \n   &lt;chr&gt;     &lt;list&gt;           \n 1 1         &lt;tibble [30 × 4]&gt;\n 2 2         &lt;tibble [30 × 4]&gt;\n 3 3         &lt;tibble [30 × 4]&gt;\n 4 4         &lt;tibble [30 × 4]&gt;\n 5 5         &lt;tibble [30 × 4]&gt;\n 6 6         &lt;tibble [30 × 4]&gt;\n 7 7         &lt;tibble [30 × 4]&gt;\n 8 8         &lt;tibble [30 × 4]&gt;\n 9 9         &lt;tibble [30 × 4]&gt;\n10 10        &lt;tibble [30 × 4]&gt;\n# ℹ 490 more rows\n\n\nCom estas amostras, podemos agora ajustar um teste-t pareado similar aquele implementado usando a função stats::t.test à cada amostra, e extrair os resultados de cada uma delas. Para isso, precisaremos também fazer uso das funções purrr::map, broom::tidy e tidyr::unnest. Os resultados abaixo representam os resultados de cada um dos testes-t pareados aplicados à cada uma das m subamostras criadas.\n\nCódigoresultados_reamostragem &lt;- reamostragem %&gt;% \n  mutate(\n    # aplicando um teste-t pareado às amostras de cada sample_id\n    test_t = map(.x = data, \n                 .f = ~t.test(x = .x$ludopedia, y = .x$bgg, \n                              alternative = 'two.sided', paired = TRUE, var.equal = FALSE)\n    ),\n    # extraindo as informacoes tidy do ajuste do test-t pareado\n    tidy_t = map(.x = test_t, .f = broom::tidy)\n  ) %&gt;% \n  # desaninhando os resultados do test-t\n  unnest(tidy_t) %&gt;% \n  # jogando fora alguma colunas que nao precisaremos mais\n  select(-data, -test_t)\nresultados_reamostragem\n\n# A tibble: 500 × 9\n   sample_id estimate statistic p.value parameter conf.low conf.high method     \n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n 1 1           0.218      1.60  0.120          29 -0.0605      0.496 Paired t-t…\n 2 2           0.274      1.93  0.0636         29 -0.0166      0.565 Paired t-t…\n 3 3           0.265      1.82  0.0789         29 -0.0326      0.563 Paired t-t…\n 4 4           0.409      2.55  0.0162         29  0.0813      0.737 Paired t-t…\n 5 5           0.285      2.02  0.0528         29 -0.00362     0.574 Paired t-t…\n 6 6           0.414      3.27  0.00278        29  0.155       0.673 Paired t-t…\n 7 7           0.152      1.14  0.265          29 -0.122       0.427 Paired t-t…\n 8 8           0.177      1.48  0.151          29 -0.0685      0.423 Paired t-t…\n 9 9           0.0513     0.338 0.738          29 -0.260       0.362 Paired t-t…\n10 10          0.409      3.06  0.00472        29  0.136       0.682 Paired t-t…\n# ℹ 490 more rows\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nFinalmente, vamos extrair a média da estimativa da diferença entre as notas (i.e., a média da coluna estimate) para ser a nossa estatística de teste, e calcular os quantis de 2.5% e 97.5% da distribuição destes valores para serem os limites inferior e superior, respectivamente, do intervalo de confiança de 95%. Com isso, podemos visualizar os resultados obtidos através da reamostragem abaixo.\n\nCódigo# pegando os quantis da distribuicao das estimativas das subamostras\nquantis_subamostra &lt;- quantile(x = resultados_reamostragem$estimate, probs = c(0.025, 0.975))\nmedia_subamostra &lt;- mean(x = resultados_reamostragem$estimate)\n\n## pegando os dados do que o density plot vai usar e retendo apenas as areas\n## que estao fora do quantil\ndf_dos_quantis &lt;- tibble(\n  # pegando os valores de x e y que serão usados para o density plot\n  x = density(x = resultados_reamostragem$estimate)$x,\n  y = density(x = resultados_reamostragem$estimate)$y\n) %&gt;% \n  mutate(\n    # sinalizando os valores de x que estão fora do quantil de 95%\n    lower_quantile = x &lt; quantis_subamostra[1],\n    upper_quantile = x &gt; quantis_subamostra[2]\n  ) %&gt;% \n  # filtrando apenas as observacoes que estao fora do quantil\n  filter(lower_quantile | upper_quantile)\n\n# criando a figura\nresultados_reamostragem %&gt;% \n  ggplot(mapping = aes(x = estimate)) +\n  geom_density(fill = 'grey80', color = 'black') +\n  geom_ribbon(data = filter(df_dos_quantis, lower_quantile),\n              mapping = aes(x = x, ymin = 0, ymax = y), fill = 'tomato') +\n  geom_ribbon(data = filter(df_dos_quantis, upper_quantile),\n              mapping = aes(x = x, ymin = 0, ymax = y), fill = 'tomato') +\n  geom_vline(xintercept = 0, color = 'black', alpha = 0.6) +\n  geom_vline(xintercept = media_subamostra, color = 'black', linetype = 2) +\n  labs(\n    title    = 'As notas do ranking da Ludopedia são maiores que as do BoardGameGeek',\n    subtitle = str_glue('A estimativa da diferença através da subamostragem é de que as notas da Ludopedia superam as do BoardGameGeek\\nem {round(media_subamostra, digits = 2)} pontos (Intervalo de Confiança de 95%: {round(quantis_subamostra[1], digits = 2)} à {round(quantis_subamostra[2], digits = 2)} pontos)'),\n    caption  = 'A área cinza representa o intervalo de confiança de 95% da estimativa da diferença nas notas entre os\\ndois portais, obtidas através de 500 estimativas independentes tomadas a partir de 30 amostras aleatórias dos dados.',\n    x        = 'Diferença entre as notas do ranking da Ludopedia e do BoardGameGeek',\n    y        = 'Densidade'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nJá é possível reparar que a diferença média estimada entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG é bastante similar àquela dos dois testes anteriores - as notas na Ludopedia são 0.36 pontos maiores, em média, do que no BGG. No entanto, o intervalo de confiança ficou bem maior: 0.08 à 0.64 pontos de diferença em favor da Ludopedia. Como podemos ver, o grande número de amostras nos dados tinha algum impacto sobre estas últimas estimativas, e ela me parece muito mais realista agora do que àquelas obtidas anteriormente. Falo isso principalmente pensando na figura que mostra a variação na diferença entre as notas de acordo com a posição do ranking: podemos ver que a diferença se mantém num patamar elevado até certo ponto e de repente despenca. Assim, a diferença que estimamos estava longe de ser tão pequena quanto àquela calculada anteriormente.\nMas, calma aí…tinha esse padrão que eu acabei de falar…será que essa estimativa que tomamos agora faz mesmo sentido, então?"
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#a-milha-extra",
    "href": "posts/2021-12-11_notas-boardgames/index.html#a-milha-extra",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "A milha extra",
    "text": "A milha extra\nFiquei um bocado incomodado com os resultados que encontrei e os padrões observados naquela figura mostrando a variação na diferença entre as notas de acordo com a posição dos rankings. Fica muito claro ali que as notas dos títulos tendem a ser maiores na Ludopedia até certo ponto, a partir do qual o padrão contrário passa a valer. Assim, me parece mais razoável assumir que existam duas populações distintas dentro da nossa amostra, e que seria importante considerar isso na análise.\nPara tentar achar o ponto onde parece existir uma mudança de tendência nas notas de acordo com a posição do ranking, utilizei o algoritmo de regressão MARS (Multivariate adaptative regression splines). Eu acho esse algoritmo bastante legal: ele segmenta a feature em vários bins e vai vendo a partir de qual bin a relação entre a feature e o target muda; quando isto ocorre, ele cria uma função hinge, dando um coeficiente beta para valores da feature menores ou iguais ao limite daquele bin e um outro coeficiente beta para valores maiores que aquele limite. Tomei vantagem deste aspecto do algoritmo e ajustei o mesmo aos dados especificando, propositalmente, que gostaria de apenas dois termos no modelo - um coeficiente para a nota até a posição do ranking em que ele se mantém em um nível, e outro para o coeficiente a partir do qual o comportamento da relação muda. Esse modelo está super underfitado, mas isso foi mesmo proposital: eu só quero o valor que aparece nos nomes dos coeficientes, e nada mais.\n\nCódigo# carregando o tidymodels \nlibrary(tidymodels)\nlibrary(earth)\n\n# ajustando um MARS aos dados, forcando apenas 2 termos\nmodelo &lt;- mars(mode = 'regression', num_terms = 2) %&gt;% \n  set_engine(engine = 'earth') %&gt;% \n  fit(diferenca ~ ranking, data = ranks)\n\n# sumario do modelo\nsummary(modelo$fit)\n\nCall: earth(formula=diferenca~ranking, data=data, keepxy=TRUE, nprune=~2)\n\n                coefficients\n(Intercept)      0.591006338\nh(ranking-2254) -0.003990402\n\nSelected 2 of 7 terms, and 1 of 1 predictors (nprune=2)\nTermination condition: RSq changed by less than 0.001 at 7 terms\nImportance: ranking\nNumber of terms at each degree of interaction: 1 1 (additive model)\nGCV 0.3724764    RSS 1044.425    GRSq 0.4263666    RSq 0.4271832\n\n\nO output acima traz dois coeficientes, o (Intercept) e o h(ranking-2254), que representam o valor dos betas quando a posição do ranking é menor ou igual a 2.254 e maior que esta posição, respectivamente. Ou seja, se pegarmos os dígitos associados à string do segundo coeficiente, podemos definir exatamente onde que o algoritmo encontrou o ponto a partir do qual a relação entre a diferença nas notas e a posição do ranking mudou.\n\nCódigoponto_de_corte &lt;- modelo$fit$coefficients %&gt;% \n  # pegando o nome das linhas\n  rownames %&gt;% \n  # pegando o segundo elemento - nome do coeficiente da funcao hinge\n  pluck(2) %&gt;% \n  # extraindo todos os numeros do string\n  str_extract(pattern = '[0-9]+') %&gt;% \n  # parseando a string para numeric\n  parse_number()\nponto_de_corte\n\n[1] 2254\n\n\nAgora que temos o ponto de corte para definir as duas populações com que vamos trabalhar, vou repetir o procedimento feito no item anterior em que subamostramos os dados. No entanto, vamos fazer apenas uma pequena modificação na função que cria as subamostras para que ela aceite um argumento que vai nos ajudar a fazer a subamostragem para cada uma das duas populações separadamente. Para fazer isso, vamos passar o teste lógico para saber se a posição do ranking é menor ou igual ao ponto de corte para group_by, que já dará conta de estratificar a subamostragem. Com isso, podemos então proceder normalmente, e ajustar um teste-t pareado para cada subamostra de cada uma das duas populações4.\n\nCódigo# criando funcao para fazer a reamostragem\ncreate_stratified_sample &lt;- function(dataset, tamanho,...) {\n  dataset %&gt;% \n    # agrupa de acordo com o teste logico que for passado em ...\n    group_by(...) %&gt;% \n    # retira uma amostra aleatoria de um determinado tamanho de cada grupo \n    slice_sample(n = tamanho) %&gt;% \n    # desagrupa o dataframe\n    ungroup\n}\n\n## criando dataframe reamostrado\nreamostragem_estratificada &lt;- rerun(.n = 500,\n                                    # rodando a funcao para criar amostrar 500 vezes, cada\n                                    # uma das quais amostrando 50 observacoes aleatoriamente\n                                    create_stratified_sample(dataset = ranks, tamanho = 30, \n                                                             ranking &lt;= ponto_de_corte)) %&gt;% \n  # juntando cada um dos dataframes e adicioando um sample id\n  bind_rows(.id = 'sample_id') %&gt;% \n  # renomeando a coluna de estratificacao\n  rename(estratificacao = `ranking &lt;= ponto_de_corte`) %&gt;% \n  # aninhando o dataframe com os dados que vamos usar para ajustar o test-t\n  # dentro de cada sample_id\n  nest(data = -c(sample_id, estratificacao))\n\n# ajustando o teste-t pareado à cada amostra pertencente à cada grupo\nresultados_reamostragem_estratificada &lt;- reamostragem_estratificada %&gt;% \n  mutate(\n    # aplicando um teste-t pareado às amostras de cada sample_id\n    test_t = map(.x = data, \n                 .f = ~t.test(x = .x$ludopedia, y = .x$bgg, \n                              alternative = 'two.sided', paired = TRUE, var.equal = FALSE)\n    ),\n    # extraindo as informacoes tidy do ajuste do test-t pareado\n    tidy_t = map(.x = test_t, .f = broom::tidy)\n  ) %&gt;% \n  # desaninhando os resultados do test-t\n  unnest(tidy_t) %&gt;% \n  # jogando fora alguma colunas que nao precisaremos mais\n  select(-data, -test_t)\nresultados_reamostragem_estratificada\n\n# A tibble: 1,000 × 10\n   sample_id estratificacao estimate statistic     p.value parameter conf.low\n   &lt;chr&gt;     &lt;lgl&gt;             &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 1         FALSE            -0.685     -4.25 0.000201           29   -1.01 \n 2 1         TRUE              0.725      6.68 0.000000253        29    0.503\n 3 2         FALSE            -0.282     -2.12 0.0427             29   -0.555\n 4 2         TRUE              0.475      4.85 0.0000389          29    0.275\n 5 3         FALSE            -0.265     -2.16 0.0393             29   -0.517\n 6 3         TRUE              0.639      6.72 0.000000224        29    0.445\n 7 4         FALSE            -0.582     -4.20 0.000235           29   -0.866\n 8 4         TRUE              0.619      4.96 0.0000280          29    0.364\n 9 5         FALSE            -0.455     -3.45 0.00174            29   -0.725\n10 5         TRUE              0.684      6.49 0.000000416        29    0.469\n# ℹ 990 more rows\n# ℹ 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nUma vez que já temos os resultados das estimativas das diferenças para cada subamostra de cada uma das duas populações, podemos agora extrair o valor da tendência central da distribuição, bem como o intervalo de confiança associado à cada grupo. Eu optei aqui por usar uma média e os quantis da distribuição para estas duas últimas informações mas, de novo, daria para usar uma meta-análise também. O resultado disso pode ser visto na tabela abaixo, em que fica muito clara que a diferença entre as notas associadas à uma mesma posição entre os rankings da Ludopedia e do BGG é muito dependente da porção da lista que estamos analisando: as avaliações dos jogos são mais enviesadas para notas maiores no portal da Ludopedia até a posição 2.254, e depois daí o viés inverte de forma muito forte.\n\nCódigoresultados_reamostragem_estratificada %&gt;% \n  # agrupando pelo grupo de posicao do ranking\n  group_by(estratificacao) %&gt;% \n  # calculando a estimativa central e intervalo de confianca por grupo\n  summarise(\n    'Diferença estimada' = mean(x = estimate),\n    'IC inferior'        = quantile(x = estimate, probs = 0.025),\n    'IC superior'        = quantile(x = estimate, probs = 0.975)\n  ) %&gt;% \n  # ajustando o texto da coluna de estratificacao\n  mutate(\n    estratificacao = ifelse(test = estratificacao, \n                            yes = str_glue('Até a {ponto_de_corte}º posição'), \n                            no = str_glue('Acima da {ponto_de_corte}º posição'))\n  ) %&gt;% \n  # arredondando tudo o que for numerico para duas casas decimais\n  mutate(across(where(is.numeric), \\(x) round(x, digits = 2))) %&gt;% \n  # renomeando a coluna do grupo de posicao\n  rename(Grupo = estratificacao) %&gt;% \n  # organizando a tabela em ordem decrescente\n  arrange(desc(`Diferença estimada`)) %&gt;% \n  # printando a tabela\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nTambém podemos visualizar estes mesmos resultados através da figura abaixo - focando aqui não só no intervalo de confiança em si, mas também nos valores que ficaram abaixo e cima do intervalo de confiança.\n\nCódigo# criando a figura\nresultados_reamostragem_estratificada %&gt;% \n  # estilizando o texto que aparecerá no eixo y\n  mutate(\n    estratificacao = ifelse(test = estratificacao, \n    yes = str_glue('Até a {ponto_de_corte}º posição'), \n    no = str_glue('Acima da {ponto_de_corte}º posição'))\n  ) %&gt;% \n  # plotando a figura\n  ggplot(mapping = aes(x = estimate, y = estratificacao, fill = factor(stat(quantile)))) +\n  stat_density_ridges(geom = 'density_ridges_gradient', scale = 0.95, \n                      calc_ecdf = TRUE, quantiles = c(0.025, 0.975), \n                      show.legend = FALSE) +\n  scale_fill_manual(values = c('tomato', 'grey80', 'tomato')) +\n  geom_vline(xintercept = 0, color = 'black', alpha = 0.5) +\n  labs(\n    title    = 'A diferença na nota entre os dois portais depende da posição do ranking',\n    subtitle = str_glue('O ranking da Ludopedia tem notas maiores que aquele do BoardGameGeek para os títulos que ocupem\\naté a {ponto_de_corte}º posição, a partir de onde eles passam a ser melhores avaliados neste último'),\n    caption  = 'A área cinza representa o intervalo de confiança de 95% da estimativa da diferença nas notas entre os\\ndois portais, obtidas através de 500 estimativas independentes tomadas a partir de 30 amostras aleatórias dos dados.',\n    x        = 'Diferença entre as notas do ranking da Ludopedia e do BoardGameGeek'\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nUma coisa que podemos perceber com estes resultados é que eles parecem retratar muito melhor a tendência apresentada por àquela figura que mostra relação entre a diferença entre as notas e a posição do ranking. Acho que, com isso, já posso me dar por satisfeito com as análises que fizemos, e parar por aqui."
  },
  {
    "objectID": "posts/2021-12-11_notas-boardgames/index.html#footnotes",
    "href": "posts/2021-12-11_notas-boardgames/index.html#footnotes",
    "title": "Quão similares são as notas dos jogos de tabuleiro entre os portais especializados?",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nhttps://infer.netlify.app/index.html↩︎\nExiste uma excelente explicação sobre a média bayesiana, sua utilização para rankear itens e implementação em código em: https://www.algolia.com/doc/guides/solutions/ecommerce/relevance-optimization/tutorials/bayesian-average/↩︎\nA variância calculada é de 0.642 na Ludopedia vs 0.2404 no BGG↩︎\nOu seriam sub-populações, nesse caso?↩︎"
  },
  {
    "objectID": "posts/2021-12-23_convertendo-coordenadas/index.html",
    "href": "posts/2021-12-23_convertendo-coordenadas/index.html",
    "title": "Convertendo coordenadas através da calculadora geográfica do INPE",
    "section": "",
    "text": "Há algumas semanas uma das pessoas com quem trabalho trouxe a necessidade de fazer uma conversão de coordenadas, a fim de que pudéssemos seguir com algumas análises que estávamos fazendo. Essa tarefa parecia ser coisa simples, pois deveríamos apenas passar as coordenadas da projeção1 de grau-minuto-segundo no datum2 SAD69 para a projeção de grau decimal no datum SIRGAS2000. Eu sabia que existia uma forma de fazer essas conversões a partir dos metadados disponíveis em um arquivo shapefile, tanto através do pacote sf no R quanto na lib geopandas no Python. Assim, não havia com o que se preocupar…certo?\nDe uma forma surpreendente, não consegui encontrar um jeito confiável de fazer a conversão das coordenadas nem no R e nem no Python. A primeira tentativa que fiz foi no R, e acabei esbarrando com a falta de suporte ao datum SIRGAS2000: apesar do código EPSG3 para o SIRGAS2000 existir, as funções do sf não parecem ter suporte para ela - falhando na conversão logo de cara. Com isso, fiz minha segunda tentativa usando o Python, mas fiquei meio desconfiado do output: na maior parte dos casos, parecia que o geopandas fazia a mudança de projeção e datum do arquivo shapefile sem que, no entanto, os valores das coordenadas em si fossem alteradas. Assim, acabamos esbarrando nesse bloqueio para avançar.\nComo não eram muitos pontos que deveriam ter as coordenadas convertidas - mais ou menos uns 50 -, surgiu a ideia de usar uma aplicação como QGIS para realizar as conversões. Apesar da ideia ser boa para o momento, ela traria muitos problemas no curto ou médio prazo: (1) precisávamos colocar as informações de latitude e longitude dentro de um dataframe geográfico (i.e., um geodataframe ou um sf) e setar o seu datum e projeção, (2) a partir daí precisaríamos salvar o arquivo para o disco para abrir no QGIS, (3) onde precisaríamos executar manualmente muitos passos para converter as coordenadas e exportar um novo shapefile que, (4) finalmente, poderíamos abrir no R/Python para usar. Além disso, em algum momento receberíamos mais um batch de dados, e precisaríamos repetir o procedimento todo de novo. Logo, resolvemos usar essa solução para sair do lugar naquele momento, mas precisávamos de outra estratégia para tornar essa etapa do pipeline de dados mais robusta e reprodutível.\nUma solução que propus para isso foi o uso da calculadora geográfica do INPE. Eu já havia usado ela para desenvolver um trabalho que no passado, e sabia que ali teríamos um resultado bastante confiável. Naquela época, eu havia feito a conversão das coordenadas toda de forma manual4, mas achava que seria tranquilo usar as técnicas de web scrapping que aprendi para automar o processo. No fim das contas, não foi! Todavia, acredito que o exercício foi útil para usar o Selenium para interagir com uma página dinâmica. Nesse contexto, acabei criando essa automação usando o Python, mas reproduzi os mesmos passos com o R e, neste post, aproveito o reticulate para contar sobre a solução usando tanto o Python quanto o R."
  },
  {
    "objectID": "posts/2021-12-23_convertendo-coordenadas/index.html#footnotes",
    "href": "posts/2021-12-23_convertendo-coordenadas/index.html#footnotes",
    "title": "Convertendo coordenadas através da calculadora geográfica do INPE",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nÉ a forma pela qual a superfície de uma esfera é achatada em um plano de forma a criar um mapa, envolvendo uma série de transformações para que a latitude e longitude de cada ponto seja equivalente entre os dois.↩︎\nÉ o sistema de referência utilizado para medir precisamente as distâncias no planeta ou qualquer outra entidade planetária.↩︎\nEste é um código numérico único para representar cada combinação de datum, sistemas de referência espacial, ellipsoides, transformações de coordenadas e unidades de medida.↩︎\nIsso foi há uns 5 anos atrás.↩︎"
  },
  {
    "objectID": "posts/2022-01-08_associacoes-gwent/index.html",
    "href": "posts/2022-01-08_associacoes-gwent/index.html",
    "title": "Quais as associações entre as cartas de Gwent nos decks existentes?",
    "section": "",
    "text": "Motivação\nGwent é um jogo de cartas do universo de The Witcher no qual dois jogadores se enfrentam em busca da maior pontuação em pelo menos 2 de 3 rodadas, cada uma com no máximo uns 10 turnos para cada jogador. Esta pontuação é dada pelo poder de cada carta e, também, através da forma que elas interagem entre si. A tabela abaixo traz um exemplo disto para 4 cartas pertencentes aos decks da facção Scoia’tael - uma das 6 facções existentes no jogo.\n\nCódigo# carrega os pacotes\nlibrary(tidyverse) # core\nlibrary(reactable) # para tabelas interativas\nlibrary(reactablefmtr) # para embedar imagens no reactable\n\n# carrega o exemplo\nread_rds(file = 'data/decks.rds') %&gt;%\n  # pegando quatro cartas de exemplo\n  filter(localizedName %in% c('Malena', 'Brigada Vrihedd', \n                              'Dol Blathanna: Guarda' , 'Bruxo Gato')) %&gt;% \n  # extraindo um exemplo único de cada carta\n  distinct(small, localizedName, power, texto) %&gt;% \n  # juntando o prefixo do link da imagem\n  mutate(small = paste0('https://www.playgwent.com/', small)) %&gt;% \n  # organiza as cartas em ordem alfabetica\n  arrange(localizedName) %&gt;% \n  # colocando os exemplos em um reactable\n  reactable(\n    compact = TRUE, borderless = TRUE, defaultColDef = colDef(align = 'left'), \n    style = list(fontFamily = \"Fira Sans\", fontSize = \"12px\"),\n    columns = list(\n      small         = colDef(name = '', cell = embed_img(height = 80, width = 60), maxWidth = 80),\n      localizedName = colDef(name = 'Carta', maxWidth = 140),\n      power         = colDef(name = 'Poder', maxWidth = 50),\n      texto         = colDef(name = 'Descrição')\n    )\n  )\n\n\n\n\n\n\nSe utilizarmos um exemplar de cada uma destas cartas em uma rodada, temos um poder total de 16 (i.e., cada uma das 4 cartas tem 4 de poder). Todavia, a cada turno dentro desta rodada, podemos tomar vantagem de cada uma das cartas:\n\nUsar a carta Malena para movimentar a Brigada Vrihedd para outra parte do tabuleiro, causando 2 de dano ao oponente (i.e., removendo 2 pontos do oponente);\n\nAo movimentar a carta acima, acionamos a habilidade da carta Dol Blathanna: Guarda, fazendo com que a Brigada Vrihedd ganhe mais 1 de poder;\n\nQuando o nosso turno acaba, ativamos a habilidade da carta Bruxo Gato, causando mais 1 ponto de dano ao oponente (ou 2 pontos de dano, se a rodada estiver próxima do fim);\n\nFinalmente, como a carta do Bruxo Gato se movimentou no tabuleiro, acionamos novamente a habilidade da carta Dol Blathanna: Guarda, adicionando 1 ponto de poder à ela.\n\nO combo destas 4 cartas é capaz criar uma diferença de 5 pontos entre nós e o oponente à cada turno em uma rodada (i.e., removendo um total de 3 pontos de poder dele e adicionando 2 pontos de poder à nós mesmos). Se você contar os 10 turnos da rodada, e o fato de que precisamos de 4 turnos para baixar estas cartas, este combo deve girar por uns 6 turnos - nos dando uma vantagem de 30 pontos ao final dele, caso o oponente não tome nenhuma contra-medida. Além disso, ainda existem os pontos que vamos abrindo de vantagem enquanto baixamos o combo, e que dependem muito mais da estratégia de jogo (i.e., que cartas baixar em que turnos) do que da estratégia de montagem do deck em si (i.e., que cartas incluir em um deck). De toda forma, sem as cartas certas no deck não há estratégia de jogo que segure…portanto, acredito que é muito importante saber identificar estas cartas e combiná-las da melhor forma possível.\nUma forma de identificarmos estas combinações é olhar os próprios decks existentes e tentar mapear os padrões de co-ocorrência das cartas. Neste contexto, se duas ou mais cartas tendem a aparecer juntas com grande frequência entre os decks é porquê, possivelmente, esta combinação pode estar envolvida em um combo. Assim, se conseguirmos identificar os padrões de co-ocorrência entre as cartas, poderíamos usar esta informação para desenhar decks que cujas cartas tenham maior sinergia entre si - garantindo que as contra-medidas não anulem totalmente a nossa estratégia.\nCom isto em mente, vou utilizar o scrapper da biblioteca de decks de Gwent que desenvolvi em outro post para analisar os padrões de co-ocorrência entre cartas de Gwent. Meu intuito aqui vai ser preparar a base de dados que já havíamos obtido anteriormente, fazer uma breve análise exploratória dos padrões existentes e, então, utilizar um algoritmo para minear regras de associações para: (1) identificar os conjuntos de cartas que ocorrem com uma frequência maior do que àquela esperada ao acaso, e (2) determinar quão diferentes são as regras de associação detectadas. Eu não espero que ter acesso à essas informações vá me dar uma vantagem competitiva frente aos outros jogadores, até porquê talento para estratégia de jogo eu não tenho, mas acredito que elas possam acabar me ajudando a montar decks mais efetivos do que aqueles horríveis que monto atualmente.\nAntes de começar a olhar os dados e tudo o mais, vou fazer um breve resumo sobre as peculiaridades e regras-padrão que devem ser seguidas para montagem de um deck de um Gwent. Isto vai facilitar bastante o entendimento dos dados que teremos à nossa disposição, bem como o contexto da análise exploratória dos dados.\nUm resumo sobre a montagem dos decks\n\nCada jogador pode montar quantos decks quiser, sendo que cada deck deve pertencer à uma de 6 facções: Monsters, Nilfgaard, Northern Realms, Scoia'tael, Skellige ou Syndicate;\n\nAs cartas de um deck devem pertencer à facção do próprio deck ou ser uma carta neutra. Existe uma única exceção: algumas cartas específicas da facção Syndicate podem ser usadas junto dos decks de outras facções (mas essas são a minoria das cartas do Syndicate);\n\nUm deck é composto por, no mínimo, 25 cartas: uma carta de habilidade do líder, uma carta de estratégia e o restante das cartas de escolha livre de cada jogador;\n\nCada facção têm 7 ‘cartas’ de habilidades do líder distintas, muito relacionadas à mecânica de jogo da facção;\n\nA carta de estratégia serve para dar algum tipo de vantagem à pessoa jogadora que vai iniciar a primeira rodada da partida, não estando disponível nas rodadas subsequentes, nem para a pessoa jogadora que é a segunda à jogar;\n\nAs cartas de estratégia são essencialmente neutras, mas existe uma versão específica delas para cada facção. Toda pessoa jogadora tem disponível desde o início do jogo a carta de estratégia neutra Vantagem Tática;\n\nExistem três tipos principais de cartas de escolha livre: cartas de unidade (com valor de poder, usadas no campo de batalha), cartas especiais (de efeito imediato, que são enviadas para a pilha de descarte após serem usadas) e cartas de artefato (sem valor de poder, usadas no campo de batalha). Um deck deve ter, no mínimo, 13 cartas de unidade;\n\nUma das formas de se obter uma carta no jogo é através de sua criação pagando o custo em restos (uma das moedas do jogo). Este custo de criação está diretamente relacionadao ao grupo da carta (bronze ou ouro) e seu nível de raridade: cartas de bronze comuns custam 30 restos, enquanto as de bronze raras custam 80 restos; já as cartas de ouro épicas custam 200 restos, ao passo que as de ouro lendárias custam 800 restos;\n\nO níveis de raridade de cada carta estão bastante relacionados ao quão boa ela é (com exceções, claro);\n\nPodem haver até 2 cópias de cada carta de bronze em um deck, mas apenas uma cópia de cada carta de ouro;\n\nCada carta também tem um custo de provisão, que descreve o quanto ela ‘pesa’ dentro de um deck. O custo de provisão total de cada deck não deve ultrapassar 168 pontos;\n\nQuanto maior o nível de raridade de uma carta, maior o seu custo de provisão.\nPreparação dos dados\nO scrapper da biblioteca de decks de Gwent nos fornece a lista de decks contribuídos pela comunidade no site oficial do jogo, bem como a composição de cartas associadas à cada um deles. Como existem atualizações relativamente frequentes do jogo, algumas cartas sofrem nerfs e outras podem acabar recebendo um boost, o que faz com que decks que tenham sido editados há muito tempo possam estar meio defasados com relação à sua performance. Além disso, os decks podem receber likes e dislikes da comunidade, o que funciona como um termômetro do quão bom aquele deck deve ser. Assim, resolvi focar apenas nos decks que tivessem sido editados no ano de 2021 e que receberam pelo menos um voto. Com isso, é mais provável que vamos usar decks que estejam mais atualizados e validados de alguma forma pela comunidade. O pedaço de código abaixo carrega a lista de decks disponíveis no site oficial do jogo quando escrevi este post, e os apresenta em uma tabela.\n\nCódigo# carregando os pacotes\nlibrary(tidytext) # para ajudar a trabalhar com texto\nlibrary(ggridges) # para os ridge plots\nlibrary(plotly) # para visualizacao com interatividade\nlibrary(igraph) # para plotar grafos\nlibrary(fs) # para manipular os paths\n\n# carregando os metadados de cada deck\nmetadados &lt;- read_rds(file = 'data/lista_de_decks.rds')\n\n# fazendo alguns ajustes à base dos metadados\nmetadados &lt;- metadados %&gt;% \n  # selecionando apenas as colunas desejadas\n  select(deck = id, name, faccao = slug, ano = modified, language, votes, craftingCost) %&gt;% \n  # implementando pequenos ajustes aos dados\n  mutate(\n    # ajustando a coluna de data\n    ano = lubridate::as_datetime(x = ano),\n    ano = lubridate::year(x = ano),\n    # ajustando coluna de slug\n    faccao = str_to_title(string = faccao),\n    faccao = case_when(faccao == 'Northernrealms' ~ 'Northern Realms',\n                       faccao == 'Scoiatael' ~ \"Scoia'tael\",\n                       TRUE ~ faccao)\n  )\n\n# printando a tabela como um reactable\nmetadados %&gt;% \n  reactable(\n    sortable = TRUE, filterable = TRUE, compact = TRUE,\n    highlight = TRUE, borderless = TRUE, showPageSizeOptions = TRUE,\n    defaultColDef = colDef(align = 'center'), defaultPageSize = 5,\n    style = list(fontFamily = \"Fira Sans\", fontSize = \"12px\"),\n    columns = list(\n      deck         = colDef(name = 'Deck'),\n      name         = colDef(name = 'Nome'),\n      ano          = colDef(name = 'Ano de Edição'),\n      faccao       = colDef(name = 'Facção'),\n      language     = colDef(name = 'Origem'),\n      votes        = colDef(name = 'Votos'),\n      craftingCost = colDef(name = 'Custo de Criação')\n    )\n  )\n\n\n\n\n\n\nA segunda parte do scrapper usa o código identificador único de cada deck (i.e., coluna Deck na tabela acima) para obter a sua composição de cartas e os metadados das mesmas. Eu raspei todos os decks atenderam aos dois requisitos descritos acima, compilei os resultados para um único dataframe e fiz alguns pequenos ajustes à base, só para fins de entendimento e clareza mesmo. O código abaixo carrega a base de dados com as cartas encontradas em cada deck, e cria uma tabela para visualizarmos todas as informações existentes ao nível do deck, apenas para fins de entendimento da estrutura de dados1.\n\nCódigo# carregando os dados dos decks\ndecks &lt;- read_rds(file = 'data/decks.rds')\n\n# fazendo alguns ajustes aos dados dos decks\ndecks &lt;- decks %&gt;% \n  # removendo algumas informacoes que nao precisamos\n  select(-small, -big, -fluff, -ownable, -short, -categoryName, -primaryCategoryId, -name) %&gt;% \n  # ajustando as colunas\n  mutate(\n    # passando o id do deck para um inteiro, para bater com os metadados\n    deck = as.integer(deck),\n    # ajustando coluna do slug\n    slug = str_to_title(string = slug),\n    slug = case_when(slug == 'Northernrealms' ~ 'Northern Realms',\n                     slug == 'Scoiatael' ~ \"Scoia'tael\",\n                     TRUE ~ slug),\n    # ajustando coluna do repeat count - quantidade daquela carta no deck\n    repeatCount = repeatCount + 1,\n    # contando quantidade de habilidade de cada carta\n    habilidades = case_when(is.na(keywords) ~ 0,\n                            TRUE ~ str_count(string = keywords, pattern = ';') + 1)\n  ) %&gt;% \n  # passando os outros strings para maiusculo\n  mutate(across(.cols = c(rarity, cardGroup, type), .fns = ~ str_to_title(string = .x))) %&gt;% \n  # juntando com id da faccao\n  left_join(y = select(metadados, deck, faccao), by = 'deck')\n\n# printando a tabela\ndecks %&gt;% \n  # selecionando apenas as colunas cujas informações estejam no nível do deck\n  select(deck, faccao, card_in_seq, localizedName, repeatCount, slug, type) %&gt;% \n  # passando para o reactable\n  reactable(\n    sortable = TRUE, filterable = TRUE, compact = TRUE,\n    highlight = TRUE, borderless = TRUE, showPageSizeOptions = TRUE,\n    defaultColDef = colDef(align = 'center'), defaultPageSize = 5,\n    style = list(fontFamily = \"Fira Sans\", fontSize = \"12px\"),\n    columns = list(\n      deck          = colDef(name = 'Deck'),\n      faccao        = colDef(name = 'Facção'),\n      card_in_seq   = colDef(name = 'Sequência'),\n      localizedName = colDef(name = 'Carta'),\n      repeatCount   = colDef(name = 'Unidades'),\n      slug          = colDef(name = 'Facção da Carta'),\n      type          = colDef(name = 'Tipo')\n    )\n  )\n\n\n\n\n\n De volta ao topoReusohttps://creativecommons.org/licenses/by/4.0/deed.ptCitaçãoBibTeX@online{marino2022,\n  author = {Marino, Nicholas},\n  title = {Quais as associações entre as cartas de Gwent nos decks\n    existentes?},\n  date = {2022-01-08},\n  url = {https://nacmarino.netlify.app//posts/2022-01-08_associacoes-gwent},\n  langid = {pt}\n}\nPor favor, cite este trabalho como:\nMarino, Nicholas. 2022. “Quais as associações entre as cartas de\nGwent nos decks existentes?” January 8, 2022. https://nacmarino.netlify.app//posts/2022-01-08_associacoes-gwent."
  },
  {
    "objectID": "posts/2022-01-23_api-boardgamegeek/index.html",
    "href": "posts/2022-01-23_api-boardgamegeek/index.html",
    "title": "Interagindo com a API XML do BoardGameGeek",
    "section": "",
    "text": "Um dos primeiros posts do blog foi sobre um scrapper para obtermos as informações da página do ranking do BoardGameGeek, um portal especializado em jogos de tabuleiro. Meu principal objetivo com isso não foi o de extrair o ranking em si, mas o de extrair o identificador único de cada jogo para usá-lo junto à API XML oferecida pelo portal. Essa API nos dá acesso à (praticamente) todas as informações sobre os jogos que estão disponíveis em suas respectivas páginas e, portanto, fornece um caminho mais simples para obtermos os dados sobre cada um deles de forma programática. No entanto, o mais importante para mim é que essas informações têm o potencial de facilitar uma das coisas que eu mais tenho dificuldade: encontrar os jogos que combinam com aquilo que eu curto.\nNeste post eu mostro como interagir com a API XML do BGG e, também, fazer o parser das informações que obtemos a partir dela. Esse processo vai ser importante para entendermos os tipos de dados que temos disponíveis e as possíveis formas de usá-los nos próximos posts."
  },
  {
    "objectID": "posts/2022-01-23_api-boardgamegeek/index.html#informações-genéricas",
    "href": "posts/2022-01-23_api-boardgamegeek/index.html#informações-genéricas",
    "title": "Interagindo com a API XML do BoardGameGeek",
    "section": "Informações genéricas",
    "text": "Informações genéricas\nA primeira informação geral que vamos parsear é àquela que está dentro da tag name. Para isso, precisaremos pegar tudo o que está associado à essa tag usando um xml_find_all e extrair os atributos de cada elemento com xml_attrs. O resultado dessa operação é uma lista onde cada elemento é um data.frame com uma única linha, contendo um nome do jogo e um indicador se esse nome é o oficial (i.e., primary) ou o não-oficial (i.e., alternate). Juntaremos essas linhas usando um bind_rows seguido de um select para organizar o resultado. Aplicando essa função ao conteúdo do xml, obtemos uma tabelinha com todos os nomes do jogo, onde podemos ver que o os nomes não-oficiais são normalmente aqueles em outras línguas. Essa tabela é bastante útil pois, com base nela, podemos fazer um de-para das informações do BGG para àquelas da Ludopedia, o que nos permite responder à algumas perguntas que havíamos aberto anteriormente. De toda forma, se quiséssemos extrair apenas o nome oficial do jogo, bastava usar um filter pelo valor primary, seguido de um pull da coluna título.\n\nCódigo## funcao para parsear a lista de nomes do jogo\nparser_nome &lt;- function(arquivo_xml) {\n  # pega o arquivo HTML\n  arquivo_xml %&gt;% \n    # extrai todas as tags name\n    xml_find_all(xpath = '*//name') %&gt;% \n    # extrai todo os atributos dessas tags\n    xml_attrs() %&gt;% \n    # junta todos os atributos em uma tibble\n    bind_rows() %&gt;% \n    # remove a coluna sortindex\n    select(titulo = value, metadado = type)\n}\n\n# parseando os nomes\nparser_nome(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 15 × 2\n   titulo                           metadado \n   &lt;chr&gt;                            &lt;chr&gt;    \n 1 Ticket to Ride                   primary  \n 2 Les Aventuriers du Rail          alternate\n 3 Jízdenky, prosím!                alternate\n 4 Menolippu                        alternate\n 5 Ticket to Ride سكة سفر           alternate\n 6 Wsiąść do Pociągu                alternate\n 7 Zug um Zug                       alternate\n 8 ¡Aventureros al Tren!            alternate\n 9 Τρενάκια                         alternate\n10 เกมต่อรถไฟ                        alternate\n11 チケット・トゥ・ライド・アメリカ alternate\n12 乗車券                           alternate\n13 鐵道任務                         alternate\n14 铁路环游                         alternate\n15 티켓 투 라이드                   alternate\n\n\nA próxima informação que vamos parsear é a descrição do jogo, que é um string contendo toda a estorinha e contexto sobre o jogo. Essa informação está dentro da tag description, e basta usarmos o xml_text para pegá-la.\n\nCódigo## funcao para parsear a descricao do jogo\nparser_descricao &lt;- function(arquivo_xml) {\n  # pega o arquivo HTML\n  arquivo_xml %&gt;% \n    # extrai todas as tags description\n    xml_find_first(xpath = '*//description') %&gt;% \n    # extrai o texto da descricao\n    xml_text()\n}\n\n# parseando a descricao\nparser_descricao(arquivo_xml = content(x = xml_do_jogo))\n\n[1] \"With elegantly simple gameplay, Ticket to Ride can be learned in under 15 minutes. Players collect cards of various types of train cars they then use to claim railway routes in North America. The longer the routes, the more points they earn. Additional points come to those who fulfill Destination Tickets &ndash; goal cards that connect distant cities; and to the player who builds the longest continuous route.&#10;&#10;&quot;The rules are simple enough to write on a train ticket &ndash; each turn you either draw more cards, claim a route, or get additional Destination Tickets,&quot; says Ticket to Ride author, Alan R. Moon. &quot;The tension comes from being forced to balance greed &ndash; adding more cards to your hand, and fear &ndash; losing a critical route to a competitor.&quot;&#10;&#10;Ticket to Ride continues in the tradition of Days of Wonder's big format board games featuring high-quality illustrations and components including: an oversize board map of North America, 225 custom-molded train cars, 144 illustrated cards, and wooden scoring markers.&#10;&#10;Since its introduction and numerous subsequent awards, Ticket to Ride has become the BoardGameGeek epitome of a &quot;gateway game&quot; -- simple enough to be taught in a few minutes, and with enough action and tension to keep new players involved and in the game for the duration.&#10;&#10;Part of the Ticket to Ride series.&#10;&#10;\"\n\n\nO próximo parser estrutura diversas informações relacionadas ao ano de publicação do jogo, quantidade de jogadores, tempo de jogo e idade mínima. Todas essas informações estão como tags soltas dentro do xml, portanto tive que usar o self dentro do xpath para pegar cada uma delas e colocar dentro do mesmo resultado. Entretanto, uma vez que conseguimos extrair essas informações, usamos um map_dfr para as colocarmos como um tibble e, depois, um pivot_wider para organizá-las entre colunas.\n\nCódigo## funcao para parsear as informacoes do jogo\nparser_informacoes &lt;- function(arquivo_xml) {\n  # pega o arquivo HTML\n  arquivo_xml %&gt;% \n    # extrai todas as tags relacionadas às informações sobre o ano de publicacao, quantidade de jogadores\n    # idade minima para o jogo e tempo de jogo\n    xml_find_all(xpath = '*//*[self::yearpublished or self::minplayers or self::maxplayers\n               or self::playingtime or self::minplaytime or self::maxplaytime or self::minage]') %&gt;% \n    # colocando todos os atributos dessa tag em um tibble\n    map_dfr(\n      ~ list(\n        caracteristica = xml_name(.x),\n        valor = xml_attrs(.x, 'value')\n      )) %&gt;% \n    # parseando tudo para numerico\n    mutate(valor = parse_number(x = valor)) %&gt;% \n    # passando o tibble do formato longo para o largo\n    pivot_wider(names_from = caracteristica, values_from = valor)\n}\n\n# parseando as informacoes\nparser_informacoes(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 1 × 7\n  yearpublished minplayers maxplayers playingtime minplaytime maxplaytime minage\n          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1          2004          2          5          60          30          60      8\n\n\nQuando passamos o método stats=1 para o endpoint também coletamos as informações sobre as estatísticas relacionadas ao ranking do jogo que escolhemos. Com isso vamos ter acesso: (a) às informações relativas às notas e (b) àquelas diretamente relacionadas aos rankings em que cada jogo aparece. A função abaixo dá conta de parsear o primeiro destes, nos dando acesso à quantidade de votos que cada jogo recebeu, a nota média (arimética e bayesiana) e outras informações relacionadas ao interesse dos usuários pelo jogo.\n\nCódigo## funcao para parsear todas as informacoes relacionadas à avaliação de cada jogo\nparser_avaliacoes &lt;- function(arquivo_xml) {\n  # extrai outras informacoes das avaliacoes e junta com as informacoes de rankings e contagem de comentarios\n  arquivo_xml %&gt;% \n    # extrai todas as tags relacionadas dentro das avaliacoes que nao estejam relacionadas ao rankeamento\n    xml_find_all(xpath = '*/statistics/ratings/*[not(self::ranks)]') %&gt;% \n    # coloca todas as informacoes dentro de um tibble\n    map_dfr(\n      ~ list(\n        estatistica = xml_name(.x),\n        valor = xml_attrs(.x, 'value')\n      )\n    ) %&gt;% \n    # parseando tudo para numerico\n    mutate(valor = parse_number(x = valor)) %&gt;% \n    # passando o tibble do formato longo para o largo\n    pivot_wider(names_from = estatistica, values_from = valor)\n}\n\n# parseando as avaliacoes\nparser_avaliacoes(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 1 × 12\n  usersrated average bayesaverage stddev median  owned trading wanting wishing\n       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      88186    7.39         7.28   1.30      0 127646    1081     706    7584\n# ℹ 3 more variables: numcomments &lt;dbl&gt;, numweights &lt;dbl&gt;, averageweight &lt;dbl&gt;\n\n\nJá a função abaixo parseia os rankings em que cada jogo aparece. Além do ranking geral, cada jogo também pode estar posicionado dentro da família de jogos à que pertence e/ou aos tipos de mecânica associados a ele. No nosso exemplo, podemos ver que o Ticket to Ride ocupa a 193º posição do ranking geral e a 40º posição quando o assunto são os jogos familiares.\n\nCódigo## funcao para parsear todas as informacoes relacionadas aos rankings em que cada jogo esta\nparser_rankings &lt;- function(arquivo_xml) {\n  # pega o arquivo html\n  arquivo_xml %&gt;% \n    # extrai todas as tags que estejam relacionadas ao ranking\n    xml_find_all(xpath = '*/statistics/ratings/ranks/rank') %&gt;% \n    # extrai todo os atributos dessas tags\n    xml_attrs('value') %&gt;% \n    # junta todos os atributos em uma tibble\n    bind_rows() %&gt;% \n    # renomeando colunas\n    rename(nivel = type, tipo = name, nome = friendlyname, \n           posicao = value, media_bayesiana = bayesaverage) %&gt;% \n    # parseando os numericos para tal\n    mutate(\n      posicao         = parse_number(x = posicao),\n      media_bayesiana = parse_number(x = media_bayesiana)\n    )\n}\n\n# parseando os rankings\nparser_rankings(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 2 × 6\n  nivel   id    tipo        nome             posicao media_bayesiana\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 subtype 1     boardgame   Board Game Rank      231            7.28\n2 family  5499  familygames Family Game Rank      53            7.28"
  },
  {
    "objectID": "posts/2022-01-23_api-boardgamegeek/index.html#comentários",
    "href": "posts/2022-01-23_api-boardgamegeek/index.html#comentários",
    "title": "Interagindo com a API XML do BoardGameGeek",
    "section": "Comentários",
    "text": "Comentários\nO método ratingcomments=1 faz com que tenhamos acesso aos comentários e notas individuais associadas à cada jogo. Essas informações estão dentro da tag comment, aninhada em comments, e podemos usar a função abaixo para extrair os 100 comentários que virão junto do xml. O resultado dela é um tibble onde temos uma coluna com o identificador único, a nota dada ao jogo e o comentário feito por um dado usuário. Esse identificador do usuário pode até ser usado em outros endpoints disponíveis na API, como aquele que nos dá acesso às coleções.\n\nCódigo## funcao para parsear os comentarios sobre o jogo\nparser_comentarios &lt;- function(arquivo_xml) {\n  # pega o arquivo HTML\n  arquivo_xml %&gt;% \n    # extrai todas as tags de comentario\n    xml_find_all(xpath = '*/comments/comment') %&gt;% \n    # extrai todo os atributos dessas tags\n    xml_attrs() %&gt;% \n    # junta todos os atributos em uma tibble\n    bind_rows() %&gt;% \n    # renomeando as colunas\n    rename(usuario = username, nota = rating, comentario = value) %&gt;% \n    # parseando as notas para numerico\n    mutate(nota = parse_number(x = nota))\n}\n\n# parseando os comentarios\nparser_comentarios(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 100 × 3\n   usuario          nota comentario                                             \n   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;                                                  \n 1 Steffen S.         10 \"\"                                                     \n 2 harry the horse    10 \"\"                                                     \n 3 zapator            10 \"Very accessible for all kind of gamers. A first choic…\n 4 madouc             10 \"TThis is the game of the year !   5 good reasons to b…\n 5 southj95           10 \"Excellent Game! Very easy to teach (took like 2 minut…\n 6 Jabberwock         10 \"\"                                                     \n 7 Skyjack            10 \"\"                                                     \n 8 Sunfox             10 \"Just as good as the comments say it is! Even getting …\n 9 SteffenS           10 \"\"                                                     \n10 Babluit            10 \"\"                                                     \n# ℹ 90 more rows\n\n\nUm ponto importante é que estamos limitados à obter um máximo de 100 comentários por requisição à API (usamos o método pagesize=100 para garantir que isso vá acontecer). Desta forma, se quisermos pegar os comentários de 101 à 200 (e assim sucessivamente), devemos correr entre as ‘páginas’ dos comentários usando o método page=numero (onde número é o número da página). No entanto, se fossemos fazer isso precisávamos ter noção de quantos comentários existem para poder chegar ao número máximo de páginas que podermos passar para o método. É possível extrairmos estas informações olhando diretamente a tag comments, e extraindo o atributo totalitems. Assim, para obtermos todos os comentários para esse jogo, bastaria dividirmos essa quantidade por 100, e iterar da página 1 até este máximo usando o método page junto do endpoint.\n\nCódigo## funcao para parsear a quantidade total de comentarios que um jogo tem\nparser_comentarios_total &lt;- function(arquivo_xml) {\n  # pega o arquivo html\n  arquivo_xml %&gt;% \n    # pega tudo o que está sobre a tag comments\n    xml_find_all(xpath = '*/comments') %&gt;% \n    # pega apenas o valor correspondente ao total de comentarios\n    xml_attr('totalitems') %&gt;% \n    # parseando o string para numero\n    parse_number()\n}\n\n# parseando a descricao\nparser_comentarios_total(arquivo_xml = content(x = xml_do_jogo))\n\n[1] 88564"
  },
  {
    "objectID": "posts/2022-01-23_api-boardgamegeek/index.html#resultados-das-votações",
    "href": "posts/2022-01-23_api-boardgamegeek/index.html#resultados-das-votações",
    "title": "Interagindo com a API XML do BoardGameGeek",
    "section": "Resultados das votações",
    "text": "Resultados das votações\nExistem três tags com o nome pool dentro do xml, e as funções a seguir tratam de fazer o parser deste conteúdo. Essas tags nada mais são do que os resultados de votações abertas sobre o número de jogadores, idade sugerida e dependência do idioma, nas quais os usuários do portal do BGG puderam opinar em torno desses três grupos de informação.\nO parser abaixo pega os resultados da votação relacionada ao número de jogadores, retornando as informações sobre a melhor e a pior quantidade de jogadores, além da quantidade recomendada per se. Se quiséssemos colocar essas informações em suas próprias colunas, faria bastante sentido agrupar a tabela abaixo pela coluna voto e pegar o num_jogadores que tivesse a maior quantidade de votos usando um slice_max com n = 1, passando o resultado disso para um pivot_wider depois. Não mostro como fazer isso aqui, mas seria uma opção viável para resumirmos essas informações em torno dessas três recomendações (i.e. a pior quantidade de jogadores, a melhor quantidade de jogadores, e a quantidade recomendada de jogadores).\n\nCódigo## funcao para parsear os resultados da votacao do melhor numero de jogadores para se jogar\nparser_votacao_n_jogadores &lt;- function(arquivo_xml) {\n  # pega o arquivo html\n  arquivo_xml %&gt;% \n    # extrai todas as tags com os resultados da votação relacionada ao melhor numero de jogadores\n    xml_find_all(xpath = '*/poll[@name=\"suggested_numplayers\"]/results') %&gt;% \n    # coloca tudo dentro de um tibble\n    map_dfr(\n      ~ xml_find_all(.x, xpath = 'result') %&gt;% \n        xml_attrs() %&gt;% \n        bind_rows() %&gt;% \n        mutate(numplayers = xml_attrs(.x))\n    )  %&gt;% \n    # renomeia e organiza as colunas\n    select(num_jogadores = numplayers, voto = value, num_votos = numvotes) %&gt;% \n    # parseando votos para numerico\n    mutate(num_votos = parse_number(x = num_votos))\n}\n\n# parseando o resultado da votacao da melhor quantidade de jogadores\nparser_votacao_n_jogadores(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 18 × 3\n   num_jogadores voto            num_votos\n   &lt;chr&gt;         &lt;chr&gt;               &lt;dbl&gt;\n 1 1             Best                    2\n 2 1             Recommended             7\n 3 1             Not Recommended       717\n 4 2             Best                   83\n 5 2             Recommended           582\n 6 2             Not Recommended       254\n 7 3             Best                  287\n 8 3             Recommended           615\n 9 3             Not Recommended        46\n10 4             Best                  724\n11 4             Recommended           256\n12 4             Not Recommended        15\n13 5             Best                  329\n14 5             Recommended           495\n15 5             Not Recommended        64\n16 5+            Best                    6\n17 5+            Recommended            23\n18 5+            Not Recommended       529\n\n\nO parser seguinte olha o resultado da votação sobre a idade mínima sugerida para o jogo. Novamente, se quiséssemos resumir as informações dessa tabela à uma única linha (i.e., qual a idade recomendada pela comunidade para o jogo), bastaria que usássemos um filter para reter a linha que tivesse a maior quantidade de votos (i.e., num_votos == max(num_votos)) e, então, usar um pull na coluna idade_ideal.\n\nCódigo## funcao para parsear os resultados da votacao da idade recomendada para o jogo\nparser_votacao_idade &lt;- function(arquivo_xml) {\n  # pega o arquivo html\n  arquivo_xml %&gt;% \n    # extrai todas as tags com os resultados da votação relacionada à idade recomendada para o jogo\n    xml_find_all(xpath = '*/poll[@name=\"suggested_playerage\"]/results') %&gt;% \n    # coloca tudo dentro de um tibble\n    map_dfr(\n      ~ xml_find_all(.x, xpath = 'result') %&gt;% \n        xml_attrs()\n    ) %&gt;% \n    # renomeia e organiza as colunas\n    select(idade_ideal = value, num_votos = numvotes) %&gt;% \n    # parseando votos para numerico\n    mutate(num_votos = parse_number(x = num_votos))\n}\n\n# parseando o resultado da votacao da melhor idade para o jogo\nparser_votacao_idade(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 12 × 2\n   idade_ideal num_votos\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 2                   0\n 2 3                   0\n 3 4                   3\n 4 5                   5\n 5 6                  71\n 6 8                 195\n 7 10                 42\n 8 12                  6\n 9 14                  2\n10 16                  0\n11 18                  1\n12 21 and up           0\n\n\nFinalmente, o último parser relacionado às votações fala da dependência do idioma para o jogo. Essa informação indica o quanto dependemos de entender o que está escrito no livro de regras, cartas e etc de forma a conseguir jogar. O resultado desse parser é mais uma vez uma tabelinha e, para extrair a recomendação da comunidade, bastaria que repetimos o processo descrito acima, mas usando um pull na coluna voto.\n\nCódigo## funcao para parsear os resultados da votacao sobre a dependencia do idioma para jogar o jogo\nparser_votacao_idioma &lt;- function(arquivo_xml) {\n  # pega o arquivo html\n  arquivo_xml %&gt;% \n    # extrai todas as tags com os resultados da votacao sobre a dependencia do idioma para jogar o jogo\n    xml_find_all(xpath = '*/poll[@name=\"language_dependence\"]/results') %&gt;% \n    # coloca tudo dentro de um tibble\n    map_dfr(\n      ~ xml_find_all(.x, xpath = 'result') %&gt;% \n        xml_attrs()\n    ) %&gt;% \n    # renomeia e organiza as colunas\n    select(voto = value, num_votos = numvotes) %&gt;% \n    # parseando votos para numerico\n    mutate(num_votos = parse_number(x = num_votos))\n}\n\n# parseando o resultado da votacao da dependencia do idioma\nparser_votacao_idioma(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 5 × 2\n  voto                                                             num_votos\n  &lt;chr&gt;                                                                &lt;dbl&gt;\n1 No necessary in-game text                                              186\n2 Some necessary text - easily memorized or small crib sheet              27\n3 Moderate in-game text - needs crib sheet or paste ups                    1\n4 Extensive use of text - massive conversion needed to be playable         0\n5 Unplayable in another language                                           1\n\n\nPara fechar essa seção, acho importante ressaltar que o processo que sugeri fazer aqui resume as informações em cada uma dessas três tabelas em torno de uma única quantidade - i.e., a informação com mais votos. No entanto, acredito ser possível criar algum tipo de embedding ou feature a partir das informações em cada uma dessas tabelas, que descreva de que forma a opinião dos usuários variou para aquele jogo. Isto talvez seja interessante para fazer uma caracterização mais refinada dos mesmos."
  },
  {
    "objectID": "posts/2022-01-23_api-boardgamegeek/index.html#metadados",
    "href": "posts/2022-01-23_api-boardgamegeek/index.html#metadados",
    "title": "Interagindo com a API XML do BoardGameGeek",
    "section": "Metadados",
    "text": "Metadados\nO parser que deixei para o final é aquele relacionado aos metadados do jogo, que estavam dentro da tag link. Se você puder lembrar, essa é àquela tag que tinha umas 200 ocorrências no xml e, assim, já podemos imaginar a quantidade de informação que existe nela. De fato, quando parseamos essas informações através da função abaixo, vemos que existe uma diversidade de informações sobre cada jogo. Como o resultado dessa operação são várias list-columns dentro de um tibble, vamos precisar de mais um pouquinho de trabalho para deixar esse dado organizado.\n\nCódigo## funcao para parsear os metadados do jogo\nparser_metadados &lt;- function(arquivo_xml) {\n  # pega o arquivo HTML\n  arquivo_xml %&gt;% \n    # extrai todas as tags link\n    xml_find_all(xpath = '*//link') %&gt;% \n    # extrai todo os atributos dessas tags\n    xml_attrs() %&gt;% \n    # junta todos os atributos em uma tibble\n    bind_rows() %&gt;% \n    # organiza as colunas\n    select(id_metadado = id, metadado = value, tipo_metadado = type) %&gt;% \n    # removendo o padrao boardgame do tipo de metadado\n    mutate(\n      tipo_metadado = str_replace(string = tipo_metadado, \n                                  pattern = 'boardgame', replacement = 'tbl_')\n      ) %&gt;% \n    # aninhando informacoes pelo tipo de metadado\n    nest(data = -tipo_metadado) %&gt;% \n    # passando o dado para o formato largo\n    pivot_wider(names_from = tipo_metadado, values_from = data)\n}\n\n# parseando os metadados\nparser_metadados(arquivo_xml = content(x = xml_do_jogo))\n\n# A tibble: 1 × 10\n  tbl_category     tbl_mechanic     tbl_family       tbl_expansion tbl_accessory\n  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           &lt;list&gt;        &lt;list&gt;       \n1 &lt;tibble [1 × 2]&gt; &lt;tibble [8 × 2]&gt; &lt;tibble [8 × 2]&gt; &lt;tibble&gt;      &lt;tibble&gt;     \n# ℹ 5 more variables: tbl_compilation &lt;list&gt;, tbl_implementation &lt;list&gt;,\n#   tbl_designer &lt;list&gt;, tbl_artist &lt;list&gt;, tbl_publisher &lt;list&gt;\n\n\nDesempacotando os metadados\nO resultado que obtivemos no parser dos metadados não é muito útil pois ele não está em um formato tidy. Apesar de termos uma informação diferente por coluna, ela está totalmente colapsada dentro de tibbles, o que dificulta (mas não impede) realizarmos muitas operações que seriam úteis para entender melhor os dados. Como a informação dentro de cada uma das list-columns deve ter uma estrutura diferente das demais, vamos começar tratando esse dado passando a base do formato largo para o formato longo, usando a função pivot_longer. A partir daí, vamos utilizar a função split para quebrar o tibble resultante em uma lista de tibbles por tipo de informação, desaninhando eles na sequência usando um unnest - eu descrevo o que são cada uma dessas informações e todas as demais no final desse post.\n\nCódigo## expandindo cada uma das tabelas que contem multiplas informacoes sobre cada jogo\ntabelas &lt;- parser_metadados(arquivo_xml = content(x = xml_do_jogo)) %&gt;% \n  # passando a base para o formato longo\n  pivot_longer(cols = everything(), names_to = 'tabela', values_to = 'dados') %&gt;% \n  # separando a base em listas de acordo com a dimensao\n  split(.$tabela) %&gt;% \n  # desaninhando cada tabela\n  map(.f = unnest, cols = dados) %&gt;% \n  # dropando a coluna do id da tabela\n  map(.f = select, -tabela) %&gt;% \n  # ordenando as tabelas por jogo em ordem alfabetica do metadado\n  map(.f = arrange, metadado)\ntabelas\n\n$tbl_accessory\n# A tibble: 12 × 2\n   id_metadado metadado                                       \n   &lt;chr&gt;       &lt;chr&gt;                                          \n 1 139656      Miscellaneous: Wooden Train Token Set (50 Pack)\n 2 347776      Ticket to Ride: Art Sleeves                    \n 3 147117      Ticket to Ride: Character Score Markers        \n 4 286313      Ticket to Ride: Custom Trainset                \n 5 309367      Ticket to Ride: Europe – e-Raptor Insert       \n 6 397244      Ticket to Ride: First Player Coin              \n 7 138370      Ticket to Ride: Halloween Freighter            \n 8 238031      Ticket to Ride: Milk Tankers                   \n 9 346380      Ticket to Ride: Play Pink                      \n10 402117      Ticket to Ride: Trans America Express Ingot    \n11 155492      Ticket to Ride: Translucent Trains             \n12 393265      Ticket to Ride: ZV3DCreations Insert           \n\n$tbl_artist\n# A tibble: 2 × 2\n  id_metadado metadado       \n  &lt;chr&gt;       &lt;chr&gt;          \n1 12519       Cyrille Daujean\n2 11886       Julien Delval  \n\n$tbl_category\n# A tibble: 1 × 2\n  id_metadado metadado\n  &lt;chr&gt;       &lt;chr&gt;   \n1 1034        Trains  \n\n$tbl_compilation\n# A tibble: 1 × 2\n  id_metadado metadado                        \n  &lt;chr&gt;       &lt;chr&gt;                           \n1 160069      Ticket to Ride: 10th Anniversary\n\n$tbl_designer\n# A tibble: 1 × 2\n  id_metadado metadado    \n  &lt;chr&gt;       &lt;chr&gt;       \n1 9           Alan R. Moon\n\n$tbl_expansion\n# A tibble: 167 × 2\n   id_metadado metadado                                                         \n   &lt;chr&gt;       &lt;chr&gt;                                                            \n 1 185197      Alaska (fan expansion for Ticket to Ride)                        \n 2 184013      Alice in Wonderland (fan expansion for Ticket to Ride)           \n 3 120037      Alsace (fan expansion for Ticket to Ride)                        \n 4 189445      Ancient Greece (fan expansion for Ticket to Ride)                \n 5 185198      Ancient Sicily (fan expansion for Ticket to Ride)                \n 6 197040      Antarctica (fan expansion for Ticket to Ride)                    \n 7 252142      Antarctica (fan expansion for Ticket to Ride)                    \n 8 402987      Baltic Sea (Östersjön) (fan expansion for Ticket to Ride: Rails …\n 9 223979      Barsoom (fan expansion for Ticket to Ride)                       \n10 211230      Belarus (fan expansion for Ticket to Ride)                       \n# ℹ 157 more rows\n\n$tbl_family\n# A tibble: 8 × 2\n  id_metadado metadado                                      \n  &lt;chr&gt;       &lt;chr&gt;                                         \n1 64960       Components: Map (Continental / National scale)\n2 61646       Continents: North America                     \n3 14835       Country: USA                                  \n4 70360       Digital Implementations: Board Game Arena     \n5 78432       Digital Implementations: Google Play          \n6 77349       Digital Implementations: Steam                \n7 17          Game: Ticket to Ride (Official)               \n8 78198       Misc: Watch It Played How To Videos           \n\n$tbl_implementation\n# A tibble: 17 × 2\n   id_metadado metadado                                  \n   &lt;chr&gt;       &lt;chr&gt;                                     \n 1 258140      Les Aventuriers du Rail Express           \n 2 244525      Ticket to Ride Demo                       \n 3 390092      Ticket to Ride Legacy: Legends of the West\n 4 309113      Ticket to Ride: Amsterdam                 \n 5 14996       Ticket to Ride: Europe                    \n 6 329841      Ticket to Ride: Europe – 15th Anniversary \n 7 205125      Ticket to Ride: First Journey (U.S.)      \n 8 225244      Ticket to Ride: Germany                   \n 9 366488      Ticket to Ride: Ghost Train               \n10 276894      Ticket to Ride: London                    \n11 21348       Ticket to Ride: Märklin                   \n12 253284      Ticket to Ride: New York                  \n13 31627       Ticket to Ride: Nordic Countries          \n14 371140      Ticket to Ride: Northern Lights           \n15 202670      Ticket to Ride: Rails & Sails             \n16 362541      Ticket to Ride: San Francisco             \n17 34127       Ticket to Ride: The Card Game             \n\n$tbl_mechanic\n# A tibble: 8 × 2\n  id_metadado metadado                  \n  &lt;chr&gt;       &lt;chr&gt;                     \n1 2883        Connections               \n2 2912        Contracts                 \n3 2875        End Game Bonuses          \n4 2040        Hand Management           \n5 2081        Network and Route Building\n6 2041        Open Drafting             \n7 2661        Push Your Luck            \n8 2004        Set Collection            \n\n$tbl_publisher\n# A tibble: 18 × 2\n   id_metadado metadado                   \n   &lt;chr&gt;       &lt;chr&gt;                      \n 1 23043       ADC Blackfire Entertainment\n 2 934         Bandai                     \n 3 34501       Boardgame Space            \n 4 1027        Days of Wonder             \n 5 2973        Edge Entertainment         \n 6 6784        Enigma (Bergsala Enigma)   \n 7 15605       Galápagos Jogos            \n 8 5530        Giochi Uniti               \n 9 8439        Happy Baobab               \n10 1391        Hobby Japan                \n11 18852       Hobby World                \n12 6214        Kaissa Chess & Games       \n13 8291        Korea Boardgames           \n14 3218        Lautapelit.fi              \n15 11107       Nordic Games GmbH          \n16 7466        Rebel Sp. z o.o.           \n17 33998       Siam Board Games           \n18 9234        Swan Panasia Co., Ltd.     \n\n\nComo podemos ver, todas essas informações já são tidy por si só, pois temos uma informação diferente por coluna e cada observação em uma linha distinta. Se o nosso objetivo for a manipulação e visualização de dados, então salvar cada uma dessas tabelas no formato em que estão já estaria ok. Por outro lado, se quisermos juntar essas informações para colocar todas as informações sobre um determinado jogo em uma única linha, usar elas para fazer um join acabaria replicando todas as informações em muitas linhas, o que não é desejável. Nesse contexto, teríamos duas opções para prevenir que isto ocorra, novamente dependendo do objetivo final que formos dar aos dados:\n\nSe a ideia for criar uma base para a análise de dados, então poderíamos fazer um one-hot-encoding de todas as informações e juntar todas as colunas. Isto, no entanto, geraria uma infinidade de colunas novas e uma matriz super esparsa1;\n\nSe quisermos apenas consolidar essas informações em uma única tabela, então basta que concatenemos cada uma delas em um único string por tibble e juntar tudo em uma tabela só. Para isso, podemos usar um paste0 dentro de um summarise para colapsar as informações de cada linha em um string só, seguido de um bind_rows para juntar cada tibble resultante linha a linha e, finalmente, um pivot_wider para colocar cada informação em uma coluna diferente. Para fins ilustrativos, apresento este tratamento abaixo.\n\n\nCódigo## colocando as tabelas no formato tidy\ntabelas_tidy &lt;- tabelas %&gt;% \n  # sumarizando todas as informacoes de forma a termos uma linha por jogo\n  map(.f = summarise, metadado = paste0(unique(metadado), collapse = ';')) %&gt;% \n  # colocando tudo em uma unica tabela\n  bind_rows(.id = 'informacao') %&gt;% \n  # removendo o prefixo tbl\n  mutate(informacao = str_remove(string = informacao, pattern = 'tbl_')) %&gt;% \n  # passando a base para o formato largo\n  pivot_wider(names_from = informacao, values_from = metadado)\ntabelas_tidy\n\n# A tibble: 1 × 10\n  accessory artist category compilation designer expansion family implementation\n  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;         \n1 Miscella… Cyril… Trains   Ticket to … Alan R.… Alaska (… Compo… Les Aventurie…\n# ℹ 2 more variables: mechanic &lt;chr&gt;, publisher &lt;chr&gt;\n\n\nPronto! Já temos uma visão de como colocar as informações dos metadados em um único tibble no formato tidy, seja para consolidar tudo em uma tabela só ou para a análise de dados. Vamos fechar esse post criando uma função para consolidar todo o parser."
  },
  {
    "objectID": "posts/2022-01-23_api-boardgamegeek/index.html#footnotes",
    "href": "posts/2022-01-23_api-boardgamegeek/index.html#footnotes",
    "title": "Interagindo com a API XML do BoardGameGeek",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nA solução aqui seria basicamente um pivot_wider em cada tibble, seguida de um bind_cols entre elas e delas com todas as demais informações.↩︎"
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html",
    "href": "posts/2022-02-28_card_embeddings/index.html",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "",
    "text": "Há algum tempo atrás construí um scrapper para raspar a biblioteca de decks de Gwent, de forma à usar esses dados para tomar melhores decisões na hora de montar meus próprios decks. Uma das primeiras análises que fiz com aqueles dados foi tentar entender os padrões de co-ocorrência das cartas entre os decks contribuídos pela comunidade, utilizando para isso uma análise orientada às regras de associação entre as cartas. Aquele primeiro exercício acabou sendo bastante positivo, pois consegui extrair alguns insights bastante relevantes que acabaram melhorando a minha estratégia e experiência de jogo.\nUm ponto importante daquela primeira análise é que ela olhou para os padrões de co-ocorrência de cartas conhecidos e explorados pela comunidade, deixando de fora àquelas combinações de cartas que teriam o potencial de funcionar juntas, mas que nunca foram testadas. Estas combinações normalmente implementam mecânicas específicas de jogo, que podem ser identificadas através da descrição dos efeitos associados à cada carta. Assim, se pudéssemos agrupar as cartas de acordo com os padrões de texto existente em suas descrições, então poderíamos identificar as cartas que implementam mecânicas similares e, portanto, poderiam ser usadas juntas.\nUma forma de implementar este tipo de agrupamento é através da modelagem de tópicos, uma técnica de aprendizado não-supervisionado que faz uso de modelos estatísticos para identificar temas abstratos de acordo com as palavras que compõem os textos analisados. Existem alguns modelos utilizados para esta finalidade, sendo o mais conhecido deles a LDA - Latent Dirichlet Allocation; todavia, vou utilizar este post para estudar, explorar e demonstrar as funcionalidades de um outro modelo de tópicos: o STM, Structural Topic Model (@stm). Meu objetivo com isso será utilizar este modelo para criar uma representação abstrata das cartas de acordo com seus padrões de texto e, então, utilizar esta representação para encontrar as cartas mais similares àquela que eu resolver buscar.\nIniciaremos falando um pouco sobre a aquisição dos dados e faremos uma breve análise exploratória. Passaremos para a modelagem de tópicos falando um pouquinho mais da intuição por trás do STM e, então, vamos ajustar o modelo - implementando uma busca pela quantidade de tópicos no caminho. A partir daí conduziremos algumas análises relacionadas ao pós-processamento e entedimento dos tópicos, bem como à validação do modelo. Fecharemos então o post mostrando a aplicação do modelo para mapear as cartas mais similares entre si."
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html#intuição-geral-sobre-o-stm",
    "href": "posts/2022-02-28_card_embeddings/index.html#intuição-geral-sobre-o-stm",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "Intuição geral sobre o STM",
    "text": "Intuição geral sobre o STM\nOs modelos de tópicos são uma família de modelos estatísticos usados para identificar os temas abstratos que permeiam uma coleção de textos, baseando-se na ideia de que se um texto fala sobre um determinado tema, então algumas palavras devem ocorrer com mais frequência ali do que em textos que falam sobre outros temas. Por exemplo, esse texto fala sobre o jogo Gwent, então possivelmente existirão muito mais citações à palavra ‘cartas’ aqui do que nos posts que focam em análises sobre os dados da Fórmula 1. Neste contexto, os modelos de tópicos buscam identificar àquelas estruturas latentes sem que estas sejam apresentadas explicitamente ao algoritmo, pertencendo à classe de algoritmos de aprendizagem não-supersionado.\nUm dos modelos de tópico mais famosos é a LDA (Latent Dirichlet Allocation), que tem a premissa principal de que cada texto pode ser representado por uma distribuição de tópicos (i.e., prevalência de tópicos) e que cada tópico é representado por uma distribuição de palavras (i.e., conteúdo dos tópicos). Posto de outra forma (e de maneira simplista), se conhecermos a frequência de ocorrência das palavras entre os tópicos e dentro de um texto qualquer, então podemos saber quais os principais temas que aquele texto aborda. Isto é possível pois a LDA é um modelo generativo, que enxerga cada um dos textos analisados como tendo sido amostrados a partir de uma distribuição de probabilidade que descreve uma coleção de tópicos e estes, por sua vez, de uma amostra de palavras vindas de outra distribuição de probabilidade. Ambas as probabilidades são governadas pela distribuição de Dirichlet, sendo que a prevalência dos tópicos \\(\\theta_{d}\\)1 é governada por um parâmetro \\(\\alpha\\) vindo daquela distribuição, enquanto o conteúdo dos tópicos \\(\\phi_{t}\\)2 é determinada por um parâmetro \\(\\beta\\)3. É importante notar que o \\(\\alpha\\) e \\(\\beta\\) são o mesmo parâmetro da distribuição Dirichlet; todavia, como cada um vêm de uma amostragem diferente, eles acabam recebendo notações diferentes para não causar confusão. Você pode encontrar alguns tutoriais e explicações sobre a LDA aqui, aqui, aqui e aqui.\n\nCódigoknitr::include_graphics(path = 'images/lda_rationale.png')\n\n\n\n\nApesar do processo de modelagem da LDA parecer ser complexo, a sua implementação em código é bastante simples. Ela consiste em:\n\nInicializar aleatoriamente a associação das palavras w aos tópicos t e dos documentos d aos tópicos t;\nIterar entre os documentos d e as palavras w e:\n\nAtualizar a estimativa da probabilidade de que cada documento d pertença a um tópico t olhando com que frequência palavras naquele documento são associadss ao tópico t; e,\n\nAtualizar a estimativa da probabilidade de que cada palavra w pertença a um tópico t com base na quantidade de documentos d que contém a palavra w que foram associados ao tópico t.\n\n\n\nCom isso, ao longo das iterações, acabamos tendo probabilidades bem calibradas para descrever a associação de cada documento d e de cada palavra w a um tópico t (links para exemplos de implementação da LDA a partir do zero em Python e em R). Todavia, essa simplicidade também traz consigo algumas premissas bem frágeis, como:\n\nA independência entre os tópicos;\nO fato de que as palavras associadas à cada tópico não devem diferir entre os documentos;\nA ideia de que os tópicos são determinados exclusivamente pelas palavras que os compõem; e,\nA ordem das palavras no texto não é relevante para a identificação dos tópicos, apenas a frequência de ocorrência das palavras.\n\nApesar do modelo parar de pé mesmo com essas premissas, elas dificilmente são válidas no mundo real.\nUma das alternativas que surgiram para sanar os gaps da LDA foi o Structural Topic Model (@stm). A primeira novidade implementada por esse modelo é que tanto o \\(\\theta_{d}\\) (i.e., prevalência dos tópicos) quanto o \\(\\phi_{t}\\) (i.e., conteúdo dos tópicos) podem ser moderados através do efeito de covariáveis (i.e., o \\(X\\) e o \\(\\tau\\) na figura abaixo). Isto é, algumas covariáveis podem fazer com que a prevalência de alguns tópicos seja naturalmente maior em alguns documentos do que outros (e.g., uma característica geral desses documentos) e/ou que algumas palavras de um tópico sejam usadas com mais frequência em algumas condições do que em outras (e.g., quando o sentimento do texto muda de positivo para negativo, mas o texto fala da mesma coisa). Outra novidade importante, é que o \\(\\theta_{d}\\) é gerado a partir de uma distribuição LogNormal, o que permite que incorporarmos uma estrutura de correlação entre os tópicos (i.e., o \\(\\epsilon\\) na figura abaixo). Finalmente, o \\(\\phi_{t}\\) passa a ser gerado através da soma de algumas distribuições exponenciais, que descrevem coisas como a frequência de ocorrência média das palavras nos tópicos e a contribuição das covariáveis sobre a ocorrência das palavras entre e dentro dos tópicos (i.e., o \\(\\mu\\) e o \\(\\tau\\) na figura abaixo). Todas estas melhorias contribuem para que o STM tenha um potencial de uso maior do que aquele da LDA, principalmente quando existem metadados associados à cada texto. Ainda assim, a utilização do STM para resolução de problemas de modelagem de tópicos ainda permanece pouco explorado.\n\nCódigoknitr::include_graphics(path = 'images/stm_rationale.png')\n\n\n\n\nA implementação do Structural Topic Model (STM) que vamos usar é àquela disponível no pacote stm do R (@stm). Enquanto escrevo este post, não existe uma implementação deste modelo para o Python, embora existam algumas issues e threads abertas no GitHub do STM. Com essa visão geral, vamos prosseguir agora para a preparação dos dados para a modelagem."
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html#preparação-dos-dados",
    "href": "posts/2022-02-28_card_embeddings/index.html#preparação-dos-dados",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "Preparação dos dados",
    "text": "Preparação dos dados\nA estrutura de dados que o stm espera receber é uma matriz esparsa onde teremos as cartas como as linhas e as palavras associadas àquela carta como colunas. Além disso, o conteúdo de cada ‘célula’ deve ser a quantidade de vezes que àquela palavra apareceu naquela carta. Colocar os dados nessa estrutura não é uma tarefa tão complexa, embora exija cuidado com alguns detalhes. Em primeiro lugar, vamos precisar remover algumas palavras simplesmente porque elas são muito frequentes e/ou pouco informativas. Nesse sentido, criei o vetor abaixo para armazenar todas as palavras que serão removidas durante a preparação dos dados - note que existem muitas preposições, substantivos e alguns pronomes entre elas, bem como o nome de algumas facções. Outro ponto importante é que existe uma frase-padrão que é sempre observada ao final do texto das cartas de líder de cada facção. Este texto não é nem um pouco informativo sobre as habilidades da carta e, portanto, devemos removê-lo também - note que teremos que usar expressões regulares para isso, uma vez que existe alguma diferença na forma como a frase é escrita entre algumas cartas.\n\nCódigo# lista de palavras para remover\nmy_stopwords &lt;- c('a', 'ao', 'aos', 'ate', 'cada', 'com', 'as', 'como', 'da', 'das', \n                  'de', 'dela', 'delas', 'dele', 'desta',  'deste', 'destas', 'destes',\n                  'deles', 'do', 'dos', 'disso', 'e', 'es', 'em', 'esta',  'ela', 'ele',\n                  'elas', 'eles', 'for', 'foi', 'la', 'lhe', 'mais', 'nas', 'nesta', \n                  'na', 'nas', 'nela', 'nele', 'no', 'nos', 'o', 'os', 'ou', 'para',\n                  'por', 'pelo', 'que', 'sao', 'se', 'so', 'sos', 'sem', 'seu', 'seus',\n                  'sua', 'suas', 's', 'si', 'todas', 'todos', 'tem', 'um', 'uma', 'voce',\n                  'vez', 'longa', 'distancia', 'corpo', 'duas', 'dois', 'metade', 'reinos',\n                  'norte', \"scoia'tael\", 'skellige', 'nilfgaard', 'sindicato', 'neutra',\n                  'concede', 'tiver', 'seguida', 'seja', 'caso', 'faz', 'usa', 'usar',\n                  'usando', 'usada', 'usado', 'tambem', 'houver', 'ha', 'pela', 'mesma',\n                  'tiver', 'nao', 'nessa', 'nessas', 'nesse', 'nesses', 'qualquer', \n                  'estiver', 'entre', 'unidade', 'unidades', 'mobilizacao', 'sempre', \n                  'mesmo', 'perto', 'apos', 'quando', 'neste', 'nestes', \"scoia'tel\",\n                  'enquanto')\n\n# regex da frase que precisaremos remover\ntxt &lt;- paste('Esta habilidade adiciona [0-9]{2} (?:(?:de )?recrutamento[s]?',\n             'ao limite )?de recrutamento (ao limite )?do (?:seu )?baralho.')\n\n\nTendo criado os vetores de palavras e a frase que precisaremos remover, vamos agora processar os textos de cada carta de forma a acabarmos em uma estrutura de dados que contenha uma coluna para o nome da carta, uma outra para a palavra e uma terceira para a quantidade de vezes que àquela palavra foi observada naquela carta. Para isso, vamos primeiro remover aquele padrão de texto com expressões regulares do campo de descrição da carta usando o str_remove e, na sequência, vamos quebrar o texto em palavras utilizando a função unnest_tokens. Vamos então remover toda a acentuação das palavras restantes usando o stri_trans_general e, então, remover todas àquelas palavras que listamos anteriormente e todos os números. A partir daí vamos utilizar a função str_replace para substiuir a forma do plural para o singular e/ou padronizar a escrita de algumas palavras. Finalmente, vamos utilizar a função count para determinar quantas vezes cada palavra ocorreu em cada carta. Com isso, chegamos ao resultado que havíamos planejado.\n\nCódigo# contando ocorrencias de cada token por faccao\ndf_tokens &lt;- cartas %&gt;% \n  # removendo texto comum a todas as cartas de habilidade do lider\n  mutate(\n    texto = str_remove(string = texto, pattern = txt)\n  ) %&gt;% \n  # quebrando o string em tokens\n  unnest_tokens(output = token, input = texto) %&gt;% \n  # removendo acentuacao\n  mutate(token = stri_trans_general(str = token, id = 'Latin-ASCII')) %&gt;%\n  # removendo stopwords e os digitos\n  filter(!token %in% my_stopwords,\n         str_detect(string = token, pattern = '[0-9]', negate = TRUE)) %&gt;% \n  # substituindo algumas as formas de algumas palavras\n  mutate(\n    # removendo o plural de algumas palavras em especifico\n    token = str_replace(string = token, pattern = '(?&lt;=o|a)s$', replacement = ''),\n    token = str_replace(string = token, pattern = '(?&lt;=d|t)es$', replacement = 'e'),\n    token = str_replace(string = token, pattern = '(?&lt;=r)es$', replacement = ''),\n    # padronizando a escrita de algumas habilidades e condicoes\n    token = str_replace(string = token, pattern = 'veneno|envenenamento|envenenad[ao]', replacement = 'envenena'),\n    token = str_replace(string = token, pattern = 'bloqueada|bloquei[ao]', replacement = 'bloqueio'),\n    token = str_replace(string = token, pattern = 'reforcad[ao]', replacement = 'reforcada'),\n    # padronizando a escrita de outras palavras\n    token = str_replace(string = token, pattern = 'anoes', replacement = 'anao'),\n    token = str_replace(string = token, pattern = 'aleatoria(?:mente)?', replacement = 'aleatorio')\n    ) %&gt;% \n  # contando ocorrencia dos lemmas por carta\n  count(localizedName, token, name = 'ocorrencias') \ndf_tokens\n\n# A tibble: 7,425 × 3\n   localizedName           token     ocorrencias\n   &lt;chr&gt;                   &lt;chr&gt;           &lt;int&gt;\n 1 A Fera                  batalha             1\n 2 A Fera                  campo               1\n 3 A Fera                  fim                 1\n 4 A Fera                  maior               1\n 5 A Fera                  poder               1\n 6 A Fera                  reforca             1\n 7 A Fera                  turno               1\n 8 A Terra das Mil Fábulas aleatorio           1\n 9 A Terra das Mil Fábulas baralho             1\n10 A Terra das Mil Fábulas carta               2\n# ℹ 7,415 more rows\n\n\nPodemos criar a matriz esparsa de input para o stm a partir do output do bloco de código anterior utilizando a função tidytext::cast_sparse. Essa função espera receber como argumentos o nome da variável que será mapeada para as linhas da matriz esparsa (i.e., row = localizedName; o nome das cartas), à que será mapeada para as colunas (i.e., column = token; cada uma das palavras) e àquela que contém os valores de cada célula da matriz (i.e., value = ocorrencias; a frequência com a qual cada palavra ocorre em cada carta). Com isso chegamos à matriz esparsa que precisamos para ajudar o STM aos dados.\n\nCódigo# criando matriz no formato document-feature matrix\ndf_esparsa &lt;- cast_sparse(data = df_tokens, row = localizedName, \n                          column = token, value = ocorrencias)\n\n\nAntes de fechar essa seção, existe uma coisa que acredito que valha a pena comentar. Eu acabei optando por passar algumas palavras para o singular e padronizar a escrita de outras palavras específicas em um dos blocos de código anteriores. Fiz isso pois os resultados preliminares sem essas alterações não ficaram muito legais: os tópicos às vezes separavam só as formas do singular para o plural e, em outros casos, falhavam em incluir as variantes de algumas palavras (e.g., bloqueada, bloqueio e bloqueia). Eu tentei usar o spacyr para corrigir a inflexão das palavras de forma mais robusta (i.e., lemmatização), mas como o corpus do spacy não têm palavras que remetam ao universo dos jogos de RPG, os resultados acabaram sendo até piores (e bizonhos). Assim, resolvi manter simples, e só corrigir aquilo que de fato parecia estar tendo um maior impacto na modelagem. De toda forma, deixo o código que usei para tentar fazer a lemmatização abaixo só para referência (ele não tem nenhum efeito sobre o objeto df_tokens).\n\nCódigo# carregando mais pacotes\nlibrary(spacyr) # para ajudar com lematizacao\n\n# inicializando o spacy\nspacy_initialize(model = 'pt_core_news_lg')\n\n# criando uma base de-para para lemmatizar os tokens\nde_para_lemma &lt;- distinct(df_tokens, token) %&gt;% \n  # colocando os tokens em um vetor\n  pull(token) %&gt;% \n  # parseando os tokens para o spacyr\n  spacy_parse(pos = FALSE, tag = FALSE, lemma = TRUE, dependency = FALSE) %&gt;% \n  # passando o resultado para um tibble\n  tibble %&gt;% \n  # pegando apenas as colunas que interessam\n  select(token, lemma)\n  \n# lemmatizando os tokens e contando ocorrencias\ndf_tokens &lt;- df_tokens %&gt;% \n  # juntando o de-para de lemmas aos tokens\n  left_join(y = de_para_lemma, by = 'token') %&gt;% \n  # contando ocorrencia dos lemmas por carta\n  count(localizedName, lemma, name = 'ocorrencias')"
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html#criando-features",
    "href": "posts/2022-02-28_card_embeddings/index.html#criando-features",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "Criando features",
    "text": "Criando features\nComo o STM dá suporte ao uso de covariáveis para modelar a prevalência (i.e., ‘quais são os tópicos?’) e o conteúdo dos tópicos (i.e., ‘quais palavras representam os tópicos?’), vamos criar um tibble com alguns metadados sobre cada carta que podem nos ajudar na modelagem. Mais especificamente, a análise exploratória que fizemos sugere que:\n\nOs textos de descrição das cartas representam temas distintos, mas que parecem estar relacionados ao conjunto de habilidades que cada carta possui. Posto de outra forma, existe a expectativa de que a prevalência dos tópicos deve variar em função das habilidades existentes entre as cartas (e.g., vai existir um tópico para habilidades relacionadas ao envenenamento, outro tópico para habilidades que causam dano direto e etc.); e,\n\nA forma pela qual uma mesma habilidade é implementada varia entre as facções. Isto é, espera-se que o conteúdo de cada tópico varie em função da facção à qual as cartas pertençam (e.g., se existe um tópico relacionado à habilidade de envenamento, as palavras que caracterizam este tópico vão variar facção à facção).\n\nDada estas duas hipóteses, vamos focar então em criar um tibble que nos permita endereçá-las da forma mais simples e objetiva possivel. Neste contexto, representar a relação entre o conteúdo do tópico e as facções é relativamente fácil: basta termos uma coluna que indique à qual facção pertence cada carta. Por outro lado, representar a relação entre a prevalência dos tópicos e as habilidades das cartas é uma tarefa bem mais complexa: existem 1.103 habilidades distintas, e precisaríamos criar uma coluna para cada uma dessas habilidades. Como essa alternativa não é muito prática, resolvi separar as habilidades em 3 grupos distintos baseado no meu conhecimento sobre o jogo: as habilidades que causam ou dão algum tipo de status à uma carta (e.g., sangramento, envenenamento,…), as habilidades que têm algum tipo de área de efeito (e.g., ‘causam dano à todas as unidades inimigas’) e todas as habilidades que não se encaixarem nestes dois últimos grupos. Embora esse agrupamento esteja longe de ser a melhor solução, acredito que ele possa servir pelo menos para nos dar uma noção mínima da relevância de usar a informação das habilidades para modelar a prevalência. O bloco de código abaixo cria todas estas covariáveis e as organiza no tibble df_covariáveis, que será usado dentro da função stm mais tarde.\n\nCódigo# listando todas as habilidades associadas a um status\nhab_status &lt;- c('bleeding', 'blood_moon', 'bounty', 'defender', 'doomed', 'immune', \n                'lock', 'poison', 'resilient', 'rupture', 'shield', 'spying', 'vitality',\n                'veil')\n\n# listando todas as habilidade que causam algum tipo de efeito de area\nhab_aoe &lt;- c('blood_moon', 'cataclysm', 'dragons_dream', 'fog', 'frost', 'rain', 'storm')\n\n# criando tabela com as covariaveis de cada carta\ndf_covariaveis &lt;- cartas %&gt;% \n  # codificando os dois grupos bem marcados de habilidades e criando um nivel para tudo o que\n  # nao se encaixa naqueles dois\n  mutate(\n    # habilidades associadas que causam ou dao um status a carta\n    habilidade_status = str_detect(string = keywords, \n                                   pattern = paste0(hab_status, collapse = '|')),\n    # habilidades com efeito de area\n    habilidade_aoe = str_detect(string = keywords, \n                                pattern = paste0(hab_aoe, collapse = '|')),\n    # todas as habilidades que não se encaixarem nas duas ultimas\n    habilidade_outras = str_detect(string = keywords, negate = TRUE,\n                                   pattern = paste0(c(hab_status, hab_aoe), collapse = '|'))\n  ) %&gt;% \n  # selecionando apenas as covariaveis que vamos usar\n  select(localizedName, slug, contains('habilidade')) %&gt;% \n  # substituindo os valores faltantes nas colunas das habilidades por FALSE\n  mutate(across(.cols = contains('habilidade'), .fns = replace_na, FALSE))\ndf_covariaveis\n\n# A tibble: 1,103 × 5\n   localizedName        slug  habilidade_status habilidade_aoe habilidade_outras\n   &lt;chr&gt;                &lt;chr&gt; &lt;lgl&gt;             &lt;lgl&gt;          &lt;lgl&gt;            \n 1 A Fera               Mons… FALSE             FALSE          FALSE            \n 2 A Terra das Mil Fáb… Neut… TRUE              FALSE          FALSE            \n 3 A Trufa Carnuda      Neut… TRUE              FALSE          FALSE            \n 4 A prática leva à pe… Nort… FALSE             FALSE          TRUE             \n 5 Abaya                Mons… FALSE             FALSE          TRUE             \n 6 Aberrações do Salaf… Synd… FALSE             FALSE          TRUE             \n 7 Abominação Salamand… Synd… TRUE              FALSE          FALSE            \n 8 Acônito              Neut… FALSE             FALSE          FALSE            \n 9 Adaga Cerimonial     Neut… TRUE              FALSE          FALSE            \n10 Adalbertus Kalkstein Synd… FALSE             FALSE          TRUE             \n# ℹ 1,093 more rows"
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html#determinando-a-quantidade-de-tópicos",
    "href": "posts/2022-02-28_card_embeddings/index.html#determinando-a-quantidade-de-tópicos",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "Determinando a quantidade de tópicos",
    "text": "Determinando a quantidade de tópicos\nComo em outras técnicas de aprendizado não-supervisionado voltadas ao agrupamento por similaridade, precisamos determinar a quantidade de tópicos K a ser utilizada pelo STM antes de ajustá-lo. Como não temos noção do melhor valor de K, faremos uma busca em passos incrementais de 3 tópicos dentro de um intervalo de K = 6 à K = 30 tópicos. Além disso, aproveitaremos para determinar se faz sentido ou não utilizar as covariáveis para modelar o conteúdo dos tópicos, a prevalência dos tópicos ou ambos. Nesse contexto, utilizaremos a identidade da facção como a covariável para modelar a variação no conteúdo dos tópicos (i.e., testar a hipótese de que as palavras que representam um mesmo tópico variam entre facções), e a presença ou ausência de cada um dos três grupos de habilidades na carta para modelar a prevalência dos tópicos (i.e., testar a hipótese de que a proporção dos tópicos entre as cartas variam em função da presença ou ausência destes três grupos de habilidades). Como precisaremos ajustar 36 modelos de tópicos aos dados (9 valores de K x 4 modelos com estrutura de covariáveis distintas), essa será uma etapa bem demorada e, portanto, vamos alavancar o processo multisessão do furrr para ganhar um pouco mais de velocidade.\n\nCódigo# carregando mais pacotes\nlibrary(stm) # para a modelagem de topicos\nlibrary(furrr) # para paralelizar a busca\n\n# setando o processamento paralelo\nplan(multisession)\n\n# setando a seed\nset.seed(33)\n\n# buscando melhor valor de K\nsearch_K &lt;- tibble(\n  K = seq(from = 6, to = 30, by = 3)\n) %&gt;% \n  mutate(\n    # rodando o STM padrao\n    Nenhuma = future_map(.x = K, \n                         .f = ~ stm(documents = df_esparsa, init.type = 'Spectral', \n                                    seed = 333, K = .x, verbose = FALSE),\n                         .options = furrr_options(seed = TRUE)\n    ),\n    # rodando o STM com covariaveis apenas para o conteudo dos topicos\n    Conteudo = future_map(.x = K, \n                          .f = ~ stm(documents = df_esparsa, init.type = 'Spectral', \n                                     seed = 333, K = .x, content = ~ slug, data = df_covariaveis,\n                                     verbose = FALSE),\n                          .options = furrr_options(seed = TRUE)\n    ),\n    # rodando o STM com covariaveis apenas para a prevalencia dos topicos\n    Prevalencia = future_map(.x = K, \n                             .f = ~ stm(documents = df_esparsa, init.type = 'Spectral', \n                                        seed = 333, K = .x, \n                                        prevalence = ~ habilidade_status + habilidade_aoe + habilidade_outras, \n                                        data = df_covariaveis, verbose = FALSE),\n                             .options = furrr_options(seed = TRUE)\n    ),\n    # rodando o STM com covariaveis para o conteudo o prevalencia dos topicos\n    Ambas = future_map(.x = K, \n                       .f = ~ stm(documents = df_esparsa, init.type = 'Spectral', \n                                  seed = 333, K = .x, content = ~ slug,\n                                  prevalence = ~ habilidade_status + habilidade_aoe + habilidade_outras, \n                                  data = df_covariaveis, verbose = FALSE),\n                       .options = furrr_options(seed = TRUE)\n    )\n  ) %&gt;% \n  pivot_longer(cols = c(Nenhuma, Conteudo, Prevalencia, Ambas), \n               names_to = 'tipo', values_to = 'modelos')\n\n# setando o processamento sequencial\nplan(sequential)\n\n\nUma vez que os modelos estejam ajustados, vamos calcular três métricas para nos ajudar a avaliar o ajuste do modelo aos dados para cada combinação de valor de K e tipo de covariável no modelo. A primeira métrica é a coerência semântica, que é uma medida do quão frequente as palavras que compõem um tópico co-ocorrem entre os documentos: quanto maior o valor desta métrica (i.e., menos negativo), maior a qualidade dos tópicos gerados. A métrica seguinte é a exclusividade, que mede o quanto as palavras associadas a um tópico são exclusivas a ele: quanto maior o valor desta métrica, menor o compartilhamento de palavras entre tópicos e, portanto, maior a qualidade dos tópicos. Um ponto importante aqui é que a métrica de exclusividade foi pensada para modelos sem covariáveis para o conteúdo dos tópicos, então não conseguiremos extrai-la para 2 dos 4 tipos de modelos que ajustamos; não há uma explicação junto à documentação da função ou do pacote sobre isso, mas acredito que isto ocorra pelo fato de que ao passar estas covariáveis estamos fazendo com que o modelo considere que as palavras podem ser compartilhadas entre os tópicos e, portanto, não são exclusivas. Finalmente, olharemos a dispersão dos resíduos do STM: quanto mais próximo de 1 eles são, maior a força da evidência de que a quantidade de tópicos está bem ajustada.\nA figura abaixo apresenta os padrões de variação daquelas três métricas de acordo com o valor de K e o tipo de modelo implementado. Alguns padrões importantes são:\n\nModelos com a covariável da identidade da facção para o conteúdo dos tópicos tendem a apresentar maior coerência semântica e menor dispersão dos resíduos do que modelos que não fazem uso dessa covariável para um mesmo valor de K;\n\nApesar da coerência semântica voltar a aumentar quando usamos mais de 27 tópicos, os resíduos do modelo de tópicos aumentam muito; e,\n\nA exclusividade das palavras aumenta com o valor de K, mas parece o fazem de forma mais lenta depois dos 18 tópicos.\n\n\nCódigo# extraindo as metricas de avaliacao da clusterizacao\nsearch_K %&gt;% \n  # calculando a exclusividade e a coerencia dos topicos\n  mutate(\n    coerencia     = map(.x = modelos, .f = semanticCoherence, documents = df_esparsa),\n    exclusividade = map(.x = modelos, .f = safely(exclusivity)),\n    exclusividade = map(.x = exclusividade, .f = 'result'),\n    residuos      = map(.x = modelos, .f = checkResiduals, df_esparsa),\n    residuos      = map(.x = residuos, 'dispersion')\n  ) %&gt;% \n  # dropando a coluna com os modelos\n  select(-modelos) %&gt;% \n  # desaninhando as colunas de coerencia e exclusividade\n  unnest(cols = c(exclusividade, coerencia, residuos)) %&gt;% \n  # passando a base para o formato longo\n  pivot_longer(cols = c(exclusividade, coerencia, residuos), \n               names_to = 'metrica', values_to = 'valor') %&gt;% \n  # dropando valores nulos\n  drop_na() %&gt;% \n  # agrupando pelo valor de K e da metrica\n  group_by(K, metrica, tipo) %&gt;% \n  # calculando o valor da media da metrica por valor de K\n  summarise(\n    valor = mean(x = valor, na.rm = TRUE), .groups = 'drop'\n  ) %&gt;% \n  # renomeando as metricas\n  mutate(\n    metrica = case_when(metrica == 'coerencia' ~ 'Coerência Semântica',\n                        TRUE ~ str_to_title(string = metrica))\n  ) %&gt;% \n  # criando a figura\n  ggplot(mapping = aes(x = as.factor(K), y = valor, group = tipo, color = tipo)) +\n  facet_wrap(~ metrica, scales = 'free') +\n  geom_line(size = 1, show.legend = TRUE) +\n  geom_point(fill = 'white', color = 'black', shape = 21, size = 2, show.legend = FALSE) +\n  labs(\n    title    = 'Quantos tópicos devemos usar?', \n    subtitle = 'A quantidade de tópicos escolhida deve atender ao melhor balanço entre uma alta coerência semântica e exclusividade, mas baixos resíduos',\n    x        = 'Quantidade de tópicos (K)',\n    y        = 'Valor da métrica',\n    color    = 'Covariáveis'\n  ) +\n  theme(\n    legend.position = 'bottom'\n  )\n\n\n\n\nDe acordo com os padrões observados acima e usando um pouco de parcimônia, parece que o melhor modelo de tópicos a ser utilizado é aquele que faz uso de 18 tópicos e da covariável da identidade da facção para modelar o conteúdo dos tópicos (esse modelo é mais simples do que aquele que faz uso de covariáveis para o conteúdo e a prevalência dos tópicos)."
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html#ajustando-o-modelo",
    "href": "posts/2022-02-28_card_embeddings/index.html#ajustando-o-modelo",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "Ajustando o modelo",
    "text": "Ajustando o modelo\nComo já ajustamos o modelo descrito acima quando fizemos a busca pelo valor de K e o tipo de modelo, não precisamos repetir todo o processo. Para poupar tempo, basta filtramos a linha correspondente aquele modelo de dentro do tibble que armazena os resultados da busca, e extrairmos o objeto do modelo treinado.\n\nCódigo# extraindo o melhor modelo\nmodelo &lt;- search_K %&gt;% \n  # pegando o modelo selecionado\n  filter(K == 18, tipo == 'Conteudo') %&gt;% \n  # extraindo o modelo selecionado\n  pull(modelos) %&gt;% \n  # tirando o modelo da lista\n  pluck(1)\nmodelo\n\nA topic model with 18 topics, 1103 documents and a 741 word dictionary."
  },
  {
    "objectID": "posts/2022-02-28_card_embeddings/index.html#footnotes",
    "href": "posts/2022-02-28_card_embeddings/index.html#footnotes",
    "title": "Como encontrar as cartas de Gwent mais similares entre si?",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nPrevalência do tópico = proporção de tópicos no documento.↩︎\nConteúdo do tópico = distribuição das palavras por tópico.↩︎\nA distribuição de Dirichlet possui dois parâmetros, K e \\(\\alpha\\): o primeiro representa a quantidade de categorias que serão descritas e o segundo é um parâmetro que controla quão concentrado em torno de uma categoria é a distribuição de probabilidade - maiores valores de \\(\\alpha\\) levam à uma distribuição de probabilidade mais uniforme. Assim, o \\(\\alpha\\) e o \\(\\beta\\) descritos no texto representam o mesmo parâmetro, mas parecem receber nomes diferentes apenas para não causar confusão. Eu repito isso à seguir, mas é justamente para clarificar esse ponto.↩︎\nSei que essa leitura dos temas dos tópicos parece um tanto quanto abstrata, mas para quem já jogou Gwent isso é bastante claro - então peço para que você confie em mim.↩︎\nEu usei a função rda do pacote vegan para isso, mas no formato utilizado ela funciona como uma PCA e não uma RDA (Análise de Redundância).↩︎\nNovamente, todas as leituras que seguem vêm da experiência de jogar Gwent por algum tempo.↩︎"
  },
  {
    "objectID": "posts/2022-03-25_api-ludopedia/index.html",
    "href": "posts/2022-03-25_api-ludopedia/index.html",
    "title": "Interagindo com a API da Ludopedia",
    "section": "",
    "text": "Ao longo dos últimos posts, apresentei alguns scrappers e análises baseadas nos dados disponíveis em dois portais de jogos de tabuleiro: o BoardGameGeek e a Ludopedia. Tudo o que foi desenvolvido focou muito nos rankings daqueles portais mas, recentemente, mostrei como podemos obter dados mais interessantes do que estes através da API XML do BoardGameGeek. A Ludopedia também oferece dados similares a esses, e que podem ser acessados através de uma REST API disponibilizada pelo portal. Assim, poderíamos usar essas APIs para obter o mesmo tipo de informação entre portais distintos, abrindo a possibilidade de desenvolvermos análises comparativas mais aprofundadas do que àquelas que já fizemos e explorar outras possibilidades.\nEste post mostrará o passo a passo de como interagir com a REST API da Ludopedia. Esta é uma API que necessita de autenticação OAUTH 2.0 e, portanto, começaremos mostrando como fazer esta autenticação. Na sequência, veremos como usar os endpoints da API para extrair a lista de jogos e os metadados associados a eles. Com isso, espero que este post ilustre os tipos de dados que teremos à nossa disposição, além de servir de referência para a aquisição e tratamento dos dados da Ludopedia em posts futuros. Vamos ao trabalho!"
  },
  {
    "objectID": "posts/2022-03-25_api-ludopedia/index.html#criando-o-aplicativo",
    "href": "posts/2022-03-25_api-ludopedia/index.html#criando-o-aplicativo",
    "title": "Interagindo com a API da Ludopedia",
    "section": "Criando o Aplicativo",
    "text": "Criando o Aplicativo\nO primeiro passo para a utilização da API é a criação de um aplicativo no portal da Ludopedia, que pode ser feita por aqui. Acredito que um pré-requisito para isso é que você tenha uma conta ativa no portal da Ludopedia, que pode ser criada gratuitamente e sem muito drama. Assumindo que você já tem essa conta, está logado no portal e tenha aberto o link acima, a página que você deve cair é àquela da figura abaixo. Para criar um aplicativo, não tem mistério: é só clicar no botão verde limão onde está escrito Novo Aplicativo.\n\nCódigomagick::image_read(path = 'images/cria_app_passo_1.png')\n\n\n\n\nAssim que a página seguinte for carregada, você verá um formulário conforme aquele apresentado na figura abaixo. Apesar de existirem alguns campos que podem ser preenchidos, você só precisa focar em dois deles:\n\n\nNome do Aplicativo, que será o nome que você dará para este aplicativo; e,\n\nURI de Retorno de Autorização (Redirect URI), que será a url para a qual o usuário do aplicativo será redirecionado após ter o acesso aprovado. Das coisas que pesquisei por aí, resolvi usar http://localhost:1410/.\n\nDepois de preencher aqueles dois campos, basta clicar no checkbox para concordar com o termos de uso e, então, clicar em Gravar.\n\nCódigomagick::image_read(path = 'images/cria_app_passo_2.png')\n\n\n\n\nSe tudo tiver dado certo, você deverá ser levado de volta à página inicial da criação do aplicativo, mas agora perceberá que existem algumas informações adicionais. Duas delas foi você quem definiu - o nome do aplicativo (APP) e a URI de redirecionamento (URIs) -, enquanto as outras três vêm da criação do aplicativo mesmo. Destas, já temos de cara o token de acesso (ACESS_TOKEN (Usuário)) que pode ser guardado e usado diretamente para fazer as requisições à API (com um detalhe importante que já falaremos). A outra informação importante para se guardar é àquela do APP_ID, que será usada para fazer a autenticação de forma programática no R.\n\nCódigomagick::image_read(path = 'images/cria_app_passo_3.png')\n\n\n\n\nCom isso, fechamos a criação do aplicativo que nos dá acesso à API. Se quisermos consumir os dados da API, basta usar o header AUTHORIZATION nas requisições dos endpoints, passando um string que será composto pela palavra Bearer, um espaço, e o código do token de acesso (e.g., Bearer &lt;token_de_acesso&gt;). Se, por outro lado, forem outras pessoas que usarão seu aplicativo, então precisaremos ter uma forma de entregar esse token de acesso à elas. É à essa tarefa que vamos passar agora."
  },
  {
    "objectID": "posts/2022-03-25_api-ludopedia/index.html#pegando-o-token",
    "href": "posts/2022-03-25_api-ludopedia/index.html#pegando-o-token",
    "title": "Interagindo com a API da Ludopedia",
    "section": "Pegando o Token",
    "text": "Pegando o Token\nAinda seguindo a documentação da API, e simplificando o que está escrito por lá, a aquisição de um token de acesso pelo usuário é feita pelo seu direcionamento para a url de autorização da API - https://ludopedia.com.br/oauth -, junto dos parâmetros APP_ID e REDIRECT URI que você obteve ao criar o aplicativo. Nessa parte, a documentação segue em frente para explicar mais um monte de coisas sobre a forma como a requisição deve ser feita, e que acaba ficando muito confusa. Entretanto, duas instruções são importantes: (1) uma vez que a autorização é feita, a requisição de acesso do token deve ser encaminhada para a url https://ludopedia.com.br/tokenrequest/ e, como resposta, (2) essa url retornará um arquivo JSON com o token de acesso. Vamos operacionalizar este processo usando as funções disponíveis no pacote httr, onde:\n\nFaremos uso função httr::oauth_endpoint para definir os endpoints de autorização e requisição do token de acesso. Para isso, definiremos os argumentos authorize = https://ludopedia.com.br/oauth e access = 'https://ludopedia.com.br/tokenrequest/', deixando o argumento request como NULL;\n\nUsaremos a função httr::oauth_app para submeter as credenciais de acesso para a autenticação, o que nos trará como resultado a chave que trocaremos posteriormente pelo token de acesso. Essa função levará como argumentos o nome do aplicativo (appname = &lt;nome do seu aplicativo&gt;) e a URI de Retorno de Autorização (redirect_uri = &lt;URI de retorno&gt;) que você criou, além do APP_ID que foi gerado depois que você criou o aplicativo no portal da Ludopedia (key = &lt;APP_ID do seu aplicativo&gt;); e,\n\nUsaremos a função httr::init_oauth2.0 para trocar a chave pelo token de acesso através dos endpoints que definimos. Essa função leva dois argumentos: o objeto criado no item 1 acima irá para o argumento endpoint, e o objeto do item 2 irá para o argumento app.\n\nSe tudo estiver certo, a função httr::init_oauth2.0 deve retornar um arquivo JSON (que você possivelmente visualizará como uma lista no R) onde um dos elementos é o access_token. Com isso, basta juntar este elemento com o string Bearer (que está dentro do elemento token_type no mesmo arquivo JSON), e teremos o token de acesso completo que deve ser usado nas requisições para a API. Falando nisso, vou usar a função que criamos abaixo para pegar o token de acesso que usaremos durante esse post.\n\nCódigo# carregando pacotes\nlibrary(tidyverse) # core\nlibrary(jsonlite) # para arquivos json\nlibrary(httr) # para o scraping\nlibrary(fs) # para mexer com paths\n\n# função para fazer a autenticacao do aplicativo\npegar_access_token &lt;- function(APP, APP_ID, URIs) {\n  \n  # setando o endpoint de acordo com os dados fornecidos na ludopedia\n  ludopedia_endpoint &lt;- httr::oauth_endpoint(\n    request = NULL,\n    authorize = 'https://ludopedia.com.br/oauth/',\n    access = 'https://ludopedia.com.br/tokenrequest/'\n  )\n  \n  # setando as configurações do aplicativo conforme definido no site da ludopedia\n  ludopedia_app &lt;- httr::oauth_app(\n    appname = APP,\n    key = APP_ID,\n    secret = NULL,\n    redirect_uri = URIs\n  )\n  \n  # pegando a autorizacao através do OAUTH2.0\n  autorizacao &lt;- httr::init_oauth2.0(endpoint = ludopedia_endpoint, \n                                     app = ludopedia_app)\n  \n  # criando string do token\n  access_token &lt;- paste0(autorizacao$token_type, ' ', autorizacao$access_token)\n  \n  # retornando o token de acesso\n  return(access_token)\n}\n\n# fazendo a autenticacao no portal\nmeu_token &lt;- pegar_access_token(\n  APP = 'meu_app', \n  APP_ID = Sys.getenv('APP_ID'), \n  URIs = 'http://localhost:1410/'\n)\n\n\n\n\n\n\n\n\nCoisas do quarto…\n\n\n\nO procedimento acima faz abrir uma janelinha no seu navegador para autorizar o acesso ao seu aplicativo. No entanto, o quarto não estava abrindo essa janela quando coloquei o post para renderizar. Não encontrei nada que pudesse explicar esse comportamento, tampouco algo que pudesse ajudar para deixar a função como ela estava anteriormente. Portanto, eu deixei o meu token hard-coded abaixo só por conta do post mesmo - mas se você for rodar o código linha a linha, o processo acima deve funcionar.\n\n\n\nCódigo# carregando pacotes\nlibrary(tidyverse) # core\nlibrary(jsonlite) # para arquivos json\nlibrary(httr) # para o scraping\nlibrary(fs) # para mexer com paths\n\nmeu_token &lt;- paste0('Bearer ', Sys.getenv('APP_TOKEN'))"
  },
  {
    "objectID": "posts/2022-03-25_api-ludopedia/index.html#pegando-a-lista-de-jogos",
    "href": "posts/2022-03-25_api-ludopedia/index.html#pegando-a-lista-de-jogos",
    "title": "Interagindo com a API da Ludopedia",
    "section": "Pegando a lista de jogos",
    "text": "Pegando a lista de jogos\nO primeiro endpoint que exploraremos é aquele que retorna a lista de jogos cadastrados no portal da Ludopedia. Para facilitar o uso deste endpoint, escrevi a função pega_lista_jogos abaixo, que já traz o endereço do endpoint, junto do header com o token de acesso (i.e., add_headers(AUTHORIZATION = access_token)) e uma lista com os parâmetros da query que precisa ser feita. Embora hajam alguns parâmetros disponíveis para esta query, focaremos em dois deles aqui: rows, que define a quantidade máxima de jogos que teremos como resposta por requisição (o limite é naturalmente 100 jogos por requisição, mas decidi deixar isso claro na função), e page, que define a página da lista dos jogos cadastrados que buscaremos (mais sobre isso no outro parágrafo). Como de costume, salvamos o arquivo JSON que recebemos de resposta em disco, de forma a acessá-lo à qualquer outro momento sem ter que ficar repetindo a requisição.\n\nCódigo# função para pegar todos os jogos em uma dada página\npegar_lista_jogos &lt;- function(pagina, access_token, path_salvar = NULL) {\n  # pegando os jogos da pagina selecionada\n  httr::GET('https://ludopedia.com.br/api/v1/jogos', \n            httr::add_headers(AUTHORIZATION = access_token),\n            query = list(rows = 100, page = pagina),\n            write_disk(path = stringr::str_glue(path_salvar, '/lista_jogos_pagina_{pagina}.json'),\n                       overwrite = TRUE))\n}\n\n# pegando a primeira pagina da lista de jogos\nlista &lt;- pegar_lista_jogos(pagina = 1, access_token = meu_token, path_salvar = path_para_jogos)\n\n\nO parser do arquivo que recebemos não tem muito mistério. Precisaremos carregar o arquivo de resposta salvo em disco usando a função jsonlite::read_json e definindo o argumento simplifyDataFrame = TRUE, para estruturarmos tudo o que conseguirmos como um data.frame. O objeto resultante dessa operação será uma lista, onde o primeiro elemento, jogos, será a tabela com a lista dos 100 jogos disponíveis na primeira página da API. Focando neste elemento, precisaremos de mais um tratamento para passar o data.frame com a lista de jogos para um tibble e, eventuamente, colocá-la em um formato tal que não hajam list-columns.\n\nCódigo# função para parsear uma pagina contendo a lista de jogos cadastrados na base da Ludopedia\nparser_lista_jogos &lt;- function(target_path) {\n  # carregando o arquivo json\n  jsonlite::read_json(path = target_path, simplifyDataFrame = TRUE) %&gt;% \n    # extraindo o elemento correspondente aos jogos\n    pluck('jogos') %&gt;% \n    # colocando como tibble\n    as_tibble() %&gt;% \n    # deixando a tibble desaninhada\n    unnest(cols = everything())\n}\n\n# aplicando o parser da lista de jogos\nparser_lista_jogos(target_path = dir_ls(path = path_para_jogos)) %&gt;% \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nCom base nessa informação, já temos acesso a lista de alguns jogos cadastrados no portal da Ludopedia, No entanto, ainda há o segundo elemento daquela lista que acabamos de parsear, total, que nos informa sobre a quantidade total de jogos cadastrados. Para pegar esta informação, basta mudarmos o string que passamos para a função pluck, conforme apresentado abaixo.\n\nCódigo# função para pegar a quantidade total de jogos disponiveis\npegar_total_jogos &lt;- function(access_token) {\n  # faz um GET da pagina default dos jogos, parseia o conteudo e pega so o elemento\n  # da lista que contem a quantidade total de jogos cadastrados na base da Ludopedia\n  httr::GET('https://ludopedia.com.br/api/v1/jogos', \n            httr::add_headers(AUTHORIZATION = access_token)) %&gt;% \n    httr::content() %&gt;% \n    purrr::pluck('total') %&gt;% \n    as.integer()\n}\n\n# pegando o total de jogos cadastrados na ludopedia\ntotal_de_jogos &lt;- pegar_total_jogos(access_token = meu_token)\ntotal_de_jogos\n\n[1] 58640\n\n\nComo podemos notar, existem 58.640 cadastrados no portal da Ludopedia. Como temos acesso à 100 jogos por página da requisição, isto nos diz que precisaremos percorrer 58.640 / 100 = 586.4 = 587 páginas para obter a lista de todos os jogos disponíveis (i.e., total de jogos dividido por 100, arredondando o resultado para cima). Assim, bastaria iterar os valores do argumento pagina da função pegar_lista_jogos do índice 1 até o número de páginas definido acima dentro de um purrr::map, por exemplo.\nUsamos o endpoint da lista de jogos cadastrados para obter os nomes dos jogos e o seu identificador numérico único, o que é ótimo se não tivermos noção do nome dos jogos nem nada do gênero. Por outro lado, se tivermos um jogo específico em mente e quisermos obter apenas o seu identificador numérico, podemos passar o seu nome como um argumento para a query do mesmo endpoint (search = &lt;nome do jogo&gt;). Eu empacotei essa aplicação do endpoint em outra função, e usei como exemplo a busca pelo identificador do jogo Ticket to Ride (que usamos como exemplo lá no post sobre a API XML do BoardGameGeek). O resultado que obtivemos (já parseado) revela que este uso do endpoint traz todos os jogos que, de alguma forma, tenham haver com aquele que buscamos - no nosso caso, também tivemos acesso à todas as expansões e implementações do jogo base.\n\nCódigo# função para pegar a lista de jogos que contenham uma string especifica no nome\npegar_id_jogo &lt;- function(nome_do_jogo, access_token) {\n  # pegando os jogos da pagina selecionada\n  httr::GET('https://ludopedia.com.br/api/v1/jogos', \n            httr::add_headers(AUTHORIZATION = access_token),\n            query = list(search = nome_do_jogo))\n}\n\n# pegando o exemplo do ticket to ride, como no scrapper do boardgamegekk\npegar_id_jogo(nome_do_jogo = 'Ticket to Ride', access_token = meu_token) %&gt;% \n  # pegando o content da resposta\n  content(simplifyDataFrame = TRUE) %&gt;% \n  # extraindo o elemento jogos de dentro da lista\n  pluck('jogos') %&gt;% \n  # criando a tabela para a visualização na página\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nAcredito que estas são as funcionalidades básicas mais importantes deste endpoint. Vamos passar ao próximo!"
  },
  {
    "objectID": "posts/2022-03-25_api-ludopedia/index.html#pegando-as-informações-de-um-jogo",
    "href": "posts/2022-03-25_api-ludopedia/index.html#pegando-as-informações-de-um-jogo",
    "title": "Interagindo com a API da Ludopedia",
    "section": "Pegando as informações de um jogo",
    "text": "Pegando as informações de um jogo\nO outro endpoint que exploraremos é aquele que traz os metadados dos jogos de tabuleiro, e que faz uso do identificador numérico do jogo diretamente na url da requisição. Desta forma, as informações da lista de jogos cadastrados no portal da Ludopedia passa a ser muito importante para utilizarmos este endpoint. Tendo isto em mente, vamos preparar a função pegar_infos_jogo adicionando um argumento que receberá o identificador do jogo (id_jogo) que buscaremos - novamente, utilizando o header com o token de acesso e salvando o arquivo JSON de resposta em disco para usarmos depois. Como exemplo, vamos usar essa função para pegar os metadados do Ticket do Ride.\n\nCódigo# função para pegar as informações de um jogo\npegar_infos_jogo &lt;- function(id_jogo, access_token, path_salvar = NULL) {\n  # pegando os detalhes do jogo\n  httr::GET(url = stringr::str_glue('https://ludopedia.com.br/api/v1/jogos/{id_jogo}'), \n            add_headers(AUTHORIZATION = access_token),\n            write_disk(path = stringr::str_glue(path_salvar, '/info_do_jogo_{id_jogo}.json'),\n                       overwrite = TRUE))\n}\n\n# pegando as informações do Ticket to Ride\nmetadados &lt;- pegar_infos_jogo(\n  id_jogo = 2, \n  access_token = meu_token, \n  path_salvar = path_para_metadata\n)\n\n\nVamos dar uma olhada na estrutura do arquivo JSON que recebemos como resposta, de forma a saber o que teremos que parsear.\n\nCódigojsonlite::read_json(path = dir_ls(path_para_metadata), simplifyDataFrame = TRUE) %&gt;% \n  glimpse\n\nList of 21\n $ id_jogo         : int 2\n $ nm_jogo         : chr \"Ticket to Ride\"\n $ thumb           : chr \"https://storage.googleapis.com/ludopedia-capas/2_t.jpg\"\n $ tp_jogo         : chr \"b\"\n $ link            : chr \"https://ludopedia.com.br/jogo/ticket-to-ride\"\n $ ano_publicacao  : int 2004\n $ ano_nacional    : int 2014\n $ qt_jogadores_min: int 2\n $ qt_jogadores_max: int 5\n $ mecanicas       :'data.frame':   2 obs. of  2 variables:\n  ..$ id_mecanica: int [1:2] 3 18\n  ..$ nm_mecanica: chr [1:2] \"Gerenciamento de Mãos\" \"Construção de Redes e Rotas\"\n $ categorias      :'data.frame':   1 obs. of  2 variables:\n  ..$ id_categoria: int 111\n  ..$ nm_categoria: chr \"Jogo de Entrada\"\n $ temas           :'data.frame':   1 obs. of  2 variables:\n  ..$ id_tema: int 27\n  ..$ nm_tema: chr \"Transportes\"\n $ artistas        :'data.frame':   2 obs. of  2 variables:\n  ..$ id_profissional: int [1:2] 6 5\n  ..$ nm_profissional: chr [1:2] \"Julien Delval\" \"Cyrille Daujean\"\n $ designers       :'data.frame':   1 obs. of  2 variables:\n  ..$ id_profissional: int 4\n  ..$ nm_profissional: chr \"Alan R. Moon\"\n $ vl_tempo_jogo   : int 45\n $ idade_minima    : int 8\n $ qt_tem          : int 4160\n $ qt_teve         : int 494\n $ qt_favorito     : int 545\n $ qt_quer         : int 2207\n $ qt_jogou        : int 5280\n\n\nComo podemos ver, existem duas principais estruturas de dados no arquivo. Em uma destas estruturas estão as informações armazenadas em vetores com um único elemento, como o ano de publicação do jogo, informações sobre quantidade de jogadores e título do jogo. A outra estrutura existente no arquivo são algumas informações armazenadas como data.frame, como as informações da mecânica de jogo, os temas e nome dos criadores do jogo. Estas informações estão nesta estrutura pois elas tendem a ser naturalmente multidimensionais, e possivelmente é mais fácil armazená-las em uma estrutura na qual você consiga discriminar o metadado linha a linha. Dada a natureza destas duas estruturas de dados, podemos parsear a primeira delas de forma bem simples, usando a função purrr::discard para descartar todos os elementos do arquivo JSON que sejam um data.frame (i.e., empregando o teste lógico is.data.frame), e juntando todos os elementos restantes coluna à coluna (i.e., usando um bind_cols).\n\nCódigojsonlite::read_json(path = dir_ls(path_para_metadata), simplifyDataFrame = TRUE) %&gt;% \n  # descartando todos os elementos que sao listas\n  discard(is.data.frame) %&gt;% \n  # juntando cada elemento da lista coluna a coluna\n  bind_cols() %&gt;% \n  # visualizando a tabela\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nA informação que buscaremos extrair da segunda estrutura de dados é àquela associada às colunas que têm o prefixo nm_ que contém as strings que definem àquela entidade. Note que em todos esses casos nós estamos interessados em extrair as informações que estão na segunda coluna de cada data.frame e que, portanto, não precisamos focar no nome da coluna em si para ter acesso à elas, apenas na sua posição. Além disso, pode existir mais de um elemento nesta coluna, e precisaremos concatenar estas informações em um string só. Assim, usaremos a função purrr::keep para reter todos os elementos que são um data.frame e, então, extrairemos a segunda coluna de cada uma delas mapeando a função purrr::pull. Finalmente, mapearemos um paste0 à cada elemento da lista para concatenar o seu conteúdo a um string só, e usaremos um bind_cols para colocar tudo como um tibble.\n\nCódigo# lendo o arquivo JSON com os metadados do jogo\njsonlite::read_json(path = dir_ls(path_para_metadata), simplifyDataFrame = TRUE) %&gt;% \n  # retendo apenas os elementos que sao dataframes\n  keep(is.data.frame) %&gt;% \n  # pegando a coluna que contem os o nome do metadado\n  map(.f = pull, var = 2) %&gt;% \n  # passando o vetor de strings para um unico string por elemento da lista\n  map(.f = paste0, collapse = ';') %&gt;% \n  # juntando os elementos da lista coluna a coluna\n  bind_cols() %&gt;% \n  # visualizando a tabela\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nComo estes dois passos são baseados no mesmo dado e devem compor o mesmo parser, vamos empacotá-los em uma única função, parser_infos_jogos.\n\nCódigo# função para parsear as informações de um jogo\nparser_infos_jogo &lt;- function(target_path){\n  # carregando o arquivo JSON com os metadados do jogo\n  metadados &lt;- jsonlite::read_json(path = target_path, simplifyDataFrame = TRUE)\n  # colocando todos os metadados em um tibble\n  bind_cols(\n    # parseando as informacoes que estao organizadas como strings ou numeros\n    metadados %&gt;% \n      # descartando todos os elementos que sao dataframes\n      discard(is.data.frame) %&gt;% \n      # juntando cada elemento da lista coluna a coluna\n      bind_cols(),\n    # parseando as informacoes que estao organizadas como uma lista de dataframes\n    metadados %&gt;% \n      # retendo apenas os elementos que sao dataframes\n      keep(is.data.frame) %&gt;% \n      # pegando a coluna que contem os o nome do metadado\n      map(.f = pull, var = 2) %&gt;% \n      # passando o vetor de strings para um unico string por elemento da lista\n      map(.f = paste0, collapse = ';') %&gt;% \n      # juntando os elementos da lista coluna a coluna\n      bind_cols()\n  )\n}\n\n## usando a função para parsear todas as informações\nparser_infos_jogo(target_path = dir_ls(path_para_metadata)) %&gt;% \n  # visualizando a tabela completa\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nPronto! Com isso podemos ter acesso às informações que caracterizam cada um dos jogos de tabuleiro cadastrados no portal da Ludopedia."
  },
  {
    "objectID": "posts/2022-03-25_api-ludopedia/index.html#outros-usos-da-api",
    "href": "posts/2022-03-25_api-ludopedia/index.html#outros-usos-da-api",
    "title": "Interagindo com a API da Ludopedia",
    "section": "Outros usos da API",
    "text": "Outros usos da API\nNo momento em que escrevo este post, a API da Ludopedia ainda está em fase de desenvolvimento e, portanto, existem algumas funcionalidades que estão previstas mas ainda não foram implementadas. De toda forma, no estado atual dela também podemos ter acesso à uma informação muito interessante: à coleção de jogos de tabuleiro de cada usuário. Este dado é legal pois ele traz coisas como as notas que a pessoa usuária deu à cada jogo, os comentários que a pessoa usuária fez sobre ele, se a pessoa tem ou não aquele jogo e coisas do gênero. Assim, ainda que a API não esteja completa, ela tem muitos outros potenciais de uso para a análise de dados.\nUm exemplo de uso daqueles dados poderia ser na criação de algum tipo de sistema de recomendação, uma vez que podemos mapear as notas dadas à cada jogo de tabuleiro por cada um dos usuários. Não é o meu intuito detalhar aqui como usar a API para extrair este tipo de informação, mas podemos ter uma visão bem alto nível através da documentação da API: usamos um endpoint para extrair a coleção de jogos de uma pessoa usuária qualquer através do seu identificador (i.e., https://ludopedia.com.br/api/v1/colecao); este identificador, por sua vez, pode ser extraído a partir de um outro endpoint, que traz a lista de usuários cadastrados no portal (i.e., https://ludopedia.com.br/api/v1/usuarios); por fim, temos acesso a um máximo de 100 jogos por vez da coleção de cada pessoa, e precisaremos usar o mesmo método para a paginação das querys para extrair a coleção completa de cada usuário - e é possível determinar o tamanho da coleção de cada usuário diretamente através de um outro endpoint (i.e., https://ludopedia.com.br/api/v1/colecao/count). É claro que precisaremos aplicar algum tratamento aos dados que a API nos retorna para chegar à estrutura necessária para um sistema de recomendação, mas acredito que dê para pegar a ideia geral aqui."
  },
  {
    "objectID": "posts/2022-06-24_state-of-data-2021/index.html",
    "href": "posts/2022-06-24_state-of-data-2021/index.html",
    "title": "Qual a diferença entre júnior, pleno e sênior?",
    "section": "",
    "text": "Nota\n\n\n\nEste post foi a minha submissão para o Challenge do State of Data 2021, que me rendeu o prêmio de 3º lugar."
  },
  {
    "objectID": "posts/2022-06-24_state-of-data-2021/index.html#footnotes",
    "href": "posts/2022-06-24_state-of-data-2021/index.html#footnotes",
    "title": "Qual a diferença entre júnior, pleno e sênior?",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nNa análise exploratória que fiz inicialmente com os dados da pesquisa cheguei a usar todas as perguntas prefixadas com P4 e P8, individualmente e tudo junto, mas os resultados foram os mesmos do que aqueles apresentados aqui. Então resolvi ir um pouco mais simples e definir como critério de inclusão da pergunta o caso dela estar falando ou não do momento atual da pessoa.↩︎\nCheguei a utilizar outras métricas de distância, mas os resultados foram praticamente os mesmos e, então, resolvi ficar com a opção mais simples mesmo.↩︎\nEsse resultado pode ser encontrado dentro do objeto permutacao, e suprimi ele no texto somente para fins de claridade.↩︎\nEsse resultado pode ser encontrado dentro do objeto analise_dispersao, e suprimi ele no texto somente para fins de claridade.↩︎\nSei que esse tema pode ser controverso e, assim como na academia, nem toda pessoa com um diploma de baixo do braço tem o mesmo grau de preparação e prontidão que um par seu…todavia, acho importante considerar este aspecto de alguma forma nas análises pois cada vez mais vemos pessoas saindo da academia e indo para o mercado de trabalho.↩︎\nA interação entre o tamanho da empresa e o tipo de indústria testará a possibilidade de que o nível de senioridade da pessoa difere entre os tipos de indústria para um mesmo tamanho de empresa.↩︎\nA interação entre o tamanho da empresa e a existência de um cargo de cientista de dados testará a possibidade de que a importância do cargo não é a mesma entre empresas e.g. de pequeno e grande porte. De forma similar, a interação entre o tipo de indústria e posse do cargo de cientista de dados testará a possibilidade de que a importância do cargo para a definição do nível de senioridade da pessoa varia de um tipo de indústria para a outra.↩︎\nNão sei se é claro para todos, mas acredito que você precisa ter um diploma de nível superior completo para fazer uma pós-graduação; assim, dá para deduzir uma coisa pela outra.↩︎\nTambém não sei se é claro para todo mundo, mas você não precisa de um diploma de pós-graduação lato sensu para fazer um stricto sensu ou vice-versa; mas, mais comumente, você precisa de um diploma de mestrado para fazer o doutorado - e, em todos esses casos, você precisa ter uma graduação de ensino superior completa.↩︎"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV (pt-br)",
    "section": "EDUCATION",
    "text": "EDUCATION\nDoctor of Philosophy – Econometrics and Business Statistics  Monash University Aug 2021 — Present\n\nNew principles and methods for complex data preparation and integration, with applications to official statistics, web-scrapped data and satellite raster images\nMonash Data Futures Institute PhD Top-Up Scholarship (2021-2024)\nExpected submission date: Nov 2024"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV (pt-br)",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nData Scientist\nFreelance, Multiple Clients Jan 2020 – June 2021\nProviding data collection, pre-processing, exploratory analysis and modelling services to clients in the early R&D stages of developing data driven products. Projects include:\n\nData description and preliminary product feasibility insights for a start-up real estate bond platform; including assessing suitability of various property transaction databases for use in initial product prototype.\nDevelopment of statistical anomaly detection regimes and key historical insights from internet quality time-series data for use in parametric insurance products, including documenting analysis tools in an R package."
  },
  {
    "objectID": "cv.html#presentations-and-workshops",
    "href": "cv.html#presentations-and-workshops",
    "title": "CV (pt-br)",
    "section": "PRESENTATIONS AND WORKSHOPS",
    "text": "PRESENTATIONS AND WORKSHOPS\n\nTalk: Misadventures with Reproducibility in R (30 Nov 2022, R Ladies Melbourne Meetup)\nTalk: Designing R Packages (4 Oct 2022, Monash EBS Data Science Research Software Study Group)\nTalk: Quarto Websites as Research Compendiums (16 Aug 2022, Monash EBS Data Science Research Software Study Group)\nWorkshop: Writing academic papers with Rmarkdown and friends (9 Aug 2022, Monash University)"
  },
  {
    "objectID": "cv.html#references",
    "href": "cv.html#references",
    "title": "Currilum vitae",
    "section": "References",
    "text": "References\nAvailable upon request"
  },
  {
    "objectID": "cv-en.html",
    "href": "cv-en.html",
    "title": "Resume",
    "section": "",
    "text": "nac.marino@gmail.com"
  },
  {
    "objectID": "cv-en.html#education",
    "href": "cv-en.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\nPostdoctoral Research in Ecology – Climate Change and Trophic Interactions Universidade Federal do Rio de Janeiro Mar 2016 — Jun 2019\n\nEffect of climate change on ecological interactions\nFellow of the National Postdoctoral Program (CAPES)\n\nPh.D. in Ecology – Climate Change, Community Ecology, and Ecosystems Universidade Federal do Rio de Janeiro Mar 2012 — Feb 2016\n\nThe effect of predators on the structure and functioning of trophic chains and their interaction with climate change\nCAPES Fellow\nBest Ph.D. thesis defended in the Ecology Graduate Program (UFRJ) in 2016\nSandwich Ph.D. period at the University of British Columbia (Oct 2014 - May 2015)\n\nMaster’s in Ecology – Community Ecology and Ecosystems Universidade Federal do Rio de Janeiro Aug 2009 — Jul 2011\n\nStructuring of the community of aquatic macroinvertebrates in tank bromeliads\nCAPES Fellow\n\nBachelor’s in Biological Sciences – Ecology Universidade Federal do Rio de Janeiro Aug 2005 — Jul 2009\n\nTank bromeliads as islands: the influence of habitat characteristics on the richness of aquatic macroinvertebrates\nScientific Initiation - FAPERJ Fellow"
  },
  {
    "objectID": "cv-en.html#work-experience",
    "href": "cv-en.html#work-experience",
    "title": "Resume",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nData Scientist\nFreelance, Multiple Clients Jan 2020 – June 2021\nProviding data collection, pre-processing, exploratory analysis and modelling services to clients in the early R&D stages of developing data driven products. Projects include:\n\nData description and preliminary product feasibility insights for a start-up real estate bond platform; including assessing suitability of various property transaction databases for use in initial product prototype.\nDevelopment of statistical anomaly detection regimes and key historical insights from internet quality time-series data for use in parametric insurance products, including documenting analysis tools in an R package."
  },
  {
    "objectID": "cv-en.html#presentations-and-workshops",
    "href": "cv-en.html#presentations-and-workshops",
    "title": "Resume",
    "section": "PRESENTATIONS AND WORKSHOPS",
    "text": "PRESENTATIONS AND WORKSHOPS\n\nTalk: Misadventures with Reproducibility in R (30 Nov 2022, R Ladies Melbourne Meetup)\nTalk: Designing R Packages (4 Oct 2022, Monash EBS Data Science Research Software Study Group)\nTalk: Quarto Websites as Research Compendiums (16 Aug 2022, Monash EBS Data Science Research Software Study Group)\nWorkshop: Writing academic papers with Rmarkdown and friends (9 Aug 2022, Monash University)"
  },
  {
    "objectID": "cv-en.html#references",
    "href": "cv-en.html#references",
    "title": "Resume",
    "section": "References",
    "text": "References\nAvailable upon request."
  },
  {
    "objectID": "cv.html#educao",
    "href": "cv.html#educao",
    "title": "Curriculum vitae (pt-br)",
    "section": "Educa��o",
    "text": "Educa��o\nDoctor of Philosophy – Econometrics and Business Statistics  Monash University Aug 2021 — Present\n\nNew principles and methods for complex data preparation and integration, with applications to official statistics, web-scrapped data and satellite raster images\nMonash Data Futures Institute PhD Top-Up Scholarship (2021-2024)\nExpected submission date: Nov 2024"
  },
  {
    "objectID": "cv.html#experincia",
    "href": "cv.html#experincia",
    "title": "Curriculum vitae (pt-br)",
    "section": "Experi�ncia",
    "text": "Experi�ncia\n\nData Scientist\nFreelance, Multiple Clients Jan 2020 – June 2021\nProviding data collection, pre-processing, exploratory analysis and modelling services to clients in the early R&D stages of developing data driven products. Projects include:\n\nData description and preliminary product feasibility insights for a start-up real estate bond platform; including assessing suitability of various property transaction databases for use in initial product prototype.\nDevelopment of statistical anomaly detection regimes and key historical insights from internet quality time-series data for use in parametric insurance products, including documenting analysis tools in an R package."
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Currilum vitae",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\n\n\n\n\n\n\n\nHARD SKILLS\n      \n    \n\n\nAnálise Exploratória\n\n\n\nEstatística Frequentista\n\n\n\nTratamento de Dados\n\n\n\nMachine Learning (Não DL)\n\n\n\nVisualização de Dados\n\n\n\nAzure\n\n\n\nControle de Versão\n\n\n\nPLN\n\n\n\nWeb Scraping\n\n\n\nEstatística Bayesiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n    \n\n\nDesenho Experimental\n\n\n\nMeta-análise\n\n\n\nDeep Learning\n\n\n\nProgramação\n\n\n\nAnálise Geoespacial\n\n\n\nBancos de Dados Relacionais\n\n\n\nMLOps\n\n\n\nSéries Temporais\n\n\n\nAWS\n\n\n\nGenAI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOFT SKILLS\n      \n    \n\n\nInglês\n\n\n\nComunicação Escrita\n\n\n\nCriatividade\n\n\n\nOrganização\n\n\n\nProatividade\n\n\n\nProblem Solving\n\n\n\nStorytelling\n\n\n\nEspanhol\n\n\n\nGestão de Pessoas\n\n\n\nFlexibilidade"
  },
  {
    "objectID": "cv.html#tecnologias",
    "href": "cv.html#tecnologias",
    "title": "Currilum vitae",
    "section": "Tecnologias",
    "text": "Tecnologias\n\n\n# A tibble: 10 × 4\n   skill  range avaliacao preenchimento\n   &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt;        \n 1 Python     1         4 TRUE         \n 2 Python     2         4 TRUE         \n 3 Python     3         4 TRUE         \n 4 Python     4         4 TRUE         \n 5 Python     5         4 FALSE        \n 6 R          1         5 TRUE         \n 7 R          2         5 TRUE         \n 8 R          3         5 TRUE         \n 9 R          4         5 TRUE         \n10 R          5         5 TRUE"
  },
  {
    "objectID": "cv.html#licenas-e-certificaes",
    "href": "cv.html#licenas-e-certificaes",
    "title": "Curriculum vitae (pt-br)",
    "section": "Licen�as e Certifica��es",
    "text": "Licen�as e Certifica��es"
  },
  {
    "objectID": "cv.html#apresentaes",
    "href": "cv.html#apresentaes",
    "title": "Curriculum vitae (pt-br)",
    "section": "Apresenta��es",
    "text": "Apresenta��es\n\nTalk: Misadventures with Reproducibility in R (30 Nov 2022, R Ladies Melbourne Meetup)\nTalk: Designing R Packages (4 Oct 2022, Monash EBS Data Science Research Software Study Group)\nTalk: Quarto Websites as Research Compendiums (16 Aug 2022, Monash EBS Data Science Research Software Study Group)\nWorkshop: Writing academic papers with Rmarkdown and friends (9 Aug 2022, Monash University)"
  },
  {
    "objectID": "cv.html#publicaes",
    "href": "cv.html#publicaes",
    "title": "Curriculum vitae (pt-br)",
    "section": "Publica��es",
    "text": "Publica��es\n\nTalk: Misadventures with Reproducibility in R (30 Nov 2022, R Ladies Melbourne Meetup)\nTalk: Designing R Packages (4 Oct 2022, Monash EBS Data Science Research Software Study Group)\nTalk: Quarto Websites as Research Compendiums (16 Aug 2022, Monash EBS Data Science Research Software Study Group)\nWorkshop: Writing academic papers with Rmarkdown and friends (9 Aug 2022, Monash University)"
  },
  {
    "objectID": "cv.html#educação",
    "href": "cv.html#educação",
    "title": "Currilum vitae",
    "section": "Educação",
    "text": "Educação\nPós-doutorado em Ecologia – Mudanças Climáticas e Interações Tróficas Universidade Federal do Rio de Janeiro Mar 2016 — Jun 2019\n\nEfeito das mudanças climáticas sobre as interações ecológicas\n\nBolsista do Programa Nacional de Pós-Doutorado (CAPES)\n\nDoutor em Ecologia – Mudanças Climáticas, Ecologia de Comunidades e Ecossistemas Universidade Federal do Rio de Janeiro Mar 2012 — Fev 2016\n\nO efeito de predadores na estrutura e funcionamento de cadeias traficas e sua interação com as mudanças climáticas\nBolsista CAPES\nMelhor tese de Doutorado defendida no Programa de Pós-Graduação em Ecologia (UFRJ) em 2016\n\nPeríodo de doutorado-sanduíche na Univerdade da Columbia Britânica (Out 2014 - Mai 2015)\n\nMestre em Ecologia – Ecologia de Comunidades e Ecossistemas Universidade Federal do Rio de Janeiro Ago 2009 — Jul 2011\n\nEstruturação da comunidade de macroinvertebrados aquáticos em bromélias-tanque\nBolsista CAPES\n\nBacharel em Ciências Biológicas – Ecologia Universidade Federal do Rio de Janeiro Ago 2005 — Jul 2009\n\nBromélias-tanque como ilhas: a influência das características do habitat sobre a riqueza de macroinvertebrados aquáticos\nIniciação Científica - Bolsista FAPERJ"
  },
  {
    "objectID": "cv.html#experiência",
    "href": "cv.html#experiência",
    "title": "Currilum vitae",
    "section": "Experiência",
    "text": "Experiência\nAccenture\nData Science Principal Jan 2021 – Atual\nTech lead em times e projetos relacionados à ciência de dados, atuando em todo o fluxo da solução analítica, especialmente na análise, desenho técnico e proposta de solução para o problema de dados, extração de insights dos dados e comunicação com stakeholders e clientes. Também ajudo com o gerenciamento dos times de dados, especialmente na mentoria dos profissionais mais jovens. Minhas principais contribuições têm sido:\n\nAtuação em projetos de Geotecnia, focando no desenvolvimento de produtos de dados voltados ao monitoramento de deformações do terreno em estruturas mineiras, baseado na análise multi-temporal de imagens interferométricas capturadas por radares de abertura sintética embarcados em satélite (InSAR). Os produtos desenvolvidos aqui têm mais de 30 exemplos de casos de uso, contribuindo para a gestão e redução do risco para o negócio;\n\nDesenvolvimento de soluções analíticas para a geofísica, onde buscamos facilitar a análise de dados do monitoramento elétrico do solo (eletrorresistividade) para a identificação de áreas com acúmulo de água dentro de barragens;\n\nDesenvolvimento de soluções analíticas para melhoria do planejamento tático e operacional do time de Segurança Empresarial atuando em um grande porto no Brasil;\n\nGestão de um projeto de previsão de demanda (i.e., previsão do volume esperado de vendas) de produtos alimentícios para uma grande empresa brasileira;\n\nMentoria de profissionais de dados em todos os níveis de senioridade;\n\nElaboração do desenho técnico das soluções para os problemas de dados em propostas;\n\nColaboração com os gestores de projetos, times e outros pares para o desenvolvimento de um melhor ambiente de trabalho.\n\nData Science Consultant Jul 2019 – Dec 2020\nAtuação em projetos relacionados à ciência de dados, com foco especial em análises preditivas. Conduzi e colaborei com o desenho das soluções para os problemas de dados, as análises exploratórias, descritivas e preditivas, mensuração dos benefícios dos entregáveis, além de ajudar a liderança à orientar os membros mais jovens dos projetos. Minhas principais contribuições foram:\n\nDesenvolvimento de soluções analíticas para melhoria do planejamento tático e operacional do time de Segurança Empresarial a fim de reduzir o impacto das paradas forçadas dos trens sobre o escoamento da produção em uma ferrovia. A área de negócio atribuiu uma redução de 30% no volume de paradas dos trens à solução analítica nos primeiros meses de operação da solução;\n\nDesenvolvimento de soluções analíticas para melhoria do planejamento tático e operacional do time de Segurança Empresarial a fim de reduzir o impacto de furtos nos ativos de sinalização em uma ferrovia. A área de negócio reportou uma queda de cerca de 50% na taxa de furtos dos ativos nos primeiros meses de operação da solução, enquanto o monitoramento dos dados da mesma mostrou uma queda de até 70% no atendimento de alarmes falsos no período da noite;\n\nDesenvolvi uma análise da causa raiz para entender os resultados de um experimento do impacto da alimentação de briquetes em um forno calcinador, que trouxe insights importantes sobre o estado dos briquetes e da operação do forno calcinador que vieram a esclarecer os resultados e orientar um novo experimento;\n\nColaborei com a entrega de previsões do volume esperado de vendas de produtos alimentícios para uma grande empresa que atua no Brasil.\nPrograma de Pós-Graduação em Ecologia (UFRJ)\nProfessor Visitante Mar 2016 – Jun 2019\nImplementei diversas disciplinas relacionadas à programação e análise de dados no programa de pós-graduação, tais como: (1) Introdução à Linguagem R, (2) Manejo, Visualização e Compartilhamento de Dados e (3) Revisão Sistemática e Meta-análise. Além disso, contribui como um dos professores do curso de Delineamento Experimental e Estatística, além de co-orientar alunos de mestrado e doutorado do programa.\nLaboratório de Limnologia (UFRJ)\nPesquisador Associado Mai 2006 – Jun 2019\nEstudei e pesquisei temas relacionados ao funcionamento e à dinâmica de ecossistemas aquáticos como rios, riachos e lagoas, além de ecossistemas aquáticos modelo como as bromélias-tanque. Os principais destaques aqui foram:\n\nPlanejamento, desenho e condução de experimentos e análise estatística dos dados;\n\nRedação e publicação de mais de 20 trabalhos em revistas científicas internacionais de grande impacto;\n\nParticipação em grupos de pesquisa internacionais;\n\nAprendi e realizei diversas análises físico-químicas da qualidade da água, tanto em laboratório quando no campo;\n\nIdentificação de insetos e outros organismos aquáticos;\n\nOrientação de alunos de iniciação científica, mestrado e doutorado;\n\nApresentação de trabalhos em congressos e aulas para alunos de iniciação científica, mestrado e doutorado.\n\nProjeto de Monitoramento dos Igarapés da FLONA Sacará-Taquera Set 2009 – Dec 2019\nFui o líder técnico e responsável pelas equipes de campo deste projeto, que tinha como objetivo desenvolver um índice de integridade ambiental dos igarapés da FLONA baseado na composição e abundância de espécies de invertebrados aquáticos e peixes. A ideia era utilizar esta última informação para determinar o grau de impacto de uma mineradora sobre cada um dos igarapés nas proximidades da operação. Conseguimos atingir este objetivo e apresentamos a metodologia utilizada ao órgão fiscalizador responsável, que a avaliou como inédita no páis. Os principais destaques aqui foram:\n\nPlanejamento dos trabalhos de campo e coordenação do time de coleta;\n\nCuradoria e organização dos dados do projeto em torno de um banco de dados relacional;\n\nDesenho do arcabouço analítico do projeto;\n\nDesenvolvimento de um algoritmo de discriminação de Igarapés impactados e não-impactados;\n\nRedação de relatórios técnicos para submissão ao órgão fiscalizador responsável (IBAMA).\n\nProjeto de Monitoramento das Lagoas Costeiras do Norte Fluminense Jan 2007 – Jun2010\nColaborei com a condução das coletas de campo, análises químicas e a interpretação dos dados sobre as características da água e da fauna planctônica das lagoas estudadados.\nPrograma de Pós-Graduação em Ecologia (UNICAMP)\nProfessor Visitante Jun 2019\nMinistrei um curso de Introdução ao Manejo e Visualização de Dados para os alunos de mestrado e doutorado do programa de pós-graduação."
  },
  {
    "objectID": "cv.html#licenças-e-certificações",
    "href": "cv.html#licenças-e-certificações",
    "title": "Currilum vitae",
    "section": "Licenças e Certificações",
    "text": "Licenças e Certificações\n\n\n\n\n\n\n\n\n\n\n\n\n2019\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\n2020\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2022\nAWS\nAWS Certified Cloud Practitioner\n\n\nAWS\nAWS Certified Machine Learning Specialty\n\n\nAWS\nAWS Certified Solutions Architect Associate\n\n\nCoursera\nAI for Medicine\n\n\nCoursera\nData Science: Foundations using R Specialization\n\n\nCoursera\nDeep Learning Specialization\n\n\nCoursera\nDeepLearning.AI TensorFlow Developer Specialization\n\n\nCoursera\nGenerative Adversarial Networks (GANs)\n\n\nCoursera\nMathematics for Machine Learning\n\n\nCoursera\nNatural Language Processing Specialization\n\n\n2023\nCoursera\nMachine Learning Engineering for Production (MLOps)\n\n\nCoursera\nMachine Learning Specialization"
  },
  {
    "objectID": "cv.html#apresentações",
    "href": "cv.html#apresentações",
    "title": "Currilum vitae",
    "section": "Apresentações",
    "text": "Apresentações\n\nInúmeras apresentações realizadas em congressos e outros eventos acadêmicos nacionais e internacionais da área da Ecologia e da Limnologia;\n\nAnálise multivariada dos dados do monitoramento geotécnico aplicada à detecção de anomalias (19 Out 2023, GTGH - Grupo Temático de Geotecnia e Hidrogeologia - Belo Horizonte, Brasil);\n\nMonitoramento geotécnico orientado à dados: desafios, a jornada e lições aprendidas (27 Set 2023, I Encontro Nacional de Centros de Monitoramento Geotécnico - Belo Horizonte, Brasil);\n\nAdvanced analytics applied to InSAR monitoring program (02 Dez 2023, LOP - Large Open Pit Project - Vancouver, Canadá)."
  },
  {
    "objectID": "cv.html#publicações",
    "href": "cv.html#publicações",
    "title": "Currilum vitae",
    "section": "Publicações",
    "text": "Publicações\n\n25+ publicações em revistas acadêmicas internacionais de alto impacto, focando na pesquisa que desenvolvi dentro da Ecologia (listadas no meu Lattes);\n\n\nBlog e portfolio pessoal: https://nacmarino.netlify.app/;\n\n\n8 Lições na construção do monitoramento geotécnico orientado à dados;\n\nMachine learning applied to control levels and anomaly detection in a tailing dam monitoringd data (9th International Conference on Tailings Management 2023, Chile);\n\nAdvanced analytics applied to InSAR Dam Monitoring Program (Tailings and Mine Waste 2023, Vancouver, Canadá)."
  },
  {
    "objectID": "cv.html#cursos",
    "href": "cv.html#cursos",
    "title": "Currilum vitae",
    "section": "Cursos",
    "text": "Cursos\n\n\n\n\n\n\n\n\n\n\n\n\n2019\n    \n\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\nDatacamp\nShiny Fundamentals with R\n\n\n2020\n    \n\nCurso-R\nDeep Learning\n\n\nData Science Academy\nBig Data Fundamentos\n\n\nData Science Academy\nDeep Learning Frameworks\n\n\nData Science Academy\nIntrodução à Inteligência Artificial\n\n\nData Science Academy\nMicrosoft Power BI para Data Science\n\n\nData Science Academy\nPython Fundamentos para a Análise de Dados\n\n\nData Science Academy\nSoft Skills: Desenvolvendo Suas Habilidades Comportamentais\n\n\nData Science Academy\nWeb Scrapping e Análise de Dados\n\n\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Fundamentals with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2021\n    \n\nCoursera\nExploratory Data Analysis\n\n\nCoursera\nGetting and Cleaning Data\n\n\nCoursera\nIntroduction to Mathematical Thinking\n\n\nCoursera\nLearn to Program: Crafting Quality Code\n\n\nCoursera\nLinear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMultivariate Calculus\n\n\nCoursera\nR Programming\n\n\nCoursera\nReproducible Research\n\n\nCoursera\nThe Data Scientist's Toolbox\n\n\nCurso-R\nDashboards\n\n\nCurso-R\nDeploy\n\n\nCurso-R\nRelatórios e Visualizaçãoo de Dados\n\n\nCurso-R\nWeb Scrapping\n\n\nData Science Academy\nDeep Learning I\n\n\nData Science Academy\nDeep Learning II\n\n\nData Science Academy\nProgramação Paralela em GPU\n\n\nDatacamp\nData Scientist: Customer Channel/Marketing (Basic, Intermediate & Advanced)\n\n\nDatacamp\nData Scientist: Risk (Basic)\n\n\n2022\n    \n\nCoursera\nAI For Medical Treatment\n\n\nCoursera\nAI for Medical Diagnosis\n\n\nCoursera\nAI for Medical Prognosis\n\n\nCoursera\nBuild Basic Generative Adversarial Networks (GANs)\n\n\nCoursera\nConvolutional Neural Networks\n\n\nCoursera\nConvolutional Neural Networks in TensorFlow\n\n\nCoursera\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\n\n\nCoursera\nImproving Deep Neural Networks: HyperparameterTuning, Regularization and Optimization\n\n\nCoursera\nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Multivariate Calculus\n\n\nCoursera\nMathematics for Machine Learning: PCA\n\n\nCoursera\nNatural Language Processing in TensorFlow\n\n\nCoursera\nNatural Language Processing with Attention Models\n\n\nCoursera\nNatural Language Processing with Classification and Vector Spaces\n\n\nCoursera\nNatural Language Processing with Probabilistic Models\n\n\nCoursera\nNatural Language Processing with Sequence Models\n\n\nCoursera\nNeural Networks and Deep Learning\n\n\nCoursera\nPrincipal Component Analysis\n\n\nCoursera\nSequence Models\n\n\nCoursera\nSequences, Time Series and Prediction\n\n\nCoursera\nStructuring Machine Learning Projects\n\n\n2023\n    \n\nCoursera\nAdvanced Learning Algorithms\n\n\nCoursera\nApply Generative Adversarial Networks (GANs)\n\n\nCoursera\nBuild Better Generative Adversarial Networks (GANs)\n\n\nCoursera\nDeploying Machine Learning Models in Production\n\n\nCoursera\nGenerative AI for Everyone\n\n\nCoursera\nGenerative AI with Large Language Models\n\n\nCoursera\nIntroduction to Machine Learning in Production\n\n\nCoursera\nMachine Learning Data Lifecycle in Production\n\n\nCoursera\nMachine Learning Modeling Pipelines in Production\n\n\nCoursera\nSupervised Machine Learning: Regression and Classification\n\n\nCoursera\nUnsupervised Learning, Recommenders, Reinforcement Learning\n\n\nDeepLearning.AI\nBuilding Generative AI Applications with Gradio\n\n\nDeepLearning.AI\nBuilding Systems with the ChatGPT API\n\n\nDeepLearning.AI\nChatGPT Prompt Engineering for Developers\n\n\nDeepLearning.AI\nEvaluating and Debugging Generative AI Models Using Weights and Biases\n\n\nDeepLearning.AI\nFinetuning Large Language Models\n\n\nDeepLearning.AI\nHow Business Thinkers Can Start Building AI Plugins With Semantic Kernel\n\n\nDeepLearning.AI\nHow Diffusion Models Work\n\n\nDeepLearning.AI\nLangChain for LLM Application Development\n\n\nDeepLearning.AI\nLangChain: Chat with Your Data\n\n\nDeepLearning.AI\nLarge Language Models with Semantic Search\n\n\nDeepLearning.AI\nPair Programming with a Large Language Model\n\n\nDeepLearning.AI\nUnderstanding and Applying Text Embeddings"
  },
  {
    "objectID": "cv.html#ferramentas",
    "href": "cv.html#ferramentas",
    "title": "Currilum vitae",
    "section": "Ferramentas",
    "text": "Ferramentas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nPython\n\n\n\npandas\n\n\n\ntidyverse\n\n\n\nShiny\n\n\n\nKeras\n\n\n\nPyTorch\n\n\n\nTensorflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Point\n\n\n\nggplot2\n\n\n\nscikit-learn\n\n\n\nAzure Machine Learning\n\n\n\ntidymodels\n\n\n\nPower BI\n\n\n\nSQL\n\n\n\nTorch for R"
  },
  {
    "objectID": "cv.html#referências",
    "href": "cv.html#referências",
    "title": "Currilum vitae",
    "section": "Referências",
    "text": "Referências\nDisponível sob demanda."
  },
  {
    "objectID": "cv-en.html#experience",
    "href": "cv-en.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\nAccenture\nData Science Principal Jan 2021 – Current\nTech Lead in teams and projects related to data science, working across the entire analytical solution flow, particularly in analysis, technical design, and proposing solutions to data problems, extracting insights from data, and communicating with stakeholders and clients. I also assist with the management of data teams, especially in mentoring younger professionals. My main contributions have been:\n\nInvolvement in Geotechnical projects, focusing on the development of data products for monitoring terrain deformations in mining structures, based on the multi-temporal analysis of interferometric images captured by satellite-borne synthetic aperture radars (InSAR). The products developed here have over 30 examples of use cases, contributing to business risk management and reduction;\nDevelopment of analytical solutions for geophysics, where we seek to facilitate the analysis of data from soil electrical monitoring (electrical resistivity) for the identification of areas with water accumulation within dams;\nDevelopment of analytical solutions to improve the tactical and operational planning of the Corporate Security team operating in a large port in Brazil;\nManagement of a demand forecasting project (i.e., forecasting the expected volume of sales) for a large Brazilian food company;\nMentoring data professionals at all levels of seniority;\nPreparation of technical design for data problems in proposals;\nCollaboration with project managers, teams, and other peers for the development of a better working environment.\n\nData Science Consultant Jul 2019 – Dec 2020\nI worked with projects related to data science, with a special focus on predictive analyses. I led and collaborated on the design of solutions for data problems, exploratory, descriptive, and predictive analyses, measurement of deliverable benefits, as well as assisting leadership in guiding younger project members. My main contributions were:\n\nDevelopment of analytical solutions to improve the tactical and operational planning of the Corporate Security team to reduce the impact of forced train stoppages on - production flow in a railway. The business area attributed a 30% reduction in train stoppage volume to the analytical solution in the first months of solution operation;\nDevelopment of analytical solutions to improve the tactical and operational planning of the Corporate Security team to reduce the impact of theft on signaling assets in a railway. The business area reported a roughly 50% drop in the rate of asset thefts in the first months of solution operation, while data monitoring showed a decrease of up to 70% in false alarm response during the night period;\nConducted a root cause analysis to understand the results of an experiment on the impact of feeding briquettes into a calcining furnace, providing important insights into the state of the briquettes and the operation of the calcining furnace, which clarified the results and guided a new experiment;\nCollaborated on delivering forecasts of expected sales volumes for food products for a large company operating in Brazil.\nPrograma de Pós-Graduação em Ecologia (UFRJ)\nVisiting Scholar Mar 2016 – Jun 2019\nI implemented several disciplines related to programming and data analysis in the postgraduate program, such as: (1) Introduction to the R Language, (2) Data Handling, Visualization, and Sharing, and (3) Systematic Review and Meta-analysis. Additionally, I contributed as one of the instructors for the course on Experimental Design and Statistics, as well as co-supervised master’s and doctoral students in the program.\nLaboratório de Limnologia (UFRJ)\nResearcher May 2006 – Jun 2019\nI studied and researched topics related to the functioning and dynamics of aquatic ecosystems such as rivers, streams, and ponds, as well as model aquatic ecosystems like tank bromeliads. The main highlights here were:\n\nPlanning, designing, and conducting experiments, and statistical analysis of data;\nWriting and publishing over 20 papers in high-impact international scientific journals;\nParticipation in international research groups;\nLearning and performing various physicochemical analyses of water quality, both in the laboratory and in the field;\nIdentification of insects and other aquatic organisms;\nSupervision of undergraduate, master’s, and doctoral students;\nPresentation of papers at conferences and lectures for undergraduate, master’s, and doctoral students.\n\nProjeto de Monitoramento dos Igarapés da FLONA Sacará-Taquera Sep 2009 – Dec 2019\nI was the technical leader and responsible for the field teams in this project, which aimed to develop an environmental integrity index for the FLONA streams based on the composition and abundance of aquatic invertebrate and fish species. The idea was to use this information to determine the impact of a mining operation on each of the streams near the operation. We successfully achieved this objective and presented the methodology used to the relevant regulatory authority, which evaluated it as unprecedented in the country. The main highlights here were:\n\nPlanning fieldwork and coordinating the collection team;\nCuration and organization of project data around a relational database;\nDesign of the project’s analytical framework;\nDevelopment of an algorithm for discriminating impacted and non-impacted streams;\nWriting technical reports for submission to the relevant regulatory authority (IBAMA).\n\nProjeto de Monitoramento das Lagoas Costeiras do Norte Fluminense Jan 2007 – Jun2010\nI collaborated in conducting field surveys, chemical analyses, and data interpretation regarding the water characteristics and planktonic fauna of the studied lagoons.\nPrograma de Pós-Graduação em Ecologia (UNICAMP)\nVisiting Scholar Jun 2019\nI taught a course on Introduction to Data Handling and Visualization for the master’s and doctoral students in the postgraduate program."
  },
  {
    "objectID": "cv-en.html#skills",
    "href": "cv-en.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\n\n\n\n\n\n\n\nHARD SKILLS\n      \n    \n\n\nData Wrangling\n\n\n\nExploratory Analysis\n\n\n\nMeta-analysis\n\n\n\nDeep Learning\n\n\n\nProgramming\n\n\n\nGeospatial Analysis\n\n\n\nPLN\n\n\n\nTime Series\n\n\n\nWeb Scraping\n\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n    \n\n\nExperimental Design\n\n\n\nFrequentist Statistics\n\n\n\nData Visualization\n\n\n\nMachine Learning (Non DL)\n\n\n\nAzure\n\n\n\nMLOps\n\n\n\nRelational Databases\n\n\n\nVersion Control\n\n\n\nAWS\n\n\n\nGenAI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOFT SKILLS\n      \n    \n\n\nEnglish\n\n\n\nCreativity\n\n\n\nProactive\n\n\n\nProblem Solving\n\n\n\nSelf-organization\n\n\n\nStorytelling\n\n\n\nWritten Communication\n\n\n\nPeople Management\n\n\n\nSpanish\n\n\n\nFlexibility"
  },
  {
    "objectID": "cv-en.html#tools",
    "href": "cv-en.html#tools",
    "title": "Resume",
    "section": "Tools",
    "text": "Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nPython\n\n\n\npandas\n\n\n\ntidyverse\n\n\n\nShiny\n\n\n\nKeras\n\n\n\nPyTorch\n\n\n\nTensorflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Point\n\n\n\nggplot2\n\n\n\nscikit-learn\n\n\n\nAzure Machine Learning\n\n\n\ntidymodels\n\n\n\nPower BI\n\n\n\nSQL\n\n\n\nTorch for R"
  },
  {
    "objectID": "cv-en.html#licenses-and-certifications",
    "href": "cv-en.html#licenses-and-certifications",
    "title": "Resume",
    "section": "Licenses and Certifications",
    "text": "Licenses and Certifications\n\n\n\n\n\n\n\n\n\n\n\n\n2019\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\n2020\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2022\nAWS\nAWS Certified Cloud Practitioner\n\n\nAWS\nAWS Certified Machine Learning Specialty\n\n\nAWS\nAWS Certified Solutions Architect Associate\n\n\nCoursera\nAI for Medicine\n\n\nCoursera\nData Science: Foundations using R Specialization\n\n\nCoursera\nDeep Learning Specialization\n\n\nCoursera\nDeepLearning.AI TensorFlow Developer Specialization\n\n\nCoursera\nGenerative Adversarial Networks (GANs)\n\n\nCoursera\nMathematics for Machine Learning\n\n\nCoursera\nNatural Language Processing Specialization\n\n\n2023\nCoursera\nMachine Learning Engineering for Production (MLOps)\n\n\nCoursera\nMachine Learning Specialization"
  },
  {
    "objectID": "cv-en.html#courses",
    "href": "cv-en.html#courses",
    "title": "Resume",
    "section": "Courses",
    "text": "Courses\n\n\n\n\n\n\n\n\n\n\n\n\n2019\n    \n\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\nDatacamp\nShiny Fundamentals with R\n\n\n2020\n    \n\nCurso-R\nDeep Learning\n\n\nData Science Academy\nBig Data Fundamentos\n\n\nData Science Academy\nDeep Learning Frameworks\n\n\nData Science Academy\nIntrodução à Inteligência Artificial\n\n\nData Science Academy\nMicrosoft Power BI para Data Science\n\n\nData Science Academy\nPython Fundamentos para a Análise de Dados\n\n\nData Science Academy\nSoft Skills: Desenvolvendo Suas Habilidades Comportamentais\n\n\nData Science Academy\nWeb Scrapping e Análise de Dados\n\n\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Fundamentals with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2021\n    \n\nCoursera\nExploratory Data Analysis\n\n\nCoursera\nGetting and Cleaning Data\n\n\nCoursera\nIntroduction to Mathematical Thinking\n\n\nCoursera\nLearn to Program: Crafting Quality Code\n\n\nCoursera\nLinear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMultivariate Calculus\n\n\nCoursera\nR Programming\n\n\nCoursera\nReproducible Research\n\n\nCoursera\nThe Data Scientist's Toolbox\n\n\nCurso-R\nDashboards\n\n\nCurso-R\nDeploy\n\n\nCurso-R\nRelatórios e Visualizaçãoo de Dados\n\n\nCurso-R\nWeb Scrapping\n\n\nData Science Academy\nDeep Learning I\n\n\nData Science Academy\nDeep Learning II\n\n\nData Science Academy\nProgramação Paralela em GPU\n\n\nDatacamp\nData Scientist: Customer Channel/Marketing (Basic, Intermediate & Advanced)\n\n\nDatacamp\nData Scientist: Risk (Basic)\n\n\n2022\n    \n\nCoursera\nAI For Medical Treatment\n\n\nCoursera\nAI for Medical Diagnosis\n\n\nCoursera\nAI for Medical Prognosis\n\n\nCoursera\nBuild Basic Generative Adversarial Networks (GANs)\n\n\nCoursera\nConvolutional Neural Networks\n\n\nCoursera\nConvolutional Neural Networks in TensorFlow\n\n\nCoursera\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\n\n\nCoursera\nImproving Deep Neural Networks: HyperparameterTuning, Regularization and Optimization\n\n\nCoursera\nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Multivariate Calculus\n\n\nCoursera\nMathematics for Machine Learning: PCA\n\n\nCoursera\nNatural Language Processing in TensorFlow\n\n\nCoursera\nNatural Language Processing with Attention Models\n\n\nCoursera\nNatural Language Processing with Classification and Vector Spaces\n\n\nCoursera\nNatural Language Processing with Probabilistic Models\n\n\nCoursera\nNatural Language Processing with Sequence Models\n\n\nCoursera\nNeural Networks and Deep Learning\n\n\nCoursera\nPrincipal Component Analysis\n\n\nCoursera\nSequence Models\n\n\nCoursera\nSequences, Time Series and Prediction\n\n\nCoursera\nStructuring Machine Learning Projects\n\n\n2023\n    \n\nCoursera\nAdvanced Learning Algorithms\n\n\nCoursera\nApply Generative Adversarial Networks (GANs)\n\n\nCoursera\nBuild Better Generative Adversarial Networks (GANs)\n\n\nCoursera\nDeploying Machine Learning Models in Production\n\n\nCoursera\nGenerative AI for Everyone\n\n\nCoursera\nGenerative AI with Large Language Models\n\n\nCoursera\nIntroduction to Machine Learning in Production\n\n\nCoursera\nMachine Learning Data Lifecycle in Production\n\n\nCoursera\nMachine Learning Modeling Pipelines in Production\n\n\nCoursera\nSupervised Machine Learning: Regression and Classification\n\n\nCoursera\nUnsupervised Learning, Recommenders, Reinforcement Learning\n\n\nDeepLearning.AI\nBuilding Generative AI Applications with Gradio\n\n\nDeepLearning.AI\nBuilding Systems with the ChatGPT API\n\n\nDeepLearning.AI\nChatGPT Prompt Engineering for Developers\n\n\nDeepLearning.AI\nEvaluating and Debugging Generative AI Models Using Weights and Biases\n\n\nDeepLearning.AI\nFinetuning Large Language Models\n\n\nDeepLearning.AI\nHow Business Thinkers Can Start Building AI Plugins With Semantic Kernel\n\n\nDeepLearning.AI\nHow Diffusion Models Work\n\n\nDeepLearning.AI\nLangChain for LLM Application Development\n\n\nDeepLearning.AI\nLangChain: Chat with Your Data\n\n\nDeepLearning.AI\nLarge Language Models with Semantic Search\n\n\nDeepLearning.AI\nPair Programming with a Large Language Model\n\n\nDeepLearning.AI\nUnderstanding and Applying Text Embeddings"
  },
  {
    "objectID": "cv-en.html#presentations",
    "href": "cv-en.html#presentations",
    "title": "Resume",
    "section": "Presentations",
    "text": "Presentations\n\nNumerous presentations given at national and international ecology and limnology conferences and other academic events;\n\nMultivariate analysis of geotechnical monitoring data applied to anomaly detection (19 Oct 2023, GTGH - Grupo Temático de Geotecnia e Hidrogeologia - Belo Horizonte, Brazil);\n\nMonitoramento geotécnico orientado à dados: desafios, a jornada e lições aprendidas (27 Sep 2023, I Encontro Nacional de Centros de Monitoramento Geotécnico - Belo Horizonte, Brazil);\n\nAdvanced analytics applied to InSAR monitoring program (02 Dez 2023, LOP - Large Open Pit Project - Vancouver, Canada)."
  },
  {
    "objectID": "cv-en.html#publications",
    "href": "cv-en.html#publications",
    "title": "Resume",
    "section": "Publications",
    "text": "Publications\n\n25+ publications in high-impact international academic journals, focusing on the research I conducted within Ecology (you can find all of them listed on my Lattes);\n\n\nBlog and personal portfolio: https://nacmarino.netlify.app/;\n\n\n8 Lições na construção do monitoramento geotécnico orientado à dados;\n\nMachine learning applied to control levels and anomaly detection in a tailing dam monitoringd data (9th International Conference on Tailings Management 2023, Chile);\n\nAdvanced analytics applied to InSAR Dam Monitoring Program (Tailings and Mine Waste 2023, Vancouver, Canadá)."
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Currilum vitae",
    "section": "",
    "text": "nac.marino@gmail.com"
  },
  {
    "objectID": "cv/cv.html#educação",
    "href": "cv/cv.html#educação",
    "title": "Currilum vitae",
    "section": "Educação",
    "text": "Educação\nPós-doutorado em Ecologia – Mudanças Climáticas e Interações Tróficas Universidade Federal do Rio de Janeiro Mar 2016 — Jun 2019\n\nEfeito das mudanças climáticas sobre as interações ecológicas\n\nBolsista do Programa Nacional de Pós-Doutorado (CAPES)\n\nDoutor em Ecologia – Mudanças Climáticas, Ecologia de Comunidades e Ecossistemas Universidade Federal do Rio de Janeiro Mar 2012 — Fev 2016\n\nO efeito de predadores na estrutura e funcionamento de cadeias traficas e sua interação com as mudanças climáticas\nBolsista CAPES\nMelhor tese de Doutorado defendida no Programa de Pós-Graduação em Ecologia (UFRJ) em 2016\n\nPeríodo de doutorado-sanduíche na Univerdade da Columbia Britânica (Out 2014 - Mai 2015)\n\nMestre em Ecologia – Ecologia de Comunidades e Ecossistemas Universidade Federal do Rio de Janeiro Ago 2009 — Jul 2011\n\nEstruturação da comunidade de macroinvertebrados aquáticos em bromélias-tanque\nBolsista CAPES\n\nBacharel em Ciências Biológicas – Ecologia Universidade Federal do Rio de Janeiro Ago 2005 — Jul 2009\n\nBromélias-tanque como ilhas: a influência das características do habitat sobre a riqueza de macroinvertebrados aquáticos\nIniciação Científica - Bolsista FAPERJ"
  },
  {
    "objectID": "cv/cv.html#experiência",
    "href": "cv/cv.html#experiência",
    "title": "Currilum vitae",
    "section": "Experiência",
    "text": "Experiência\nAccenture\nData Science Principal Jan 2021 – Atual\nTech lead em times e projetos relacionados à ciência de dados, atuando em todo o fluxo da solução analítica, especialmente na análise, desenho técnico e proposta de solução para o problema de dados, extração de insights dos dados e comunicação com stakeholders e clientes. Também ajudo com o gerenciamento dos times de dados, especialmente na mentoria dos profissionais mais jovens. Minhas principais contribuições têm sido:\n\nAtuação em projetos de Geotecnia, focando no desenvolvimento de produtos de dados voltados ao monitoramento de deformações do terreno em estruturas mineiras, baseado na análise multi-temporal de imagens interferométricas capturadas por radares de abertura sintética embarcados em satélite (InSAR). Os produtos desenvolvidos aqui têm mais de 30 exemplos de casos de uso, contribuindo para a gestão e redução do risco para o negócio;\n\nDesenvolvimento de soluções analíticas para a geofísica, onde buscamos facilitar a análise de dados do monitoramento elétrico do solo (eletrorresistividade) para a identificação de áreas com acúmulo de água dentro de barragens;\n\nDesenvolvimento de soluções analíticas para melhoria do planejamento tático e operacional do time de Segurança Empresarial atuando em um grande porto no Brasil;\n\nGestão de um projeto de previsão de demanda (i.e., previsão do volume esperado de vendas) de produtos alimentícios para uma grande empresa brasileira;\n\nMentoria de profissionais de dados em todos os níveis de senioridade;\n\nElaboração do desenho técnico das soluções para os problemas de dados em propostas;\n\nColaboração com os gestores de projetos, times e outros pares para o desenvolvimento de um melhor ambiente de trabalho.\n\nData Science Consultant Jul 2019 – Dec 2020\nAtuação em projetos relacionados à ciência de dados, com foco especial em análises preditivas. Conduzi e colaborei com o desenho das soluções para os problemas de dados, as análises exploratórias, descritivas e preditivas, mensuração dos benefícios dos entregáveis, além de ajudar a liderança à orientar os membros mais jovens dos projetos. Minhas principais contribuições foram:\n\nDesenvolvimento de soluções analíticas para melhoria do planejamento tático e operacional do time de Segurança Empresarial a fim de reduzir o impacto das paradas forçadas dos trens sobre o escoamento da produção em uma ferrovia. A área de negócio atribuiu uma redução de 30% no volume de paradas dos trens à solução analítica nos primeiros meses de operação da solução;\n\nDesenvolvimento de soluções analíticas para melhoria do planejamento tático e operacional do time de Segurança Empresarial a fim de reduzir o impacto de furtos nos ativos de sinalização em uma ferrovia. A área de negócio reportou uma queda de cerca de 50% na taxa de furtos dos ativos nos primeiros meses de operação da solução, enquanto o monitoramento dos dados da mesma mostrou uma queda de até 70% no atendimento de alarmes falsos no período da noite;\n\nDesenvolvi uma análise da causa raiz para entender os resultados de um experimento do impacto da alimentação de briquetes em um forno calcinador, que trouxe insights importantes sobre o estado dos briquetes e da operação do forno calcinador que vieram a esclarecer os resultados e orientar um novo experimento;\n\nColaborei com a entrega de previsões do volume esperado de vendas de produtos alimentícios para uma grande empresa que atua no Brasil.\nPrograma de Pós-Graduação em Ecologia (UFRJ)\nProfessor Visitante Mar 2016 – Jun 2019\nImplementei diversas disciplinas relacionadas à programação e análise de dados no programa de pós-graduação, tais como: (1) Introdução à Linguagem R, (2) Manejo, Visualização e Compartilhamento de Dados e (3) Revisão Sistemática e Meta-análise. Além disso, contribui como um dos professores do curso de Delineamento Experimental e Estatística, além de co-orientar alunos de mestrado e doutorado do programa.\nLaboratório de Limnologia (UFRJ)\nPesquisador Associado Mai 2006 – Jun 2019\nEstudei e pesquisei temas relacionados ao funcionamento e à dinâmica de ecossistemas aquáticos como rios, riachos e lagoas, além de ecossistemas aquáticos modelo como as bromélias-tanque. Os principais destaques aqui foram:\n\nPlanejamento, desenho e condução de experimentos e análise estatística dos dados;\n\nRedação e publicação de mais de 20 trabalhos em revistas científicas internacionais de grande impacto;\n\nParticipação em grupos de pesquisa internacionais;\n\nAprendi e realizei diversas análises físico-químicas da qualidade da água, tanto em laboratório quando no campo;\n\nIdentificação de insetos e outros organismos aquáticos;\n\nOrientação de alunos de iniciação científica, mestrado e doutorado;\n\nApresentação de trabalhos em congressos e aulas para alunos de iniciação científica, mestrado e doutorado.\n\nProjeto de Monitoramento dos Igarapés da FLONA Sacará-Taquera Set 2009 – Dec 2019\nFui o líder técnico e responsável pelas equipes de campo deste projeto, que tinha como objetivo desenvolver um índice de integridade ambiental dos igarapés da FLONA baseado na composição e abundância de espécies de invertebrados aquáticos e peixes. A ideia era utilizar esta última informação para determinar o grau de impacto de uma mineradora sobre cada um dos igarapés nas proximidades da operação. Conseguimos atingir este objetivo e apresentamos a metodologia utilizada ao órgão fiscalizador responsável, que a avaliou como inédita no páis. Os principais destaques aqui foram:\n\nPlanejamento dos trabalhos de campo e coordenação do time de coleta;\n\nCuradoria e organização dos dados do projeto em torno de um banco de dados relacional;\n\nDesenho do arcabouço analítico do projeto;\n\nDesenvolvimento de um algoritmo de discriminação de Igarapés impactados e não-impactados;\n\nRedação de relatórios técnicos para submissão ao órgão fiscalizador responsável (IBAMA).\n\nProjeto de Monitoramento das Lagoas Costeiras do Norte Fluminense Jan 2007 – Jun2010\nColaborei com a condução das coletas de campo, análises químicas e a interpretação dos dados sobre as características da água e da fauna planctônica das lagoas estudadados.\nPrograma de Pós-Graduação em Ecologia (UNICAMP)\nProfessor Visitante Jun 2019\nMinistrei um curso de Introdução ao Manejo e Visualização de Dados para os alunos de mestrado e doutorado do programa de pós-graduação."
  },
  {
    "objectID": "cv/cv.html#skills",
    "href": "cv/cv.html#skills",
    "title": "Currilum vitae",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\n\n\n\n\n\n\n\nHARD SKILLS\n      \n    \n\n\nAnálise Exploratória\n\n\n\nEstatística Frequentista\n\n\n\nTratamento de Dados\n\n\n\nMachine Learning (Não DL)\n\n\n\nVisualização de Dados\n\n\n\nAzure\n\n\n\nControle de Versão\n\n\n\nPLN\n\n\n\nWeb Scraping\n\n\n\nEstatística Bayesiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n    \n\n\nDesenho Experimental\n\n\n\nMeta-análise\n\n\n\nDeep Learning\n\n\n\nProgramação\n\n\n\nAnálise Geoespacial\n\n\n\nBancos de Dados Relacionais\n\n\n\nMLOps\n\n\n\nSéries Temporais\n\n\n\nAWS\n\n\n\nGenAI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOFT SKILLS\n      \n    \n\n\nInglês\n\n\n\nComunicação Escrita\n\n\n\nCriatividade\n\n\n\nOrganização\n\n\n\nProatividade\n\n\n\nProblem Solving\n\n\n\nStorytelling\n\n\n\nEspanhol\n\n\n\nGestão de Pessoas\n\n\n\nFlexibilidade"
  },
  {
    "objectID": "cv/cv.html#ferramentas",
    "href": "cv/cv.html#ferramentas",
    "title": "Currilum vitae",
    "section": "Ferramentas",
    "text": "Ferramentas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nPython\n\n\n\npandas\n\n\n\ntidyverse\n\n\n\nShiny\n\n\n\nKeras\n\n\n\nPyTorch\n\n\n\nTensorflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Point\n\n\n\nggplot2\n\n\n\nscikit-learn\n\n\n\nAzure Machine Learning\n\n\n\ntidymodels\n\n\n\nPower BI\n\n\n\nSQL\n\n\n\nTorch for R"
  },
  {
    "objectID": "cv/cv.html#licenças-e-certificações",
    "href": "cv/cv.html#licenças-e-certificações",
    "title": "Currilum vitae",
    "section": "Licenças e Certificações",
    "text": "Licenças e Certificações\n\n\n\n\n\n\n\n\n\n\n\n\n2019\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\n2020\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2022\nAWS\nAWS Certified Cloud Practitioner\n\n\nAWS\nAWS Certified Machine Learning Specialty\n\n\nAWS\nAWS Certified Solutions Architect Associate\n\n\nCoursera\nAI for Medicine\n\n\nCoursera\nData Science: Foundations using R Specialization\n\n\nCoursera\nDeep Learning Specialization\n\n\nCoursera\nDeepLearning.AI TensorFlow Developer Specialization\n\n\nCoursera\nGenerative Adversarial Networks (GANs)\n\n\nCoursera\nMathematics for Machine Learning\n\n\nCoursera\nNatural Language Processing Specialization\n\n\n2023\nCoursera\nMachine Learning Engineering for Production (MLOps)\n\n\nCoursera\nMachine Learning Specialization"
  },
  {
    "objectID": "cv/cv.html#cursos",
    "href": "cv/cv.html#cursos",
    "title": "Currilum vitae",
    "section": "Cursos",
    "text": "Cursos\n\n\n\n\n\n\n\n\n\n\n\n\n2019\n    \n\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\nDatacamp\nShiny Fundamentals with R\n\n\n2020\n    \n\nCurso-R\nDeep Learning\n\n\nData Science Academy\nBig Data Fundamentos\n\n\nData Science Academy\nDeep Learning Frameworks\n\n\nData Science Academy\nIntrodução à Inteligência Artificial\n\n\nData Science Academy\nMicrosoft Power BI para Data Science\n\n\nData Science Academy\nPython Fundamentos para a Análise de Dados\n\n\nData Science Academy\nSoft Skills: Desenvolvendo Suas Habilidades Comportamentais\n\n\nData Science Academy\nWeb Scrapping e Análise de Dados\n\n\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Fundamentals with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2021\n    \n\nCoursera\nExploratory Data Analysis\n\n\nCoursera\nGetting and Cleaning Data\n\n\nCoursera\nIntroduction to Mathematical Thinking\n\n\nCoursera\nLearn to Program: Crafting Quality Code\n\n\nCoursera\nLinear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMultivariate Calculus\n\n\nCoursera\nR Programming\n\n\nCoursera\nReproducible Research\n\n\nCoursera\nThe Data Scientist's Toolbox\n\n\nCurso-R\nDashboards\n\n\nCurso-R\nDeploy\n\n\nCurso-R\nRelatórios e Visualizaçãoo de Dados\n\n\nCurso-R\nWeb Scrapping\n\n\nData Science Academy\nDeep Learning I\n\n\nData Science Academy\nDeep Learning II\n\n\nData Science Academy\nProgramação Paralela em GPU\n\n\nDatacamp\nData Scientist: Customer Channel/Marketing (Basic, Intermediate & Advanced)\n\n\nDatacamp\nData Scientist: Risk (Basic)\n\n\n2022\n    \n\nCoursera\nAI For Medical Treatment\n\n\nCoursera\nAI for Medical Diagnosis\n\n\nCoursera\nAI for Medical Prognosis\n\n\nCoursera\nBuild Basic Generative Adversarial Networks (GANs)\n\n\nCoursera\nConvolutional Neural Networks\n\n\nCoursera\nConvolutional Neural Networks in TensorFlow\n\n\nCoursera\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\n\n\nCoursera\nImproving Deep Neural Networks: HyperparameterTuning, Regularization and Optimization\n\n\nCoursera\nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Multivariate Calculus\n\n\nCoursera\nMathematics for Machine Learning: PCA\n\n\nCoursera\nNatural Language Processing in TensorFlow\n\n\nCoursera\nNatural Language Processing with Attention Models\n\n\nCoursera\nNatural Language Processing with Classification and Vector Spaces\n\n\nCoursera\nNatural Language Processing with Probabilistic Models\n\n\nCoursera\nNatural Language Processing with Sequence Models\n\n\nCoursera\nNeural Networks and Deep Learning\n\n\nCoursera\nPrincipal Component Analysis\n\n\nCoursera\nSequence Models\n\n\nCoursera\nSequences, Time Series and Prediction\n\n\nCoursera\nStructuring Machine Learning Projects\n\n\n2023\n    \n\nCoursera\nAdvanced Learning Algorithms\n\n\nCoursera\nApply Generative Adversarial Networks (GANs)\n\n\nCoursera\nBuild Better Generative Adversarial Networks (GANs)\n\n\nCoursera\nDeploying Machine Learning Models in Production\n\n\nCoursera\nGenerative AI for Everyone\n\n\nCoursera\nGenerative AI with Large Language Models\n\n\nCoursera\nIntroduction to Machine Learning in Production\n\n\nCoursera\nMachine Learning Data Lifecycle in Production\n\n\nCoursera\nMachine Learning Modeling Pipelines in Production\n\n\nCoursera\nSupervised Machine Learning: Regression and Classification\n\n\nCoursera\nUnsupervised Learning, Recommenders, Reinforcement Learning\n\n\nDeepLearning.AI\nBuilding Generative AI Applications with Gradio\n\n\nDeepLearning.AI\nBuilding Systems with the ChatGPT API\n\n\nDeepLearning.AI\nChatGPT Prompt Engineering for Developers\n\n\nDeepLearning.AI\nEvaluating and Debugging Generative AI Models Using Weights and Biases\n\n\nDeepLearning.AI\nFinetuning Large Language Models\n\n\nDeepLearning.AI\nHow Business Thinkers Can Start Building AI Plugins With Semantic Kernel\n\n\nDeepLearning.AI\nHow Diffusion Models Work\n\n\nDeepLearning.AI\nLangChain for LLM Application Development\n\n\nDeepLearning.AI\nLangChain: Chat with Your Data\n\n\nDeepLearning.AI\nLarge Language Models with Semantic Search\n\n\nDeepLearning.AI\nPair Programming with a Large Language Model\n\n\nDeepLearning.AI\nUnderstanding and Applying Text Embeddings\n\n\n2024\n    \n\nDeepLearning.AI\nBuilding and Evaluating Advanced RAG\n\n\nDeepLearning.AI\nFunctions, Tools and Agents with LangChain\n\n\nDeepLearning.AI\nLLMOps\n\n\nDeepLearning.AI\nOpen Source Models with Hugging Face\n\n\nDeepLearning.AI\nPreprocessing Unstructured Data for LLM Applications\n\n\nDeepLearning.AI\nQuality and Safety for LLM Applications\n\n\nDeepLearning.AI\nServerless LLM Apps Amazon Bedrock\n\n\nDeepLearning.AI\nVector Databases: from Embeddings to Applications"
  },
  {
    "objectID": "cv/cv.html#apresentações",
    "href": "cv/cv.html#apresentações",
    "title": "Currilum vitae",
    "section": "Apresentações",
    "text": "Apresentações\n\nInúmeras apresentações realizadas em congressos e outros eventos acadêmicos nacionais e internacionais da área da Ecologia e da Limnologia;\n\nAnálise multivariada dos dados do monitoramento geotécnico aplicada à detecção de anomalias (19 Out 2023, GTGH - Grupo Temático de Geotecnia e Hidrogeologia - Belo Horizonte, Brasil);\n\nMonitoramento geotécnico orientado à dados: desafios, a jornada e lições aprendidas (27 Set 2023, I Encontro Nacional de Centros de Monitoramento Geotécnico - Belo Horizonte, Brasil);\n\nAdvanced analytics applied to InSAR monitoring program (02 Dez 2023, LOP - Large Open Pit Project - Vancouver, Canadá)."
  },
  {
    "objectID": "cv/cv.html#publicações",
    "href": "cv/cv.html#publicações",
    "title": "Currilum vitae",
    "section": "Publicações",
    "text": "Publicações\n\n25+ publicações em revistas acadêmicas internacionais de alto impacto, focando na pesquisa que desenvolvi dentro da Ecologia (listadas no meu Lattes);\n\n\nBlog e portfolio pessoal: https://nacmarino.netlify.app/;\n\n\n8 Lições na construção do monitoramento geotécnico orientado à dados;\n\nMachine learning applied to control levels and anomaly detection in a tailing dam monitoringd data (9th International Conference on Tailings Management 2023, Chile);\n\nAdvanced analytics applied to InSAR Dam Monitoring Program (Tailings and Mine Waste 2023, Vancouver, Canadá)."
  },
  {
    "objectID": "cv/cv.html#referências",
    "href": "cv/cv.html#referências",
    "title": "Currilum vitae",
    "section": "Referências",
    "text": "Referências\nDisponível sob demanda."
  },
  {
    "objectID": "cv/cv-en.html",
    "href": "cv/cv-en.html",
    "title": "Resume",
    "section": "",
    "text": "nac.marino@gmail.com"
  },
  {
    "objectID": "cv/cv-en.html#education",
    "href": "cv/cv-en.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\nPostdoctoral Research in Ecology – Climate Change and Trophic Interactions Universidade Federal do Rio de Janeiro Mar 2016 — Jun 2019\n\nEffect of climate change on ecological interactions\nFellow of the National Postdoctoral Program (CAPES)\n\nPh.D. in Ecology – Climate Change, Community Ecology, and Ecosystems Universidade Federal do Rio de Janeiro Mar 2012 — Feb 2016\n\nThe effect of predators on the structure and functioning of trophic chains and their interaction with climate change\nCAPES Fellow\nBest Ph.D. thesis defended in the Ecology Graduate Program (UFRJ) in 2016\nSandwich Ph.D. period at the University of British Columbia (Oct 2014 - May 2015)\n\nMaster’s in Ecology – Community Ecology and Ecosystems Universidade Federal do Rio de Janeiro Aug 2009 — Jul 2011\n\nStructuring of the community of aquatic macroinvertebrates in tank bromeliads\nCAPES Fellow\n\nBachelor’s in Biological Sciences – Ecology Universidade Federal do Rio de Janeiro Aug 2005 — Jul 2009\n\nTank bromeliads as islands: the influence of habitat characteristics on the richness of aquatic macroinvertebrates\nScientific Initiation - FAPERJ Fellow"
  },
  {
    "objectID": "cv/cv-en.html#experience",
    "href": "cv/cv-en.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\nAccenture\nData Science Principal Jan 2021 – Current\nTech Lead in teams and projects related to data science, working across the entire analytical solution flow, particularly in analysis, technical design, and proposing solutions to data problems, extracting insights from data, and communicating with stakeholders and clients. I also assist with the management of data teams, especially in mentoring younger professionals. My main contributions have been:\n\nInvolvement in Geotechnical projects, focusing on the development of data products for monitoring terrain deformations in mining structures, based on the multi-temporal analysis of interferometric images captured by satellite-borne synthetic aperture radars (InSAR). The products developed here have over 30 examples of use cases, contributing to business risk management and reduction;\nDevelopment of analytical solutions for geophysics, where we seek to facilitate the analysis of data from soil electrical monitoring (electrical resistivity) for the identification of areas with water accumulation within dams;\nDevelopment of analytical solutions to improve the tactical and operational planning of the Corporate Security team operating in a large port in Brazil;\nManagement of a demand forecasting project (i.e., forecasting the expected volume of sales) for a large Brazilian food company;\nMentoring data professionals at all levels of seniority;\nPreparation of technical design for data problems in proposals;\nCollaboration with project managers, teams, and other peers for the development of a better working environment.\n\nData Science Consultant Jul 2019 – Dec 2020\nI worked with projects related to data science, with a special focus on predictive analyses. I led and collaborated on the design of solutions for data problems, exploratory, descriptive, and predictive analyses, measurement of deliverable benefits, as well as assisting leadership in guiding younger project members. My main contributions were:\n\nDevelopment of analytical solutions to improve the tactical and operational planning of the Corporate Security team to reduce the impact of forced train stoppages on - production flow in a railway. The business area attributed a 30% reduction in train stoppage volume to the analytical solution in the first months of solution operation;\nDevelopment of analytical solutions to improve the tactical and operational planning of the Corporate Security team to reduce the impact of theft on signaling assets in a railway. The business area reported a roughly 50% drop in the rate of asset thefts in the first months of solution operation, while data monitoring showed a decrease of up to 70% in false alarm response during the night period;\nConducted a root cause analysis to understand the results of an experiment on the impact of feeding briquettes into a calcining furnace, providing important insights into the state of the briquettes and the operation of the calcining furnace, which clarified the results and guided a new experiment;\nCollaborated on delivering forecasts of expected sales volumes for food products for a large company operating in Brazil.\nPrograma de Pós-Graduação em Ecologia (UFRJ)\nVisiting Scholar Mar 2016 – Jun 2019\nI implemented several disciplines related to programming and data analysis in the postgraduate program, such as: (1) Introduction to the R Language, (2) Data Handling, Visualization, and Sharing, and (3) Systematic Review and Meta-analysis. Additionally, I contributed as one of the instructors for the course on Experimental Design and Statistics, as well as co-supervised master’s and doctoral students in the program.\nLaboratório de Limnologia (UFRJ)\nResearcher May 2006 – Jun 2019\nI studied and researched topics related to the functioning and dynamics of aquatic ecosystems such as rivers, streams, and ponds, as well as model aquatic ecosystems like tank bromeliads. The main highlights here were:\n\nPlanning, designing, and conducting experiments, and statistical analysis of data;\nWriting and publishing over 20 papers in high-impact international scientific journals;\nParticipation in international research groups;\nLearning and performing various physicochemical analyses of water quality, both in the laboratory and in the field;\nIdentification of insects and other aquatic organisms;\nSupervision of undergraduate, master’s, and doctoral students;\nPresentation of papers at conferences and lectures for undergraduate, master’s, and doctoral students.\n\nProjeto de Monitoramento dos Igarapés da FLONA Sacará-Taquera Sep 2009 – Dec 2019\nI was the technical leader and responsible for the field teams in this project, which aimed to develop an environmental integrity index for the FLONA streams based on the composition and abundance of aquatic invertebrate and fish species. The idea was to use this information to determine the impact of a mining operation on each of the streams near the operation. We successfully achieved this objective and presented the methodology used to the relevant regulatory authority, which evaluated it as unprecedented in the country. The main highlights here were:\n\nPlanning fieldwork and coordinating the collection team;\nCuration and organization of project data around a relational database;\nDesign of the project’s analytical framework;\nDevelopment of an algorithm for discriminating impacted and non-impacted streams;\nWriting technical reports for submission to the relevant regulatory authority (IBAMA).\n\nProjeto de Monitoramento das Lagoas Costeiras do Norte Fluminense Jan 2007 – Jun2010\nI collaborated in conducting field surveys, chemical analyses, and data interpretation regarding the water characteristics and planktonic fauna of the studied lagoons.\nPrograma de Pós-Graduação em Ecologia (UNICAMP)\nVisiting Scholar Jun 2019\nI taught a course on Introduction to Data Handling and Visualization for the master’s and doctoral students in the postgraduate program."
  },
  {
    "objectID": "cv/cv-en.html#skills",
    "href": "cv/cv-en.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\n\n\n\n\n\n\n\nHARD SKILLS\n      \n    \n\n\nData Wrangling\n\n\n\nExploratory Analysis\n\n\n\nMeta-analysis\n\n\n\nDeep Learning\n\n\n\nProgramming\n\n\n\nGeospatial Analysis\n\n\n\nPLN\n\n\n\nTime Series\n\n\n\nWeb Scraping\n\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n    \n\n\nExperimental Design\n\n\n\nFrequentist Statistics\n\n\n\nData Visualization\n\n\n\nMachine Learning (Non DL)\n\n\n\nAzure\n\n\n\nMLOps\n\n\n\nRelational Databases\n\n\n\nVersion Control\n\n\n\nAWS\n\n\n\nGenAI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOFT SKILLS\n      \n    \n\n\nEnglish\n\n\n\nCreativity\n\n\n\nProactive\n\n\n\nProblem Solving\n\n\n\nSelf-organization\n\n\n\nStorytelling\n\n\n\nWritten Communication\n\n\n\nPeople Management\n\n\n\nSpanish\n\n\n\nFlexibility"
  },
  {
    "objectID": "cv/cv-en.html#tools",
    "href": "cv/cv-en.html#tools",
    "title": "Resume",
    "section": "Tools",
    "text": "Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nPython\n\n\n\npandas\n\n\n\ntidyverse\n\n\n\nShiny\n\n\n\nKeras\n\n\n\nPyTorch\n\n\n\nTensorflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Point\n\n\n\nggplot2\n\n\n\nscikit-learn\n\n\n\nAzure Machine Learning\n\n\n\ntidymodels\n\n\n\nPower BI\n\n\n\nSQL\n\n\n\nTorch for R"
  },
  {
    "objectID": "cv/cv-en.html#licenses-and-certifications",
    "href": "cv/cv-en.html#licenses-and-certifications",
    "title": "Resume",
    "section": "Licenses and Certifications",
    "text": "Licenses and Certifications\n\n\n\n\n\n\n\n\n\n\n\n\n2019\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\n2020\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2022\nAWS\nAWS Certified Cloud Practitioner\n\n\nAWS\nAWS Certified Machine Learning Specialty\n\n\nAWS\nAWS Certified Solutions Architect Associate\n\n\nCoursera\nAI for Medicine\n\n\nCoursera\nData Science: Foundations using R Specialization\n\n\nCoursera\nDeep Learning Specialization\n\n\nCoursera\nDeepLearning.AI TensorFlow Developer Specialization\n\n\nCoursera\nGenerative Adversarial Networks (GANs)\n\n\nCoursera\nMathematics for Machine Learning\n\n\nCoursera\nNatural Language Processing Specialization\n\n\n2023\nCoursera\nMachine Learning Engineering for Production (MLOps)\n\n\nCoursera\nMachine Learning Specialization"
  },
  {
    "objectID": "cv/cv-en.html#courses",
    "href": "cv/cv-en.html#courses",
    "title": "Resume",
    "section": "Courses",
    "text": "Courses\n\n\n\n\n\n\n\n\n\n\n\n\n2019\n    \n\nDatacamp\nData Analyst with Python\n\n\nDatacamp\nData Scientist with Python\n\n\nDatacamp\nPython Programmer\n\n\nDatacamp\nShiny Fundamentals with R\n\n\n2020\n    \n\nCurso-R\nDeep Learning\n\n\nData Science Academy\nBig Data Fundamentos\n\n\nData Science Academy\nDeep Learning Frameworks\n\n\nData Science Academy\nIntrodução à Inteligência Artificial\n\n\nData Science Academy\nMicrosoft Power BI para Data Science\n\n\nData Science Academy\nPython Fundamentos para a Análise de Dados\n\n\nData Science Academy\nSoft Skills: Desenvolvendo Suas Habilidades Comportamentais\n\n\nData Science Academy\nWeb Scrapping e Análise de Dados\n\n\nDatacamp\nInteractive Data Visualization with R\n\n\nDatacamp\nMachine Learning Fundamentals with R\n\n\nDatacamp\nMachine Learning Specialist with R\n\n\nDatacamp\nSupervised Machine Learning with R\n\n\nDatacamp\nUnsupervised Machine Learning with R\n\n\n2021\n    \n\nCoursera\nExploratory Data Analysis\n\n\nCoursera\nGetting and Cleaning Data\n\n\nCoursera\nIntroduction to Mathematical Thinking\n\n\nCoursera\nLearn to Program: Crafting Quality Code\n\n\nCoursera\nLinear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMultivariate Calculus\n\n\nCoursera\nR Programming\n\n\nCoursera\nReproducible Research\n\n\nCoursera\nThe Data Scientist's Toolbox\n\n\nCurso-R\nDashboards\n\n\nCurso-R\nDeploy\n\n\nCurso-R\nRelatórios e Visualizaçãoo de Dados\n\n\nCurso-R\nWeb Scrapping\n\n\nData Science Academy\nDeep Learning I\n\n\nData Science Academy\nDeep Learning II\n\n\nData Science Academy\nProgramação Paralela em GPU\n\n\nDatacamp\nData Scientist: Customer Channel/Marketing (Basic, Intermediate & Advanced)\n\n\nDatacamp\nData Scientist: Risk (Basic)\n\n\n2022\n    \n\nCoursera\nAI For Medical Treatment\n\n\nCoursera\nAI for Medical Diagnosis\n\n\nCoursera\nAI for Medical Prognosis\n\n\nCoursera\nBuild Basic Generative Adversarial Networks (GANs)\n\n\nCoursera\nConvolutional Neural Networks\n\n\nCoursera\nConvolutional Neural Networks in TensorFlow\n\n\nCoursera\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\n\n\nCoursera\nImproving Deep Neural Networks: HyperparameterTuning, Regularization and Optimization\n\n\nCoursera\nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n\n\nCoursera\nMathematics for Machine Learning: Linear Algebra\n\n\nCoursera\nMathematics for Machine Learning: Multivariate Calculus\n\n\nCoursera\nMathematics for Machine Learning: PCA\n\n\nCoursera\nNatural Language Processing in TensorFlow\n\n\nCoursera\nNatural Language Processing with Attention Models\n\n\nCoursera\nNatural Language Processing with Classification and Vector Spaces\n\n\nCoursera\nNatural Language Processing with Probabilistic Models\n\n\nCoursera\nNatural Language Processing with Sequence Models\n\n\nCoursera\nNeural Networks and Deep Learning\n\n\nCoursera\nPrincipal Component Analysis\n\n\nCoursera\nSequence Models\n\n\nCoursera\nSequences, Time Series and Prediction\n\n\nCoursera\nStructuring Machine Learning Projects\n\n\n2023\n    \n\nCoursera\nAdvanced Learning Algorithms\n\n\nCoursera\nApply Generative Adversarial Networks (GANs)\n\n\nCoursera\nBuild Better Generative Adversarial Networks (GANs)\n\n\nCoursera\nDeploying Machine Learning Models in Production\n\n\nCoursera\nGenerative AI for Everyone\n\n\nCoursera\nGenerative AI with Large Language Models\n\n\nCoursera\nIntroduction to Machine Learning in Production\n\n\nCoursera\nMachine Learning Data Lifecycle in Production\n\n\nCoursera\nMachine Learning Modeling Pipelines in Production\n\n\nCoursera\nSupervised Machine Learning: Regression and Classification\n\n\nCoursera\nUnsupervised Learning, Recommenders, Reinforcement Learning\n\n\nDeepLearning.AI\nBuilding Generative AI Applications with Gradio\n\n\nDeepLearning.AI\nBuilding Systems with the ChatGPT API\n\n\nDeepLearning.AI\nChatGPT Prompt Engineering for Developers\n\n\nDeepLearning.AI\nEvaluating and Debugging Generative AI Models Using Weights and Biases\n\n\nDeepLearning.AI\nFinetuning Large Language Models\n\n\nDeepLearning.AI\nHow Business Thinkers Can Start Building AI Plugins With Semantic Kernel\n\n\nDeepLearning.AI\nHow Diffusion Models Work\n\n\nDeepLearning.AI\nLangChain for LLM Application Development\n\n\nDeepLearning.AI\nLangChain: Chat with Your Data\n\n\nDeepLearning.AI\nLarge Language Models with Semantic Search\n\n\nDeepLearning.AI\nPair Programming with a Large Language Model\n\n\nDeepLearning.AI\nUnderstanding and Applying Text Embeddings\n\n\n2024\n    \n\nDeepLearning.AI\nBuilding and Evaluating Advanced RAG\n\n\nDeepLearning.AI\nFunctions, Tools and Agents with LangChain\n\n\nDeepLearning.AI\nLLMOps\n\n\nDeepLearning.AI\nOpen Source Models with Hugging Face\n\n\nDeepLearning.AI\nPreprocessing Unstructured Data for LLM Applications\n\n\nDeepLearning.AI\nQuality and Safety for LLM Applications\n\n\nDeepLearning.AI\nServerless LLM Apps Amazon Bedrock\n\n\nDeepLearning.AI\nVector Databases: from Embeddings to Applications"
  },
  {
    "objectID": "cv/cv-en.html#presentations",
    "href": "cv/cv-en.html#presentations",
    "title": "Resume",
    "section": "Presentations",
    "text": "Presentations\n\nNumerous presentations given at national and international ecology and limnology conferences and other academic events;\n\nMultivariate analysis of geotechnical monitoring data applied to anomaly detection (19 Oct 2023, GTGH - Grupo Temático de Geotecnia e Hidrogeologia - Belo Horizonte, Brazil);\n\nMonitoramento geotécnico orientado à dados: desafios, a jornada e lições aprendidas (27 Sep 2023, I Encontro Nacional de Centros de Monitoramento Geotécnico - Belo Horizonte, Brazil);\n\nAdvanced analytics applied to InSAR monitoring program (02 Dez 2023, LOP - Large Open Pit Project - Vancouver, Canada)."
  },
  {
    "objectID": "cv/cv-en.html#publications",
    "href": "cv/cv-en.html#publications",
    "title": "Resume",
    "section": "Publications",
    "text": "Publications\n\n25+ publications in high-impact international academic journals, focusing on the research I conducted within Ecology (you can find all of them listed on my Lattes);\n\n\nBlog and personal portfolio: https://nacmarino.netlify.app/;\n\n\n8 Lições na construção do monitoramento geotécnico orientado à dados;\n\nMachine learning applied to control levels and anomaly detection in a tailing dam monitoringd data (9th International Conference on Tailings Management 2023, Chile);\n\nAdvanced analytics applied to InSAR Dam Monitoring Program (Tailings and Mine Waste 2023, Vancouver, Canadá)."
  },
  {
    "objectID": "cv/cv-en.html#references",
    "href": "cv/cv-en.html#references",
    "title": "Resume",
    "section": "References",
    "text": "References\nAvailable upon request."
  },
  {
    "objectID": "posts/2024-02-28_la-e-de-volta/index.html",
    "href": "posts/2024-02-28_la-e-de-volta/index.html",
    "title": "Lá e de volta outra vez",
    "section": "",
    "text": "Em 2021 comecei a escrever esse blog com a intenção de praticar e documentar meu aprendizado enquanto eu tentava abordar as perguntas e curiosidades do meu dia a dia. Além disso, ele também era uma válvula de escape das tarefas do trabalho, onde muitas vezes acabamos tendo que ficar mais distantes de colocar a mão na massa para poder atuar através das outras pessoas. Olhando para trás, acredito que a ideia do blog foi boa e funcional, conseguindo preencher bem a necessidade que eu sinto de continuar perto da área técnica.\nOutro nascimento importante em 2021 foi o do meu filho. Essa é uma experiência de vida incrível e, como não queria perder um momento sequer, fui alocando cada vez menos tempo para o blog e outras coisas além do trabalho e estar com ele. Com isso, resolvi dar uma parada com a escrita em meados de 2022.\nDe lá para cá a disponibilidade de tempo está cada vez menor…então, por quê voltar com o blog? O ponto é que a vontade de sujar as mãos continua alta e, na realidade, eu só deixei de escrever sobre o perocesso, não de fazê-lo. Outra coisa foi o amadurecimento que veio com a minha atuação profissional: seus pontos de vista, comentários e orientações passam a ter um valor e impacto maior, podendo ser tão relevantes quanto o código que você escreve1. Assim, me pareceu certo voltar a ter esse espaço que me faz bem e que, agora, pode ser usado para outras coisas que não só codar."
  },
  {
    "objectID": "posts/2024-02-28_la-e-de-volta/index.html#footnotes",
    "href": "posts/2024-02-28_la-e-de-volta/index.html#footnotes",
    "title": "Lá e de volta outra vez",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nNão acho que eu escreva código tão bem, tampouco que seja um influenciador na área de dados, IA, ML…no entanto, fico feliz se o que penso faz sentido e possa ajudar uma ou outra pessoa de vez em quando.↩︎\nE, para piorar, eu não sabia a versão dos pacotes que havia usado há dois anos atrás↩︎"
  },
  {
    "objectID": "posts/2024-03-08_baselines/index.html",
    "href": "posts/2024-03-08_baselines/index.html",
    "title": "Tudo começa de algum lugar - até mesmo a sua solução analítica",
    "section": "",
    "text": "Comecei a minha carreira na academia, onde tive a oportunidade de aprender algumas coisas interessantes e incorporá-las à forma de pensar. Uma delas foi a habilidade de duvidar, questionar e criticar1. Existem casos em que é muito fácil exercitar essa habilidade, mas têm vezes que a questão não é tão trivial. Ainda assim, esse tipo de exercício tende a ser inconsciente para mim. Mas o que é que isso tem haver com Ciência de Dados?\nÁs vezes me parece que passamos batido, mas existe a palavra ciência no nome daquela disciplina - e a ciência nos oferece um jeito muito poderoso de pensar, testar hipóteses e avaliar o que nos é mostrado. Por exemplo: como você sabe que A é melhor do que B, que uma intervenção funciona, que não tem diferença entre duas coisas…como saber se você realmente descobriu o que você diz ter descoberto2? O método científico é a resposta para estas perguntas, e ele nos ensina a pensar de forma estruturada para ir das nossas perguntas à respostas bem sustentadas. E essas são os tipos de coisas que estão diretamente relacionadas ao nosso dia a dia enquanto pessoas que trabalham com ciência de dados.\nCom isso em mente, o objetivo desse post é falar um pouco mais sobre como pensar cientificamente no nosso dia a dia. Vamos focar primeiro em uma tarefa mais cotidiana, que é a modelagem preditiva; depois, vamos ver como esse tipo de pensamento faz parte não só da modelagem, mas de qualquer outra etapa de um projeto de Ciência de Dados."
  },
  {
    "objectID": "posts/2024-03-08_baselines/index.html#footnotes",
    "href": "posts/2024-03-08_baselines/index.html#footnotes",
    "title": "Tudo começa de algum lugar - até mesmo a sua solução analítica",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nNão é no sentido ruim, mas sim no de ter bom-senso para julgar o que me é mostrado.↩︎\nIsto é, algo robusto, generalizável, livre de artefatos, viéses ou frutos de relações espúrias.↩︎\nInclusive, essa é uma abordagem comum quando temos uma frequência muito desigual das categorias na base de dados, mas isso é estória para outro momento.↩︎\nVou abstrair muitas coisas nesses exemplos, como a forma como esse baseline precisa conversar com o objetivo do negócio, a métrica de avaliação, as etapas de preparação dos dados, pré-processamento e tudo o mais.↩︎\nAlgo que já vi ser comum para aplicações com redes neurais.↩︎\nConsiderando o caso da classificação binária, o objetivo aqui é simular o comportamneto do modelo em dar uma probabilidade próximo à 0 para uma instância pertencer a classe 1.↩︎\nNo entanto, a pessoa acabou ignorando esse fato e partindo para uma solução analítica direto.↩︎\nAfinal, tais modelos são apenas um componente de um processo muito maior, e pode ser que as ineficiências estejam escondidas em outros lugares.↩︎\nLembro de ter visto alguma coisa no PowerBI que podia ser utilizado para o mesmo propósito.↩︎\ni.e., um tratamento↩︎\ni.e., um baseline ou controle↩︎"
  },
  {
    "objectID": "posts/2024-12-31_genai-track/index.html",
    "href": "posts/2024-12-31_genai-track/index.html",
    "title": "Como eu começaria a estudar GenAI hoje?",
    "section": "",
    "text": "Código# carregando o reticulate para interagir com o Python\nlibrary(reticulate)\n\n# criando um ambiente do conda para esse post, caso ele nao exista\nif(!condaenv_exists(envname = 'blog_post_env')) {\n  # criando o ambiente do conda que sera usado nessa sessao\n  conda_create(envname = 'blog_post_env', python_version = '3.11')\n  \n  # lendo os pacotes e versoes que precisam ser instaladas no ambiente a partir do requirements.txt\n  requirements &lt;- read.delim(file = 'requirements.txt', header = FALSE)$V1\n  \n  # importando as libs para este post\n  conda_install(envname = 'blog_post_env', packages = requirements, pip = TRUE)\n}\n\n# apontando para o ambiente desse post\nuse_condaenv(condaenv = 'blog_post_env')\n\n\nUma ideia que sempre me fez muito sentido é a fala da Rainha Vermelha, do conto Alice no País das Maravilhas: ‘…aqui, você vê, é necessário correr o mais rápido que puder para continuar no mesmo lugar. Se você quiser chegar a algum outro lugar, você deve correr pelo menos duas vezes mais rápido do que está fazendo.’. No mundo de Dados, IA e Machine Learning, isso acontece o tempo todo: ciência de dados era a profissão mais sexy na última década…nessa, parece ser a de analista de dados. De forma similar, novas ferramentas e tecnologias surgem todo dia, e é difícil conseguir manter o ritmo e se manter atualizado.\n\nNessa linha, a hype da vez é IA Generativa (i.e., GenAI), que tomou o mundo desde o ano passado. Se você não vive em uma caverna, deve estar sendo bombardeado com um volume enorme de notícias, informações, aplicações, casos de uso e tudo o mais relacionado à essa nova tecnologia. Inclusive, é possível até que você tenha visto casos em que GenAI esteja sendo adicionado à soluções à qualquer custo - ainda que isso não faça sentido. De qualquer forma, tudo isso pode gerar um medo de ficar de fora e, voltando à Rainha Vermelha, por mais que tentemos nos manter à par de tudo o que está rolando, o volume de novidades e coisas para aprender é alto demais para darmos vazão.\nComo uma das inúmeras pessoas que vive esse dilema diariamente, acredito que a primeira ação para mudar um pouco essa perspectiva é aceitar que realmente não há como manter o ritmo de aprendizado. E está tudo bem com isso: afinal, existem muitos outros pratinhos que precisamos equilibrar e prioridades que têm mais valor em nossa vida. A segunda ação é tentar enxergar que volume de informação não é a mesma coisa que qualidade: a maior parte dos conteúdos não vão agregar tanta informação nova, complementar ou que vai ser realmente uma tendência. Assim, se conseguíssemos focar naquilo que têm mais impacto, poderíamos conseguir nos manter dentro de uma zona de conhecimento que talvez seja mais sustentável.1\nAssumindo que essa linha de raciocínio faça sentido, voltamos ao problema inicial: têm muita material disponível, e pode ser difícil saber separar o que é útil e o que não é. Ao longo dos últimos meses tenho percebido esse padrão, e colocado um bocado de energia em tentar me manter atualizado e informado sobre o que têm saído. E, durante um tempo, eu achei que estava dando sorte em achar e escolher os conteúdos certos…mas depois o que eu comecei a pensar é que talvez eu esteja em uma posição muito privilegiada de saber distilar o que faz sentido e o que não faz, e priorizar no que focar no tempo que tenho.\nIsso me trouxe um questionamento…e se eu estivesse começando hoje a estudar IA Generativa, que jornada eu percorreria? Mais do que isso…nem todo mundo quer ou precisa ser um expert em IA Generativa…pode ser que só saber a ferramenta esteja de bom tamanho, ou ter mais profundidade sobre um assunto específico no tema. Por outro lado, pode ser o caso de querer ter um conhecimento mais abrangente, focando não só em IA Generativa, mas tudo o que está na fundação e por trás disso.\n\nCódigo# carregando os pacotes para esse post\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# criando um dataframe para armazenar as informacoes que vamos plotar e facilitar o processo\ndf = pd.DataFrame({\n  'id'       : ['rect_' + str(i) for i in range(6)],\n  'position' : [(0, 0), (0.3, 0.1), (0.58, 0.15), (0.8, 0.2), (0.84, 0.21), (0.87, 0.22)],\n  'x'        : [1, 0.65, 0.35, 0.12, 0.07, 0.03],\n  'y'        : [1, 0.6, 0.3, 0.12, 0.06, 0.01],\n  'texto'    : ['Inteligência Artificial', 'Aprendizado de Máquina', 'Deep Learning', np.nan, np.nan, np.nan],\n  'subs'     : ['(e.g., sistemas baseados em regra, aprendizado de máquina,...)', '(e.g., supervisionado, não-supervisionado,\\npor reforço,...)', '(e.g., redes convolucionais,\\nrecorrentes, atenção,...)', np.nan, np.nan, np.nan],\n  'linecolor': ['black', 'white', 'white', 'white', 'white', 'white'],\n  'linesize' : [1, 0.7, 0.7, 0.7, 0.7, 0.7],\n  'rectfill' : ['#C6DBEF', '#9ECAE1', '#6BAED6', '#4292C6', '#2171B5', '#08306B'],\n  'texto_x'  : [0.03, 0.33, 0.61, 0.63, np.nan, np.nan],\n  'texto_y'  : [0.95, 0.65, 0.4, 0.35, np.nan, np.nan],\n  'sub_x'    : [0.03, 0.33, 0.61, np.nan, np.nan, np.nan],\n  'sub_y'    : [0.92, 0.6, 0.35, np.nan, np.nan, np.nan]\n})\n\n# criando um dicionario para definir a fonte associada à maior parte dos textos\nfonte = {'size': 12, 'weight': 'bold', 'family': 'Permanent Marker'}\nsubs = {'size': 8, 'family': 'Raleway'}\n\n# criando o canvas da figura\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (6, 6))\n\n# criando uma lista vazia para armazenar cada um dos retangulos criados no loop\nrects = []\n\n# criando cada um dos retângulos através de um loop\nfor row in df.itertuples(index = False):\n  rects.append(patches.Rectangle(\n    xy = row.position, width = row.x, height = row.y, \n    linewidth = row.linesize,edgecolor = row.linecolor, facecolor = row.rectfill)\n  )\n  if row.texto is not np.nan:\n    plt.text(x = row.texto_x, y = row.texto_y, s = row.texto, color = 'black', fontdict = fonte)\n    plt.text(x = row.sub_x, y = row.sub_y, s = row.subs, color = 'black', fontdict = subs)\n\n# adicionando cada um dos retângulos em um loop através da lista\nfor rect in rects:\n  ax.add_patch(rect)\n\n# adicionando anotações na lateral direita do plot\nax.annotate(text = 'GANs', xy = (1.05, 0.35), fontsize = 12, fontweight = 'bold', annotation_clip = False, \n            family = 'Permanent Marker')\nax.annotate(text = '(Generative Adversarial\\nNetworks; aka GenAI)', xy = (1.05, 0.31), fontsize = 7, \n            annotation_clip = False, family = 'Raleway')\nax.annotate(text = '', xy = (0.88, 0.29), xytext = (1.04, 0.36), xycoords = 'data', \n            arrowprops = dict(arrowstyle = '-|&gt;', color = 'black', lw = 1, connectionstyle = \"arc3,rad=0.3\"))\nax.annotate(text = 'LMs', xy = (1.05, 0.25), fontsize = 12, fontweight = 'bold', annotation_clip = False, \n            family = 'Permanent Marker')\nax.annotate(text = '(Language\\nModels)', xy = (1.05, 0.21), fontsize = 7, annotation_clip = False, \n            family = 'Raleway')\nax.annotate(text = '', xy = (0.88, 0.24), xytext = (1.04, 0.26), xycoords = 'data', \n            arrowprops = dict(arrowstyle = '-|&gt;', color = 'black', lw = 1, connectionstyle = \"arc3,rad=0.2\"))\nax.annotate(text = 'GPT-family', xy = (1.05, 0.15), fontsize = 12, fontweight = 'bold', annotation_clip = False, \n            family = 'Permanent Marker')\nax.annotate(text = '(GPT-4 Turbo, GPT-4,\\nGPT-3.5 Turbo,...)', xy = (1.05, 0.11), fontsize = 7, \n            annotation_clip = False, family = 'Raleway')\nax.annotate(text = '', xy = (0.88, 0.225), xytext = (1.04, 0.17), xycoords = 'data', \n            arrowprops = dict(arrowstyle = '-|&gt;', color = 'black', lw = 1, connectionstyle = \"arc3,rad=0.3\"))\n            \n# adicionando anotações na parte de baixo da figura\nax.annotate(text = 'A relação entre o tamanho das caixas é apenas qualitativa e ilustrativa.', \n            xy = (0, -0.02), fontsize = 6, annotation_clip = False, family = 'Raleway')\nax.annotate(text = 'nacmarino.netlify.app', xy = (1.1, -0.02), fontsize = 6, ha = 'left', \n            annotation_clip = False, family = 'Passion One')\n\n# removendo os completamente as informações dos eixos da figura \nax.set_axis_off()\nfig.patch.set_facecolor('white')\n\n# plotando a figura\nplt.tight_layout()\nplt.show()\n\n\n\nApesar de parecer que IA Generativa e Language Models são uma coisa por si só, na realidade, eles são um dos componentes e campos do conhecimento de algo muito maior - e que fornece todas as bases teóricas para podemos entender no detalhe como funcionam e fazem o que fazem. No entanto, uma pessoa não precisa dominar toda essa hierarquia de conhecimento para atuar com IA Generativa - os focos e jornadas de aprendizado são individuais, e é perfeitamente possível pular alguns desses níveiss dependendo do seu objetivo.\n\n\n\n\n\n\n\n\n\nNota sobre a figura\n\n\n\n\n\nA figura acima abstrai um pouco algumas complexidades de se colocar àquelas áreas do conhecimento em caixinhas. Por exemplo, os (Large) Language Models combinam alguma coisa de Redes Generativas Adversariais, arquitetura de Transformers e aprendizagem por reforço (além de outras áreas do conhecimento como, por exemplo, linguística). Ficaria bem difícil representar toda essa sobreposição em uma imagem em 2D, então optei pelo caminho mais simples para facilitar a compreensão da mensagem principal. De toda forma, achei relevante ter esse disclaimer aqui.\n\n\n\nEssas questões me levaram a pensar em por quê não criar uma lista que consolide uma jornada de aprendizado em IA Generativa. Talvez uma lista mais genérica, que possa servir tanto de guia para quem só está interessado em ter contexto, quanto para quem quiser percorrer uma jornada de aprendizado mais longa e profunda. O intuito não é ter a lista definitiva ou a lista certa, nem nada disso…mas talvez uma lista que tenha um sentido lógico e uma cadência, que não seja nada muito complexo ou que vá exigir foco contínuo por muito tempo…e, mais importante, que seja flexível o suficiente para a maior parte dos objetivos de aprendizado."
  },
  {
    "objectID": "posts/2024-12-31_genai-track/index.html#footnotes",
    "href": "posts/2024-12-31_genai-track/index.html#footnotes",
    "title": "Como eu começaria a estudar GenAI hoje?",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nSei que isso pode parecer conformismo ou falta de vontade de se esforçar, mas meu ponto de vista é que existem inúmeras formas de se chegar no mesmo lugar…a escolha entre elas vai depender de quanto tempo você tem e quão rápido você quer percorrer esse caminho.↩︎"
  },
  {
    "objectID": "posts/2024-06-24_genai-track/index.html",
    "href": "posts/2024-06-24_genai-track/index.html",
    "title": "Como eu começaria a estudar GenAI hoje?",
    "section": "",
    "text": "Código# carregando o reticulate para interagir com o Python\nlibrary(reticulate)\n\n# criando um ambiente do conda para esse post, caso ele nao exista\nif(!condaenv_exists(envname = 'blog_post_env')) {\n  # criando o ambiente do conda que sera usado nessa sessao\n  conda_create(envname = 'blog_post_env', python_version = '3.11')\n  \n  # lendo os pacotes e versoes que precisam ser instaladas no ambiente a partir do requirements.txt\n  requirements &lt;- read.delim(file = 'requirements.txt', header = FALSE)$V1\n  \n  # importando as libs para este post\n  conda_install(envname = 'blog_post_env', packages = requirements, pip = TRUE)\n}\n\n# apontando para o ambiente desse post\nuse_condaenv(condaenv = 'blog_post_env')\n\n\nUma ideia que sempre me fez muito sentido é a fala da Rainha Vermelha, do conto Alice no País das Maravilhas: ‘…aqui, você vê, é necessário correr o mais rápido que puder para continuar no mesmo lugar. Se você quiser chegar a algum outro lugar, você deve correr pelo menos duas vezes mais rápido do que está fazendo.’. No mundo de Dados, IA e Machine Learning, isso acontece o tempo todo: ciência de dados era a profissão mais sexy na última década…nessa, parece ser a de analista de dados. De forma similar, novas ferramentas e tecnologias surgem todo dia, e é difícil conseguir manter o ritmo e se manter atualizado.\n\nNessa linha, a hype da vez é IA Generativa (i.e., GenAI), que tomou o mundo desde o ano passado. Se você não vive em uma caverna, deve estar sendo bombardeado com um volume enorme de notícias, informações, aplicações, casos de uso e tudo o mais relacionado à essa nova tecnologia. Inclusive, é possível até que você tenha visto casos em que GenAI esteja sendo adicionado à soluções à qualquer custo - ainda que isso não faça sentido. De qualquer forma, tudo isso pode gerar um medo de ficar de fora e, voltando à Rainha Vermelha, por mais que tentemos nos manter à par de tudo o que está rolando, o volume de novidades e coisas para aprender é alto demais para darmos vazão.\nComo uma das inúmeras pessoas que vive esse dilema diariamente, acredito que a primeira ação para mudar um pouco essa perspectiva é aceitar que realmente não há como manter o ritmo de aprendizado. E está tudo bem com isso: afinal, existem muitos outros pratinhos que precisamos equilibrar e prioridades que têm mais valor em nossa vida. A segunda ação é tentar enxergar que volume de informação não é a mesma coisa que qualidade: a maior parte dos conteúdos não vão agregar tanta informação nova, complementar ou que vai ser realmente uma tendência. Assim, se conseguíssemos focar naquilo que têm mais impacto, poderíamos conseguir nos manter dentro de uma zona de conhecimento que talvez seja mais sustentável.1\nAssumindo que essa linha de raciocínio faça sentido, voltamos ao problema inicial: têm muita material disponível, e pode ser difícil saber separar o que é útil e o que não é. Ao longo dos últimos meses tenho percebido esse padrão, e colocado um bocado de energia em tentar me manter atualizado e informado sobre o que têm saído. E, durante um tempo, eu achei que estava dando sorte em achar e escolher os conteúdos certos…mas depois o que eu comecei a pensar é que talvez eu esteja em uma posição muito privilegiada de saber distilar o que faz sentido e o que não faz, e priorizar no que focar no tempo que tenho.\nIsso me trouxe um questionamento…e se eu estivesse começando hoje a estudar IA Generativa, que jornada eu percorreria? Mais do que isso…nem todo mundo quer ou precisa ser um expert em IA Generativa…pode ser que só saber a ferramenta esteja de bom tamanho, ou ter mais profundidade sobre um assunto específico no tema. Por outro lado, pode ser o caso de querer ter um conhecimento mais abrangente, focando não só em IA Generativa, mas tudo o que está na fundação e por trás disso.\n\nCódigo# carregando os pacotes para esse post\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# criando um dataframe para armazenar as informacoes que vamos plotar e facilitar o processo\ndf = pd.DataFrame({\n  'id'       : ['rect_' + str(i) for i in range(6)],\n  'position' : [(0, 0), (0.3, 0.1), (0.58, 0.15), (0.8, 0.2), (0.84, 0.21), (0.87, 0.22)],\n  'x'        : [1, 0.65, 0.35, 0.12, 0.07, 0.03],\n  'y'        : [1, 0.6, 0.3, 0.12, 0.06, 0.01],\n  'texto'    : ['Inteligência Artificial', 'Aprendizado de Máquina', 'Deep Learning', np.nan, np.nan, np.nan],\n  'subs'     : ['(e.g., sistemas baseados em regra, aprendizado de máquina,...)', '(e.g., supervisionado, não-supervisionado,\\npor reforço,...)', '(e.g., redes convolucionais,\\nrecorrentes, atenção,...)', np.nan, np.nan, np.nan],\n  'linecolor': ['black', 'white', 'white', 'white', 'white', 'white'],\n  'linesize' : [1, 0.7, 0.7, 0.7, 0.7, 0.7],\n  'rectfill' : ['#C6DBEF', '#9ECAE1', '#6BAED6', '#4292C6', '#2171B5', '#08306B'],\n  'texto_x'  : [0.03, 0.33, 0.61, 0.63, np.nan, np.nan],\n  'texto_y'  : [0.95, 0.65, 0.4, 0.35, np.nan, np.nan],\n  'sub_x'    : [0.03, 0.33, 0.61, np.nan, np.nan, np.nan],\n  'sub_y'    : [0.92, 0.6, 0.35, np.nan, np.nan, np.nan]\n})\n\n# criando um dicionario para definir a fonte associada à maior parte dos textos\nfonte = {'size': 12, 'weight': 'bold', 'family': 'Permanent Marker'}\nsubs = {'size': 8, 'family': 'Raleway'}\n\n# criando o canvas da figura\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (6, 6))\n\n# criando uma lista vazia para armazenar cada um dos retangulos criados no loop\nrects = []\n\n# criando cada um dos retângulos através de um loop\nfor row in df.itertuples(index = False):\n  rects.append(patches.Rectangle(\n    xy = row.position, width = row.x, height = row.y, \n    linewidth = row.linesize,edgecolor = row.linecolor, facecolor = row.rectfill)\n  )\n  if row.texto is not np.nan:\n    plt.text(x = row.texto_x, y = row.texto_y, s = row.texto, color = 'black', fontdict = fonte)\n    plt.text(x = row.sub_x, y = row.sub_y, s = row.subs, color = 'black', fontdict = subs)\n\n# adicionando cada um dos retângulos em um loop através da lista\nfor rect in rects:\n  ax.add_patch(rect)\n\n# adicionando anotações na lateral direita do plot\nax.annotate(text = 'GANs', xy = (1.05, 0.35), fontsize = 12, fontweight = 'bold', annotation_clip = False, \n            family = 'Permanent Marker')\nax.annotate(text = '(Generative Adversarial\\nNetworks; aka GenAI)', xy = (1.05, 0.31), fontsize = 7, \n            annotation_clip = False, family = 'Raleway')\nax.annotate(text = '', xy = (0.88, 0.29), xytext = (1.04, 0.36), xycoords = 'data', \n            arrowprops = dict(arrowstyle = '-|&gt;', color = 'black', lw = 1, connectionstyle = \"arc3,rad=0.3\"))\nax.annotate(text = 'LMs', xy = (1.05, 0.25), fontsize = 12, fontweight = 'bold', annotation_clip = False, \n            family = 'Permanent Marker')\nax.annotate(text = '(Language\\nModels)', xy = (1.05, 0.21), fontsize = 7, annotation_clip = False, \n            family = 'Raleway')\nax.annotate(text = '', xy = (0.88, 0.24), xytext = (1.04, 0.26), xycoords = 'data', \n            arrowprops = dict(arrowstyle = '-|&gt;', color = 'black', lw = 1, connectionstyle = \"arc3,rad=0.2\"))\nax.annotate(text = 'GPT-family', xy = (1.05, 0.15), fontsize = 12, fontweight = 'bold', annotation_clip = False, \n            family = 'Permanent Marker')\nax.annotate(text = '(GPT-4 Turbo, GPT-4,\\nGPT-3.5 Turbo,...)', xy = (1.05, 0.11), fontsize = 7, \n            annotation_clip = False, family = 'Raleway')\nax.annotate(text = '', xy = (0.88, 0.225), xytext = (1.04, 0.17), xycoords = 'data', \n            arrowprops = dict(arrowstyle = '-|&gt;', color = 'black', lw = 1, connectionstyle = \"arc3,rad=0.3\"))\n            \n# adicionando anotações na parte de baixo da figura\nax.annotate(text = 'A relação entre o tamanho das caixas é apenas qualitativa e ilustrativa.', \n            xy = (0, -0.02), fontsize = 6, annotation_clip = False, family = 'Raleway')\nax.annotate(text = 'nacmarino.netlify.app', xy = (1.1, -0.02), fontsize = 6, ha = 'left', \n            annotation_clip = False, family = 'Passion One')\n\n# removendo os completamente as informações dos eixos da figura \nax.set_axis_off()\nfig.patch.set_facecolor('white')\n\n# plotando a figura\nplt.tight_layout()\nplt.show()\n\n\n\nApesar de parecer que IA Generativa e Language Models são uma coisa por si só, na realidade, eles são um dos componentes e campos do conhecimento de algo muito maior - e que fornece todas as bases teóricas para podemos entender no detalhe como funcionam e fazem o que fazem. No entanto, uma pessoa não precisa dominar toda essa hierarquia de conhecimento para atuar com IA Generativa - os focos e jornadas de aprendizado são individuais, e é perfeitamente possível pular alguns desses níveiss dependendo do seu objetivo.\n\n\n\n\n\n\n\n\n\nNota sobre a figura\n\n\n\n\n\nA figura acima abstrai um pouco algumas complexidades de se colocar àquelas áreas do conhecimento em caixinhas. Por exemplo, os (Large) Language Models combinam alguma coisa de Redes Generativas Adversariais, arquitetura de Transformers e aprendizagem por reforço (além de outras áreas do conhecimento como, por exemplo, linguística). Ficaria bem difícil representar toda essa sobreposição em uma imagem em 2D, então optei pelo caminho mais simples para facilitar a compreensão da mensagem principal. De toda forma, achei relevante ter esse disclaimer aqui.\n\n\n\nEssas questões me levaram a pensar em por quê não criar uma lista que consolide uma jornada de aprendizado em IA Generativa. Talvez uma lista mais genérica, que possa servir tanto de guia para quem só está interessado em ter contexto, quanto para quem quiser percorrer uma jornada de aprendizado mais longa e profunda. O intuito não é ter a lista definitiva ou a lista certa, nem nada disso…mas talvez uma lista que tenha um sentido lógico e uma cadência, que não seja nada muito complexo ou que vá exigir foco contínuo por muito tempo…e, mais importante, que seja flexível o suficiente para a maior parte dos objetivos de aprendizado."
  },
  {
    "objectID": "posts/2024-06-24_genai-track/index.html#footnotes",
    "href": "posts/2024-06-24_genai-track/index.html#footnotes",
    "title": "Como eu começaria a estudar GenAI hoje?",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\nSei que isso pode parecer conformismo ou falta de vontade de se esforçar, mas meu ponto de vista é que existem inúmeras formas de se chegar no mesmo lugar…a escolha entre elas vai depender de quanto tempo você tem e quão rápido você quer percorrer esse caminho.↩︎"
  }
]