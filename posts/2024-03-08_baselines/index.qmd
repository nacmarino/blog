---
title: "Por quê você precisa de um baseline?"
subtitle: |
  Adicionar descrição.
date: 03-08-2024
categories:
  - machine learning
  - r
execute:
  warning: false
  fig-align: center
draft: true
---

```{r setup}
#| include: false
#| echo: false
#| code-fold: false
#| results: 'hide'
# loading the renv file associated with this blog post
renv::use(lockfile = 'renv.lock')

# presetando o ggplot2
library(ggplot2)
library(extrafont)

# setando o tema geral do ggplot2
theme_set(new = theme_minimal(base_family = 'Roboto'))

# atualizando o tema
theme_update(
  plot.title    = element_text(face = 'bold', size = 10),
  plot.subtitle = element_text(size = 8),
  plot.caption  = element_text(size = 8),
  axis.title    = element_text(face = 'bold', size = 8),
  axis.text     = element_text(color = 'black', size = 8),
  strip.text    = element_text(face = 'bold', size = 8)
)
```

::: {.callout-note}
Escrevi este post originalmente há alguns anos atrás para compartilhar com alguns colegas de trabalho, mas acredito que ele ainda é contemporâneo. Portanto, trouxe basicamente o mesmo conteúdo, apenas fazendo algumas atualizações necessárias aqui e ali.
:::

# O __Science__ em Data

Comecei a minha carreira na academia, onde tive a oportunidade de aprender algumas coisas interessantes e incorporá-las à forma de pensar. Uma delas foi a habilidade de duvidar, questionar e criticar^[Não é no sentido ruim, mas sim no de ter bom-senso para julgar o que me é mostrado.]. Existem casos em que é muito fácil exercitar essa habilidade, como [aqui](https://youtu.be/Yx6UgfQreYY)^[Isso é um pseudo-experimento, com um desenho experimental horroroso e resultados que não se sustentam.], mas têm vezes que a questão não é tão trivial. Ainda assim, esse tipo de exercício tende a ser inconsciente para mim. Mas o que é que isso tem haver com Ciência de Dados?

Ás vezes me parece que passamos batido, mas existe a palavra **ciência** no nome daquela disciplina - e a ciência nos oferece um jeito muito poderoso de pensar, testar hipóteses e avaliar o que nos é mostrado. Por exemplo: como você sabe que A é melhor do que B, que uma intervenção funciona, que não tem diferença entre duas coisas...como saber se você realmente descobriu o que você diz ter descoberto^[Isto é, algo robusto, generalizável, livre de artefatos, viéses ou frutos de relações espúrias.]? O método científico é a resposta para estas perguntas, e ele nos ensina a pensar de forma estruturada para ir do primeiro ao segundo. E essas são os tipos de coisas que estão diretamente relacionadas ao nosso dia a dia enquanto pessoas que trabalham com ciência de dados^[~~ou como qualquer ser humano~~].

Com isso em mente, o objetivo desse post é falar um pouco mais sobre como pensar cientificamente no nosso dia a dia. Vamos focar primeiro em uma tarefa mais cotidiana, que é a modelagem preditiva; depois, vamos ver como esse tipo de pensamento faz parte não só da modelagem, mas de qualquer outra etapa de um projeto de Ciência de Dados.

# Como você sabe que seu modelo é bom?

Quando pensamos em criar um modelo preditiva, geralmente o fazemos porque achamos que de alguma forma ele vai ser melhor do que alguma coisa. Mas que coisa é essa? Isso é um ponto importante termos em mente quando começamos essa tarefa: precisamos ter um ponto de partida para definir se aquilo que estamos propondo vai ser melhor do que aquilo que já temos. O nome que damos à isso é *baseline*.

É difícil chegarmos à uma regra geral para definir esse *baseline*, uma vez que temos diferentes tipos de problemas de dados (e.g., classificação, regressão, séries temporais,...). Cada um daqueles problemas de dados têm peculiaridades distintas, que balizam a forma como devemos definir um *baseline* - por exemplo, a dependência temporal entre observações pode ser muito informativa para séries temporais, mas ela está ausente em problemas de regressão tradicionais. Assim, cada problema traz uma abordagem diferente para a definição do *baseline*, mas todas elas convergem para o mesmo objetivo: nos ajudar a definir se o modelo realmente tem o potencial de trazer um benefício sobre o que já temos.

# Baselines de livro-texto

Em um problema de classificação normalmente estamos querendo fornecer pelo menos uma categoria à uma observação. Nesse caso, nosso ponto de partida - o *baseline* - pode ser utilizarmos uma abordagem mais inocente, como se não houvesse nenhuma inteligência por traz do fenômeno em questão, e 'dizermos' que todas as observações pertencem à uma única categoria - normalmente àquela com maior frequência em nossa base de dados. Ao 'chutarmos' que toda a observação pertence à categoria mais frequente, estamos definindo um ponto de partida bem otimista para nossa tarefa de modelagem^[Inclusive, prevenindo problemas que normalmente acontecem quando trabalhamos com uma frequência muito desigual de representação das categorias na base de dados, mas isso é estória para outro momento.]. Essa noção é representada no exemplo abaixo:

```{r}
library(tidyverse) # manipulação de dados
library(tidymodels) # modelagem
library(modeldata) # dados

########################################################################################
#                          Separando a base em treino e teste                          #
########################################################################################

# criando um split entre treino e teste, estratificando a separação da base pelo variável
# que queremos usar no problema de classificação; além disso, setando uma constante para
# o gerador de números aleatórios para garantir que vamos conseguir reproduzir o split
set.seed(42)
splits <- initial_split(data = credit_data, prop = 0.7, strata = 'Status')

########################################################################################
#        Estimando a performance do baseline: prevendo a classe mais frequente         #
########################################################################################

# estimando a classe mais frequente na base de treino: contando quantas vezes cada classe
# aparece dentro da variável que queremos prever e, então, tirando o nome da classe mais comum
classe_frequente <- training(x = splits) %>% 
  count(Status) %>% 
  top_n(n = 1, wt = n) %>% 
  pull(Status) %>% 
  as.character()

# calculando a performance do baseline na base de teste - a ideia aqui vai ser definir que todas
# as instancias na base de teste pertencem à classe mais frequente na base de treino (como a função
# accuracy precisa receber um fato, o tratamento dentro do mutate serve apenas para compatibilizar
# o tipo de dado esperado pela funcao)
metricas_baseline <- testing(splits) %>% 
  mutate(baseline = factor(classe_frequente, levels = unique(training(x = splits)$Status))) %>% 
  accuracy(truth = Status, estimate = baseline) %>% 
  mutate(origem = 'baseline')

########################################################################################
#            Ajustando e estimando a performance de um algoritmo preditivo             # 
########################################################################################

# criando uma regressão logística simples para o problema de classificação
## primeiro setando o algoritmo
algoritmo <- logistic_reg() %>% 
  set_engine(engine = 'glm') %>% 
  set_mode(mode = 'classification')

## agora ajustando o algoritmo aos dados
data_prep <- recipe(Status ~ Income + Amount + Debt + Price + Home + Job, data = splits) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors())

## conectando os dados com o algoritmo
wf <- workflow() %>% 
  add_model(algoritmo) %>% 
  add_recipe(data_prep)

## ajustando o algoritmo
modelo <- fit(object = wf, data = training(x = splits))

# usando a classe mais frequente para definir a performance do baseline para o problema de classificação
metricas_modelo <- modelo %>% 
  augment(new_data = testing(x = splits)) %>% 
  accuracy(truth = Status, estimate = .pred_class) %>% 
  mutate(origem = 'modelo')

########################################################################################
#                        Visualizando os resultados encontrados                        #
########################################################################################

bind_rows(metricas_baseline, metricas_modelo) %>% 
  arrange(origem) %>% 
  mutate(
    .estimate = ifelse(test = origem == 'baseline', yes = .estimate, no = .estimate - first(.estimate)),
    origem    = fct_relevel(origem, 'modelo')
  ) %>% 
  ggplot(mapping = aes(x = 1, y = .estimate, fill = origem)) +
  geom_col(color = 'black', show.legend = FALSE, linewidth = 1, width = 0.6) +
  scale_fill_manual(values = c('white', 'grey50')) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  labs(
    y = 'Acurácia'
  ) +
  coord_flip() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y  = element_blank(),
    panel.grid   = element_blank(),
    axis.line.x  = element_line()
  )
```


# Baselines não-tão-óbivos

# Em resumo...

Existe uma forma simples de pensar nisso: conseguimos dizer que uma **intervenção**^[i.e., um *tratamento*] é melhor do que a **condição atual**^[i.e., o *baseline* ou o *controle*] se a única diferença entre os dois é aquilo que estamos querendo testar e tudo o mais é igual. 

# Mas o baseline pára na modelagem?

Esta é uma das bases do método científico, e um aspecto fundamental de qualquer tipo de experimento.

Como uma ciência, também é necessário termos esse tipo de pensamento em todas as etapas de um trabalho de Ciência de Dados. Isto é especialmente importante quando iniciamos a etapa de modelagem, quando deveríamos fazer perguntas como:  

* Eu realmente preciso de Machine Learning para resolver esse problema?  
* Esse modelo presta?  
* Será que um algoritmo mais simples não é equivalente a este outro mais complexo?  
* A minha otimização de hiperparâmetros está sendo efetiva?  
* Eu realmente preciso dessa covariável?  

Neste aspecto, é extremamente importante que sejamos capaz de definir um __baseline__ razoável para o problema de ciência de dados que buscamos resolver - isto é, é a solução mais __simples__ que se pode dar a um problema de dados que estamos trabalhando. Exemplos de um baseline seriam algum tipo de regra de negócio, algum sumário estatístico dos dados ou, ainda, valores aleatórios. Assim como em um experimento, a ideia aqui é algum tipo de intervenção que não contenha aquilo que você quer implementar de forma à gerar uma predição. 

A pergunta agora que segue é: qual baseline para os diferentes tipos de problemas de dados que normalmente encontramos? Vamos ver alguns exemplos disto nos itens a seguir.

# Exemplos práticos de baseline

## Classificação

Em um problema de classificação normalmente queremos:
1. Prever uma ou mais classes a qual pertence cada instância; e/ou,  
2. Prever a probabilidade de uma instância pertencer à uma ou mais classes.  

Quando estamos trabalhando com o objetivo de prever de classes, uma prática comum é definir como o baseline da nossa solução que todas as instâncias pertencem à apenas uma das classes disponíveis (normalmente, a classe mais comum). No exemplo abaixo, temos que grande parte das instâncias pertencem à classe negativa:

```{r baseline_exemplo_classificacao_imbalance}
library(tidyverse) # core
library(tidymodels) # machine learning
library(modeltime) # series temporais

## setando a seed para gerar as instâncias
set.seed(seed = 33)
## gerando as instâncias
targets <- sample(x = c('Positivo', 'Negativo'), size = 500, prob = c(0.05, 0.95), replace = TRUE)
## passando pela elas para um fator
targets <- as.factor(targets)
## gerando a tabela
kableExtra::kable(table(targets), row.names = FALSE, col.names = c('Classe', 'Observações'), align = 'c', format = 'html')
```

Assim, se estamos utilizando a acurácia como a métrica de performance da nossa solução, e utilizamos a estratégia descrita acima para definir nosso baseline, temos como nível do nosso baseline uma acurácia de 94.8%. Isto quer dizer que qualquer algoritmo que vamos tentar ajustar a este problema deve apresentar uma performance superior a este baseline, de forma a ser considerado útil à resolução ao problema de dados. Caso contrário, temos evidência de que o algoritmo utilizado não é melhor do que uma regra de negócio qualquer, e o modelo que obtivemos não é útil para resolver o problema de dados.

```{r baseline_exemplo_classificacao_chute}
# criando o nosso baseline naive
classe_naive <- rep(x = 'Negativo', times = 500) 
classe_naive <- factor(x = classe_naive, levels = c('Negativo', 'Positivo'))

# calculando a acurácia no nosso baseline naive
accuracy_vec(truth = targets, estimate = classe_naive)
```

Quando estamos prevendo probabilidades, por outro lado, uma abordagem que pode ser utilizada é definir como a proporção de instâncias na classe alvo como o nosso baseline. Utilizando novamente o nosso exemplo acima, isto corresponderia dizer que a probabilidade seria equivalente à proporção de vezes que a classe positiva ocorre no nosso conjunto de dados. Isto é:

```{r}
# calculando a probabilidade do baseline como a proporção de casos positivos nos dados
probabilidade_naive <- sum(targets == 'Positivo') / length(targets)

# definindo esta probabilidade para todas as instâncias
probabilidade_naive <- rep(x = probabilidade_naive, times = 500)
```

Assim, se estamos utilizando a entropia cruzada como nossa métrica de avaliação, teríamos que como o nosso baseline o valor de `r mn_log_loss_vec(truth = targets, estimate = probabilidade_naive)`. Consequentemente, para que qualquer modelo que desenvolvamos seja considerado útil, ele precisaria fornecer um valor menor do que este.

```{r}
mn_log_loss_vec(truth = targets, estimate = probabilidade_naive)
```

## Regressão

A idéia por trás da técnica de regressão é que estamos utilizando algum tipo de modelo que possa prever o valor esperado de uma variável resposta dado os valores de uma ou mais covariáveis - conforme representado na figura abaixo. Se fizermos uma secção vertical nesta relação (representada pela caixa com as linhas tracejadas vermelhas, abaixo), temos que a previsão dada por aquele segmento de reta naquele trecho nada mais é do que o valor esperado da média de _y_. Na realidade, se pudéssemos dar um zoom a cada microsegmento da reta que define esta relação, teríamos ali os valores da média de _y_ para aquele(s) valor(es) de _x_. Em última instância, é daí que vem o nome da técnica: _regressão à média_. 

```{r echo=FALSE, message=FALSE, fig.align='center'}
ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) +
  geom_smooth(method = 'lm', se = FALSE, color = 'black') +
  geom_point(shape = 21, size = 4, stroke = 1.5, color = 'black', fill = 'gold') +
  geom_rect(mapping = aes(xmin = 165, xmax = 190, ymin = 14, ymax = 21), fill = NA, color = 'red', linetype = 2, size = 1) +
  scale_x_continuous(name = 'Cavalos de Potência', n.breaks = 10) +
  scale_y_continuous(name = 'Milhas por Galão', n.breaks = 10) +
  ggtitle(label = 'Relação entre a Potência e o Consumo de Combustível',
          subtitle = 'A linha contínua escura representa a reta da regressão linear\nentre a variável preditora e a variável resposta') +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 16), 
        plot.subtitle = element_text(hjust = 0.5))
```

Desta forma, uma baseline razoável de ser utilizado para problemas de regressão é definirmos que o valor previsto de _y_ para todo e qualquer valor de _x_ é a média de _y_. Em outras palavras: $$f(y) = \bar{y}$$. Visualmente, isto seria equivalente à:

```{r echo=FALSE, message=FALSE, fig.align='center'}
ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) +
  geom_abline(slope = 0, intercept = mean(mtcars$mpg), size = 1.5, color = 'red', linetype = 1) +
  geom_point(shape = 21, size = 4, stroke = 1.5, color = 'black', fill = 'gold') +
  scale_x_continuous(name = 'Cavalos de Potência', n.breaks = 10) +
  scale_y_continuous(name = 'Milhas por Galão', n.breaks = 10) +
  ggtitle(label = 'Relação entre a Potência e o Consumo de Combustível',
          subtitle = 'A linha contínua vermelha representa a média dos valores de y para todo e qualquer x') +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 16), 
        plot.subtitle = element_text(hjust = 0.5))
```

Assim, se estivéssemos usando o RMSE como nossa métrica de performance para este problema de regressão, teríamos como baseline um valor de 5.93203. Novamente, qualquer algoritmo que utilizemos deve nos fornecer uma RMSE menor do que este de forma a ser considerado útil ao problema de negócio.

```{r}
# calculando a média da variável resposta
regressao_naive <- mean(mtcars$mpg)

# repetindo ela tantas vezes quantas forem o número de instâncias
regressao_naive <- rep(x = regressao_naive, times = nrow(mtcars))

# calculando o RMSE
rmse_vec(truth = mtcars$mpg, estimate = regressao_naive)
```

## Séries Temporais

Apesar de também prevermos valores numéricos contínuos quando trabalhamos com séries temporais, é importante considerarmos que existe algum tipo de correlação temporal entre os valores nestas sequências. Assim, uma forma de estabelecer um baseline para um problema de previsão de séries temporais é considerar que os valores observados nos períodos de tempo à frente serão idênticos aqueles observados nos períodos anteriores.

Por exemplo, se queremos prever os valores em t+1, t+2 e t+3, podemos utilizar como baseline para previsão o valor observado em t+0, para todos os três passos de tempo:

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
# passando os dados para um tibble
dados <- tibble(Valor = as.numeric(AirPassengers)) %>% 
  mutate(Tempo = 1:n()) %>% 
  filter(Tempo <= 30) %>% 
  add_column(
    previsao_1 = c(rep(x = NA, times = 10), rep(.$Valor[10], times = 3), rep(x = NA, times = nrow(.) - 10 - 3)),
    previsao_2 = c(rep(x = NA, times = 15), rep(.$Valor[15], times = 3), rep(x = NA, times = nrow(.) - 15 - 3)),
    previsao_3 = c(rep(x = NA, times = 20), rep(.$Valor[20], times = 3), rep(x = NA, times = nrow(.) - 20 - 3)),
    previsao_4 = c(rep(x = NA, times = 25), rep(.$Valor[25], times = 3), rep(x = NA, times = nrow(.) - 25 - 3))
  )

# gerando a figura
dados %>% 
  pivot_longer(-Tempo) %>% 
  mutate(tipo = str_detect(string = name, pattern = 'previsao')) %>% 
  ggplot(mapping = aes(x = Tempo, y = value, group = name)) +
  geom_line(size = 1) +
  geom_point(mapping = aes(fill = tipo, size = tipo), shape = 21) +
  scale_size_manual(values = c(4, 3)) +
  scale_fill_manual(name = 'Previsão', values = c('deepskyblue2', 'indianred3')) +
  scale_x_continuous(name = 'Unidades de Tempo', n.breaks = 10) +
  scale_y_continuous(name = 'Volume de Vendas', n.breaks = 10) +
  ggtitle(label = 'Série Temporal do Volume de Vendas',
          subtitle = 'Os pontos azuis representam os valores observados e, os pontos vermelhos, os valores preditos') +
  theme_classic() +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5, size = 16), 
        plot.subtitle = element_text(hjust = 0.5))

# pegando apenas as previsao e o valor observado
dados <- dados %>% 
  slice(11:13, 16:18, 21:23, 26:28) %>% 
  mutate(previsao = rowSums(.[, 3:6], na.rm = TRUE)) %>% 
  select(Valor, previsao_naive = previsao)
previsao_naive <- dados$previsao_naive
volume_observado <- dados$Valor
```

Se estivéssemos usando o MAPE como a métrica de avaliação de performance, teríamos como baseline um valor de 13.23461 - e qualquer modelo viemos a gerar no futuro deve fornecer um valor menor do que este de forma a ser considerado útil para endereçar o problema de negócio.

```{r}
mape_vec(truth = volume_observado, estimate = previsao_naive)
```

# Mas deixa eu perguntar...

A idéia aqui foi fornecer uma noção básica do que é um baseline e de que forma podemos estimá-lo quando começamos a trabalhar com um problema de predição em Ciência de Dados. Os exemplos que utilizamos aqui são bastante simplistas, e endereçam a pergunta inicial que devemos fazer sempre que começamos um projeto: __'eu preciso de Machine Learning para resolver este problema?'__. É claro que isso deixa de fora alguns outros pontos fundamentais quando pensamos na questão de definição de baselines, tais como:

> E os meus modelos? Eu não devo definir um baseline para eles?

Sim, você deve, e sempre! Uma boa prática é sempre definir o baseline da modelagem com o algoritmo mais simples possível em sua configuração padrão.A partir daí, começamos a testar as versões tunadas deste algoritmo e/ou, ainda, outros algoritmos mais complexos. Aqui, novamente, devemos tentar a versão padrão de cada algoritmo utilizado, antes de partir para testar as versões tunadas destes algoritmos - afinal, como saber que uma intervenção que estamos fazendo é efetiva sem saber o resultado que teríamos obtido sem essa intervenção? 

> Quantos baselines eu preciso definir?

Tantos quantos forem os necessários. Como discutido acima, precisamos de um baseline para testar a efetividade de cada intervenção que fazemos durante a modelagem. Além disso, é preciso não perder de vista o fato de que em muitos problemas já possuímos alguma espécie de baseline do negócio. No entanto, não devemos confundir o que é um baseline da modelagem com o que é o baseline de negócio: podemos ter um baseline de negócio tão alto que chega a ser irrelevante calcular algum baseline da modelagem (nada que possamos fazer vai bater aquele baseline de negócio e aí voltamos à pergunta: precisamos de Machine Learning aqui?).

O ponto principal aqui é que nenhum baseline é mutuamente exclusivo: precisamos sempre saber o que conseguimos atingir de performance antes de tentar qualquer tipo de intervenção. Assim, considere tantos baselines quanto os necessários no seu projeto de dados.

> Qual a métrica de performance que eu uso para o meu baseline?

Este é um ponto fundamental para continuarmos esta discussão, mas não é uma pergunta com uma resposta direta. A métrica de performance que você vai usar dependende de inúmeros fatores, tais como:

- Que tipo de problema de dados você quer resolver? (_e.g._, classificação, regressão,...)  
- Que tipo de fenômeno você quer prever? (_e.g._, classes, probabilidades,...)  
- Quão desbalanceada é a sua base de dados? (problema especialmente relevante para problemas de classificação)  
- O que é mais importante para o negócio? (_e.g._., prevenir falsos positivos, projetar de acordo com as previsões,...)  
- Qual o trade-off que o negócio está disposto à trabalhar? (_e.g._, balancear falsos positivos e falsos negativos,...)  
- E diversas outras perguntas que dependem do contexto no qual seu projeto está inserido.  

A discussão sobre a escolha das métricas de performance por si só é bastante extensa, e merecerá um espaço para isso em breve. 

# Conclusão
Existem vários aspectos relacionados ao desenho de um experimento para testar uma hipótese que eu gostaria de falar aqui.
